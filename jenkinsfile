//////////
////////// System part
//////////

import io.jenkins.blueocean.rest.impl.pipeline.PipelineNodeGraphVisitor
import io.jenkins.blueocean.rest.impl.pipeline.FlowNodeWrapper
import org.jenkinsci.plugins.workflow.support.steps.build.RunWrapper

// Get information about all stages, including the failure causes.
// Returns a list of maps: [[id, displayName, result]]
// The 'errors' member is a list of unique exceptions.

@NonCPS
List<Map> getStageResults( RunWrapper build ) {

    // Get all pipeline nodes that represent stages
    def visitor = new PipelineNodeGraphVisitor( build.rawBuild )
    def stages = visitor.pipelineNodes.findAll{ it.type == FlowNodeWrapper.NodeType.STAGE || it.type == FlowNodeWrapper.NodeType.PARALLEL }

    return stages.collect{ stage ->
        return [
            id: stage.id,
            displayName: stage.displayName,
            result: "${stage.status.result}",
        ]
    }
}

// Get information of all failed stages
@NonCPS
List<Map> getFailedStages( RunWrapper build ) {
    return getStageResults( build ).findAll{ it.result == 'FAILURE' }
}

def setupMavenSettings() {
    script {
        withCredentials([file(credentialsId: 'maven-artifactory-settings', variable: 'MVN_SETTINGS_PATH')]) {
            sh script: '''
                mkdir -p ~/.m2
                cp -f $MVN_SETTINGS_PATH ~/.m2/settings.xml
            ''',
            label: 'setup maven settings'
        }
    }
}

def mavenInstall() {
    setupMavenSettings()
    sh  script: '${MVN} clean install -q -ff ${MAVEN_SKIP} ${MAVEN_SKIP_TESTS} -T 1C',
        label: 'mvn install'
}

def dockerCleanup() {
    sh  script: 'docker system prune -f -a --volumes || true',
        label: 'cleanup docker data'
}

def mavenDockerBackup() {
    rtDockerPull(
        serverId: "${RT_SERVER_ID}",
        image: "${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}",
        sourceRepo: 'docker-local'
    )
    sh script: '''#!/bin/bash -e
        TEMP_DIR=$(mktemp -d)
        git status --ignored --porcelain | \
            grep -E '^\\!\\!' | sed 's|/$||' | awk -F'\\!\\! ' '{print $2}' | grep -v -E '^web-console' | \
        while read mvnresult; do mkdir -p ${TEMP_DIR}/$(dirname -- $mvnresult); cp -R $mvnresult ${TEMP_DIR}/$mvnresult; done
        tar -czf /tmp/mvnresult.tar.gz -C ${TEMP_DIR} .
        tar -czf /tmp/m2repository.tar.gz -C ~/.m2 repository
        rm -rf ${TEMP_DIR}

        cd /tmp
        echo "FROM ${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}" >> Dockerfile
        echo "COPY m2repository.tar.gz /tmp/m2repository.tar.gz" >> Dockerfile
        echo "COPY mvnresult.tar.gz /tmp/mvnresult.tar.gz" >> Dockerfile
        docker build -t ${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}-${DOCKER_TAG} .
    ''',
    label: 'build temporary docker image for the build'
    rtDockerPush(
        serverId: "${RT_SERVER_ID}",
        image: "${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}-${DOCKER_TAG}",
        targetRepo: 'docker-local'
    )
}

def mavenCacheRestore() {
    setupMavenSettings()
    sh script: '''#!/bin/bash -e
        tar -xzf /tmp/mvnresult.tar.gz --no-overwrite-dir -C ./
        mkdir -p ~/.m2
        tar -xzf /tmp/m2repository.tar.gz --no-overwrite-dir -C ~/.m2/
    ''',
    label: 'restore build cache'
}

def prepareIfconfig() {
    sh script: '''
        echo -e '#!/bin/bash\nPATH=/sbin:$PATH exec ifconfig eth0 $@' > /usr/bin/ifconfig
        chmod +x /usr/bin/ifconfig
        hash -r
    ''',
    label: "prepare ifconfig"
}

def buildArtifacts(stageName) {
    script {
        def stageArtifactsDirPath = "stage_${stageName.replaceAll(~/[^A-Za-z0-9_-]/,'_')}"
        sh script: "mkdir -p ${stageArtifactsDirPath}"
        // copy logs
        sh script: """#!/bin/bash -x
            [ -d ~/shared/logs ] && cp -R ~/shared/logs ${stageArtifactsDirPath}/ || true
            [ -d ~/shared/tasklogs ] && cp -R ~/shared/tasklogs ${stageArtifactsDirPath}/ || true

            shopt -s globstar
            copy_files_by_pattern() {
                if  ls \${1}; then
                    for fname in \${1}; do
                        fname_root=\$(echo \${fname} | cut -d "/" -f1)
                        if ! [[ "\$fname_root" = "\${2}" ]]; then
                            mkdir -p \$(dirname -- "\${2}/\${fname}")
                            cp \$fname \${2}/\${fname}
                        fi
                    done
                fi
            }

            # copy test reports
            copy_files_by_pattern '**/target/surefire-reports/*.xml' ${stageArtifactsDirPath}
            copy_files_by_pattern '**/target/failsafe-reports/*.xml' ${stageArtifactsDirPath}
            # copy top-level jacoco reports
            copy_files_by_pattern '**/target/*.exec' ${stageArtifactsDirPath}
            copy_files_by_pattern '**/jacoco/*.html' ${stageArtifactsDirPath}
            copy_files_by_pattern '**/jacoco/*.xml' ${stageArtifactsDirPath}
            copy_files_by_pattern '**/jacoco/*.csv' ${stageArtifactsDirPath}
            copy_files_by_pattern '**/jacoco/jacoco-resources/*' ${stageArtifactsDirPath}
            # copy detailed jacoco reports
            copy_files_by_pattern '**/jacoco/**/*.html' ${stageArtifactsDirPath}
            copy_files_by_pattern '**/jacoco/**/*.xml' ${stageArtifactsDirPath}
            copy_files_by_pattern '**/jacoco/**/*.csv' ${stageArtifactsDirPath}
            # inspection results
            copy_files_by_pattern 'inspection-results/*.xml' ${stageArtifactsDirPath}
        """
        // fixate artifacts
        archiveArtifacts artifacts: "${stageArtifactsDirPath}/**", allowEmptyArchive: true
        // cleanup tmp artifacts dir
        sh script: "rm -rf ${stageArtifactsDirPath}"
    }
}

def resetWs() {
    sh script: "git clean -fdx", label: "Clean up everything but files from git"
}

def isEligibleToNotify() {
    return  !env.CHANGE_ID \
            && (env.BRANCH_NAME ==~ env.ACTIVE_DEV_BRANCH_REGEX || env.BRANCH_NAME ==~ (env.RELEASE_BRANCH_PREFIX_REGEX+env.RELEASE_BRANCH_SUFFIX_REGEX))
}

def notify(fixed, failedStageNames) {
    def slackMessageColor = fixed ? '#33CC33' : '#FF3333'
    def titleBeginning = fixed ? 'Fixed' : 'Failed'

    def artifactSlackTip = fixed ? "" : "\ncheck artifacts (docker logs): ${env.BUILD_URL}artifact"
    def artifactEmailTip = fixed ? "" : "<br/>check artifacts (docker logs) :${env.BUILD_URL}artifact"

    def failedStageSlackTip = fixed ? "" : "\nFailed stages:\n" + failedStageNames.collect { "* ${it}" }.join("\n")
    def failedStageEmailTip = fixed ? "" : "<br/>Failed stages:<br/>" + failedStageNames.collect { "* ${it}" }.join("<br/>")

    def slackTarget = "#javabeans"
    def emailTarget = "eng.druid@imply.io"

    try {
        slackSend   color: slackMessageColor,
                    channel: slackTarget,
                    message: """
                        *${titleBeginning} implydata/druid build: ${env.BRANCH_NAME}#${env.BUILD_NUMBER}*
                        classic link: ${env.BUILD_URL}
                        blueocean link: ${env.RUN_DISPLAY_URL} ${artifactSlackTip} ${failedStageSlackTip}
                    """.replaceAll(/\n\s+/, "\n")
    } catch (err) {
        echo "Couldn't send slack notification"
        echo "Exception: ${err}"
    }

    try {
        mail    body: """
                    classic link: ${env.BUILD_URL}
                    <br/>blueocean link: ${env.RUN_DISPLAY_URL} ${artifactEmailTip} ${failedStageEmailTip}
                """.replaceAll(/\n\s+/, "\n"),
                cc: '', bcc: '', replyTo: '',
                from: 'jenkins@qa.imply.io',
                mimeType: 'text/html',
                subject: "${titleBeginning} implydata/druid build: ${env.BRANCH_NAME}#${env.BUILD_NUMBER}",
                to: emailTarget
    } catch (err) {
        echo "Couldn't send email notification"
        echo "Exception: ${err}"
    }
}

def implyQueryIntegrationTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    sh script: """
        \${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
        -Dgroups=query \
        -Dit.indexer=middleManager \
        ${jvmRuntimeOpt} \
        -Ddruid.test.config.extraDatasourceNameSuffix="" \
        -ff \${MAVEN_SKIP} -Djacoco.skip=true
    """,
    label: "imply query integration tests with ${jvmRuntimeOpt}"
    dockerCleanup()
}

def implyIngestServiceIntegrationTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    sh script: """
        \${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
        -Dgroups=ingest-service \
        -Dit.indexer=middleManager \
        ${jvmRuntimeOpt} \
        -ff \${MAVEN_SKIP} -Djacoco.skip=true
    """,
    label: "imply ingest service integration tests with ${jvmRuntimeOpt}"
    dockerCleanup()
}

def implyKeycloakSecurityIntegrationTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    sh script: """
        \${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
        -Dgroups=keycloak-security \
        -Dit.indexer=middleManager \
        ${jvmRuntimeOpt} \
        -ff \${MAVEN_SKIP} -Djacoco.skip=true
    """,
    label: "imply keycloak security integration tests with ${jvmRuntimeOpt}"
}

def s3DeepStorageTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    script {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'aws', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
            def cloudpath = UUID.randomUUID().toString()
            sh script: """
                aws s3 sync \
                ./integration-tests/src/test/resources/data/batch_index/json/ s3://druid-qa/${cloudpath} \
                --exclude "*" --include "wikipedia_index_data*.json"
            """
            writeFile   file: "jenkins/s3-config",
                        text: "druid_storage_type=s3\ndruid_storage_bucket=druid-qa\ndruid_storage_baseKey=${cloudpath}\ndruid_s3_accessKey=${AWS_ACCESS_KEY_ID}\ndruid_s3_secretKey=${AWS_SECRET_ACCESS_KEY}\nAWS_REGION=us-east-1\ndruid_extensions_loadList=[\"druid-s3-extensions\",\"druid-hdfs-storage\"]"
            try {
                sh script: """
                    \${MVN} verify -P integration-tests -pl integration-tests \
                    -Dgroups=s3-deep-storage \
                    -Doverride.config.path=\${WORKSPACE}/jenkins/s3-config \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.cloudBucket=druid-qa \
                    -Ddruid.test.config.cloudPath=${cloudpath}/ \
                    -Ddocker.build.hadoop=true \
                    -Dstart.hadoop.docker=true \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "s3-deep-storage with ${jvmRuntimeOpt}"
            }
            finally {
                sh script: "aws s3 rm s3://druid-qa/${cloudpath}  --recursive"
            }
        }
    }
    dockerCleanup()
}

def kinesisDeepStorageTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    script {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId:  'aws', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
            lock('awsResource') {
                writeFile   file: "jenkins/kinesis-config",
                            text: "druid_kinesis_accessKey=${AWS_ACCESS_KEY_ID}\ndruid_kinesis_secretKey=${AWS_SECRET_ACCESS_KEY}\nAWS_REGION=us-east-1\ndruid_extensions_loadList=[\"druid-kinesis-indexing-service\"]"
                sh script: """
                    \${MVN} verify -P integration-tests -pl integration-tests \
                    -Dgroups=kinesis-index \
                    -Doverride.config.path=\${WORKSPACE}/jenkins/kinesis-config \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.streamEndpoint=kinesis.us-east-1.amazonaws.com \
                    -Ddocker.build.hadoop=true \
                    -Dstart.hadoop.docker=true \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "kinesis-deep-storage with ${jvmRuntimeOpt}"
            }
        }
    }
    dockerCleanup()
}

def azureDeepStorageTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    script {
        withCredentials([usernamePassword(credentialsId: 'azure_credentials', usernameVariable: 'AZURE_ACCOUNT', passwordVariable: 'AZURE_KEY')]) {
            def containerName = UUID.randomUUID().toString()
            sh script: """
                az storage container create -n ${containerName} \
                --public-access blob \
                --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY}
            """
            writeFile   file: "jenkins/azure-config",
                        text: "druid_storage_type=azure\ndruid_azure_account=${AZURE_ACCOUNT}\ndruid_azure_key=${AZURE_KEY}\ndruid_azure_container=${containerName}\ndruid_extensions_loadList=[\"druid-azure-extensions\",\"druid-hdfs-storage\"]"
            try {
                sh script: """
                    az storage blob upload-batch \
                    --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY} \
                    -d ${containerName} \
                    --source ./integration-tests/src/test/resources/data/batch_index/json/ --pattern "wikipedia_index_data*.json"
                """
                sh script: """
                    \${MVN} verify -P integration-tests -pl integration-tests \
                    -Dgroups=azure-deep-storage \
                    -Doverride.config.path=\${WORKSPACE}/jenkins/azure-config \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.cloudBucket=${containerName} \
                    -Ddruid.test.config.cloudPath= \
                    -Ddocker.build.hadoop=true \
                    -Dstart.hadoop.docker=true \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "azure-deep-storage with ${jvmRuntimeOpt}"
            }
            finally {
                sh script: """
                    az storage container delete -n ${containerName}\
                    --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY}
                """
            }
        }
    }
    dockerCleanup()
}

def gcsDeepStorageTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    script {
        withCredentials([file(credentialsId: 'gcs-bucket-qa', variable: 'GC_KEY')]) {
            def cloudpath = "gcs-test-${UUID.randomUUID().toString()}/"
            def bucket = "imply-qa-testing"

            writeFile   file: "jenkins/gcs-config",
                        text: "druid_storage_type=google\ndruid_google_bucket=${bucket}\ndruid_google_prefix=${cloudpath}\ndruid_extensions_loadList=[\"druid-google-extensions\",\"druid-hdfs-storage\"]\nGOOGLE_APPLICATION_CREDENTIALS=/shared/docker/credentials/creds.json"
            sh script: """
                mkdir -p jenkins/gcs
                cp \${GC_KEY} jenkins/gcs/creds.json
                chmod 764 jenkins/gcs/creds.json
                gsutil \
                    -o Credentials:gs_service_key_file=\${WORKSPACE}/jenkins/gcs/creds.json \
                    cp \${WORKSPACE}/integration-tests/src/test/resources/data/batch_index/json/wikipedia_index_data*.json \
                    gs://${bucket}/${cloudpath}
                """,
                label: "copy gcs creds"
            try {
                sh script: """
                    \${MVN} verify -P integration-tests -pl integration-tests \
                    -Doverride.config.path=\${WORKSPACE}/jenkins/gcs-config \
                    -Dresource.file.dir.path=\${WORKSPACE}/jenkins/gcs \
                    -Dgroups=gcs-deep-storage \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.cloudBucket=${bucket} \
                    -Ddruid.test.config.cloudPath=${cloudpath} \
                    -Ddocker.build.hadoop=true \
                    -Dstart.hadoop.docker=true \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "gcs-deep-storage with ${jvmRuntimeOpt}"
            } finally {
                sh script: "gsutil -o Credentials:gs_service_key_file=\${WORKSPACE}/jenkins/gcs/creds.json rm -r gs://${bucket}/${cloudpath}"
            }
        }
    }
    dockerCleanup()
}

def hdfsDeepStorageTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    writeFile   file:   "jenkins/hdfs-config",
                text:   "druid_storage_type=hdfs\n" +
                        "druid_storage_storageDirectory=/druid/segments\n" +
                        "druid_extensions_loadList=[\"druid-hdfs-storage\"]\n" +
                        "druid_indexer_logs_type=hdfs\n" +
                        "druid_indexer_logs_directory=/druid/indexing-logs"
    sh script: """
        \${MVN} verify -P integration-tests -pl integration-tests \
        -Doverride.config.path=\${WORKSPACE}/jenkins/hdfs-config \
        -Dgroups=hdfs-deep-storage \
        -Ddocker.build.hadoop=true \
        -Dstart.hadoop.docker=true \
        ${jvmRuntimeOpt} \
        -Ddruid.test.config.extraDatasourceNameSuffix="" \
        -Dit.test=ITHdfsToHdfsParallelIndexTest \
        -ff \${MAVEN_SKIP} -Djacoco.skip=true
    """,
    label: "hdfs-deep-storage with ${jvmRuntimeOpt}"
    dockerCleanup()
}

def processingModuleTest() {
    mavenCacheRestore()
    sh script: '''#!/bin/bash
        unset _JAVA_OPTIONS
        # Set MAVEN_OPTS for Surefire launcher. Skip remoteresources to avoid intermittent connection timeouts when
        # resolving the SIGAR dependency.
        export BUILD_STATUS=0
        MAVEN_OPTS='-Xmx1100m' ${MVN} test -pl ${MAVEN_PROJECTS:-processing} \
            ${MAVEN_SKIP} -Dremoteresources.skip=true -Ddruid.generic.useDefaultValueForNull=${DRUID_USE_DEFAULT_VALUE_FOR_NULL:-true} \
            || export BUILD_STATUS=1

        ${MVN} -pl ${MAVEN_PROJECTS} jacoco:report
        exit $BUILD_STATUS
    ''',
    label: "processing module test"
    dockerCleanup()
}

//////////
////////// Pipeline part
//////////

pipeline {
    options {
        timeout(time: 150, unit: 'MINUTES')
        buildDiscarder(logRotator(artifactDaysToKeepStr: '15', artifactNumToKeepStr: '10', daysToKeepStr: '30', numToKeepStr: '20'))
    }

    parameters {
        booleanParam(name: 'SKIP_ALL_JENKINS_TESTS', defaultValue: false, description: 'Skip all jenkins tests defined in the jenkinsfile')
        booleanParam(name: 'SKIP_JENKINS_INTEGRATION_TESTS', defaultValue: false, description: 'Skip integration tests defined in the jenkinsfile')
        booleanParam(name: 'SKIP_JENKINS_UNIT_TESTS_AND_CHECKS', defaultValue: false, description: 'Skip unit test and checks defined in the jenkinsfile')
        booleanParam(name: 'PUBLISH_ON_ANY_BRANCH', defaultValue: false, description: 'Build and publish to artifactory regardless branch name')
    }

    agent none

    environment {
        MVN = "mvn -B"
        MAVEN_SKIP = "-Danimal.sniffer.skip=true -Dcheckstyle.skip=true -Ddruid.console.skip=true -Denforcer.skip=true -Dforbiddenapis.skip=true -Dmaven.javadoc.skip=true -Dpmd.skip=true -Dspotbugs.skip=true"
        MAVEN_SKIP_TESTS = "-DskipTests -Djacoco.skip=true"
        MAVEN_OPTS = "-Xms4g -Xmx8g -XX:MaxDirectMemorySize=2048m"
        DOCKER_IP = "127.0.0.1"
        ZK_VERSION = "3.5"
        APACHE_ARCHIVE_MIRROR_HOST = "https://repo.qa.imply.io/artifactory/archive-apache-org-remote"

        ACTIVE_DEV_BRANCH_REGEX = '^monthly$'
        RELEASE_BRANCH_PREFIX_REGEX = '^release\\/'
        RELEASE_BRANCH_SUFFIX_REGEX = '[^/]+$'

        IMAGE_JDK8 = "docker/druid-ci-jdk8:1616503008"
        IMAGE_JDK11 = "docker/druid-ci-jdk11:1616503008"
        DOCKER_TAG = "${BUILD_TAG}".replaceAll(~/[^A-Za-z0-9_-]/,'_')
        RT_REGISTRY_URL = "https://repo.qa.imply.io/artifactory"
        RT_REGISTRY_CREDS = "repo.qa.imply.io"
        RT_SERVER_ID = "repo-qa-imply-io"
        RT_DOCKER_REPOSITORY_PREFIX = "repo.qa.imply.io"
        DOCKER_REGISTRY_MIRROR = "https://registry-mirror.qa.imply.io:443"
    }

    stages {
        stage('Maven install') {
            when {
                expression { !params.SKIP_ALL_JENKINS_TESTS }
                beforeAgent true
            }
            environment {
                DOCKER_AGENT_ARGS = "-u root:root --runtime=sysbox-runc -e RUN_DOCKER=1"
                DOCKER_AGENT_LABEL = "jenkinsOnDemand"
            }
            parallel {
                stage('(openjdk8) maven install') {
                    agent {docker {image "${IMAGE_JDK8}"; args "${DOCKER_AGENT_ARGS}"; label "${DOCKER_AGENT_LABEL}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"}}
                    environment { CURRENT_DOCKER_IMAGE="${IMAGE_JDK8}" }
                    steps {
                        mavenInstall()
                        mavenDockerBackup()
                    }
                    post {
                        cleanup {
                            resetWs()
                            dockerCleanup()
                        }
                    }
                }
                stage('(openjdk11) maven install') {
                    agent {docker {image "${IMAGE_JDK11}"; args "${DOCKER_AGENT_ARGS}"; label "${DOCKER_AGENT_LABEL}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"}}
                    environment { CURRENT_DOCKER_IMAGE="${IMAGE_JDK11}" }
                    steps {
                        mavenInstall()
                        mavenDockerBackup()

                    }
                    post {
                        cleanup {
                            resetWs()
                            dockerCleanup()
                        }
                    }
                }
            }
        }

        stage('Unit tests and checks') {
            when {
                expression { !params.SKIP_JENKINS_UNIT_TESTS_AND_CHECKS && !params.SKIP_ALL_JENKINS_TESTS }
                beforeAgent true
            }
            environment {
                DOCKER_AGENT_ARGS = "-u root:root"
                DOCKER_AGENT_LABEL = "jenkinsOnDemandMultiExec"
            }
            parallel {
                stage('security vulnerabilities') {
                    agent {
                        docker {
                            image "${IMAGE_JDK8}-${DOCKER_TAG}"
                            args "${DOCKER_AGENT_ARGS}"
                            label "${DOCKER_AGENT_LABEL}"
                            registryUrl "${RT_REGISTRY_URL}"
                            registryCredentialsId "${RT_REGISTRY_CREDS}"
                        }
                    }
                    steps {
                        mavenCacheRestore()
                        sh script: '''#!/bin/bash
                            set -o pipefail

                            dependency_check() {
                                rm -f /tmp/component-reports-failed
                                touch /tmp/component-reports-failed
                                ${MVN} dependency-check:aggregate -pl '!integration-tests,!integration-tests-imply,!api-contract' 2>&1 | \
                                tee >(grep -F 'Failed to request component-reports' >> /tmp/component-reports-failed)
                            }

                            DEPENDENCY_CHECK_STATUS=0
                            ALL_ATTEMPTS_FAILED=0

                            for i in $(seq 1 10); do
                                if dependency_check; then
                                    echo "Dependency check succeeded"
                                    break
                                elif [[ $(cat /tmp/component-reports-failed) ]]; then
                                    if [[ "${i}" = "10" ]]; then
                                        echo "All attempts are failed"
                                        ALL_ATTEMPTS_FAILED=1
                                    else
                                        echo "Sleeping 30 seconds before next attempt"
                                        sleep 30
                                    fi
                                else
                                    echo "Dependency check failed not from component-reports issue. Failing"
                                    DEPENDENCY_CHECK_STATUS=1
                                    break
                                fi
                            done

                            [[ "${DEPENDENCY_CHECK_STATUS}" = "0" ]] || { echo "The OWASP dependency check has found security vulnerabilities. Please use a newer version
                            of the dependency that does not have vulnerabilities. If the analysis has false positives,
                            they can be suppressed by adding entries to owasp-dependency-check-suppressions.xml (for more
                            information, see https://jeremylong.github.io/DependencyCheck/general/suppression.html).
                            " && exit 1; }

                            [[ "${ALL_ATTEMPTS_FAILED}" = "0" ]] || false
                        ''',
                        label: 'dependency check'
                    }
                    post { cleanup { resetWs() } }
                }

                stage('animal sniffer checks') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { mavenCacheRestore(); sh "${MVN} animal-sniffer:check --fail-at-end" }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('checkstyle') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { mavenCacheRestore(); sh "${MVN} checkstyle:checkstyle --fail-at-end" }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('enforcer checks') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { mavenCacheRestore(); sh "${MVN} enforcer:enforce --fail-at-end" }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('forbidden api checks') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { mavenCacheRestore(); sh "${MVN} forbiddenapis:check forbiddenapis:testCheck --fail-at-end" }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('pmd checks') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { mavenCacheRestore(); sh "${MVN} pmd:check --fail-at-end" }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('spotbugs checks') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { mavenCacheRestore(); sh "${MVN} spotbugs:check --fail-at-end -pl '!benchmarks'" }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('license checks') {
                    agent {
                        docker {
                            image "${IMAGE_JDK8}-${DOCKER_TAG}"
                            args "${DOCKER_AGENT_ARGS}"
                            registryUrl "${RT_REGISTRY_URL}"
                            registryCredentialsId "${RT_REGISTRY_CREDS}"
                            label "jenkinsOnDemand"
                        }
                    }
                    steps {
                        mavenCacheRestore()
                        sh """
                            ${MVN} apache-rat:check -Prat --fail-at-end \
                                -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn \
                                -Drat.consoleOutput=true
                            # Generate dependency reports and checks they are valid. When running on Travis CI, 2 cores are available
                            # (https://docs.travis-ci.com/user/reference/overview/#virtualisation-environment-vs-operating-system).
                            mkdir -p target
                            distribution/bin/generate-license-dependency-reports.py . target --clean-maven-artifact-transfer --parallel 2
                            distribution/bin/check-licenses.py licenses.yaml target/license-reports
                        """
                    }
                    post {
                        always { buildArtifacts(env.STAGE_NAME) }
                        cleanup { resetWs() }
                    }
                }

                stage('(openjdk8) strict compilation') {
                    agent {
                        docker {
                            image "${IMAGE_JDK8}"
                            args "${DOCKER_AGENT_ARGS}"
                            registryUrl "${RT_REGISTRY_URL}"
                            registryCredentialsId "${RT_REGISTRY_CREDS}"
                            label "jenkinsOnDemand"
                        }
                    }
                    steps {
                        setupMavenSettings()
                        sh """
                            ${MVN} clean -Pstrict compile test-compile --fail-at-end \
                                -pl '!benchmarks' ${MAVEN_SKIP} ${MAVEN_SKIP_TESTS}
                        """
                    }
                    post {
                        always { buildArtifacts(env.STAGE_NAME) }
                        cleanup { resetWs() }
                    }
                }

                stage('analyze dependencies') {
                    agent {
                        docker {
                            image "${IMAGE_JDK8}-${DOCKER_TAG}"
                            args "${DOCKER_AGENT_ARGS}"
                            registryUrl "${RT_REGISTRY_URL}"
                            registryCredentialsId "${RT_REGISTRY_CREDS}"
                            label "jenkinsOnDemand"
                        }
                    }
                    steps {
                        mavenCacheRestore()
                        sh """
                            MAVEN_OPTS='-Xmx3000m' ${MVN} ${MAVEN_SKIP} dependency:analyze -DoutputXML=true -DignoreNonCompile=true -DfailOnWarning=true || { echo "

                            The dependency analysis has found a dependency that is either:

                            1) Used and undeclared: These are available as a transitive dependency but should be explicitly
                            added to the POM to ensure the dependency version. The XML to add the dependencies to the POM is
                            shown above.

                            2) Unused and declared: These are not needed and removing them from the POM will speed up the build
                            and reduce the artifact size. The dependencies to remove are shown above.

                            If there are false positive dependency analysis warnings, they can be suppressed:
                            https://maven.apache.org/plugins/maven-dependency-plugin/analyze-mojo.html#usedDependencies
                            https://maven.apache.org/plugins/maven-dependency-plugin/examples/exclude-dependencies-from-dependency-analysis.html

                            For more information, refer to:
                            https://maven.apache.org/plugins/maven-dependency-plugin/analyze-mojo.html

                            " && false; }
                        """
                    }
                    post {
                        always { buildArtifacts(env.STAGE_NAME) }
                        cleanup { resetWs() }
                    }
                }

                stage('intellij inspections') {
                    agent {
                        docker {
                            image "${IMAGE_JDK8}-${DOCKER_TAG}"
                            args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"
                            registryUrl "${RT_REGISTRY_URL}"
                            registryCredentialsId "${RT_REGISTRY_CREDS}"
                            label "${DOCKER_AGENT_LABEL}"
                        }
                    }
                    steps {
                        mavenCacheRestore()
                        sh script:'''
                            mkdir -p inspection-results
                            chmod -R 777 inspection-results
                        ''',
                        label: 'prepare output directory'

                        sh script:'''
                            docker run --rm \
                            -v ${WORKSPACE}:/project \
                            -v ${HOME}/.m2/repository:/home/inspect/.m2/repository \
                            -v ${WORKSPACE}/inspection-results:/home/inspect/inspection-results \
                            ccaominh/intellij-inspect:1.0.0 \
                            /project/pom.xml \
                            /project/.idea/inspectionProfiles/Druid.xml \
                            --levels ERROR \
                            --scope JavaInspectionsScope
                        ''',
                        label: 'intellij inspections'
                    }
                    post {
                        always { buildArtifacts(env.STAGE_NAME) }
                        cleanup { resetWs() }
                    }
                }

                stage('(openjdk11) packaging check') {
                    agent {
                        docker {
                            image "${IMAGE_JDK11}-${DOCKER_TAG}"
                            args "${DOCKER_AGENT_ARGS}"
                            registryUrl "${RT_REGISTRY_URL}"
                            registryCredentialsId "${RT_REGISTRY_CREDS}"
                            label "${DOCKER_AGENT_LABEL}"
                        }
                    }
                    steps {
                        setupMavenSettings()
                        sh script:'''
                            MAVEN_OPTS='-Xmx3000m' ${MVN} clean install -Prat -Pdist -Pbundle-contrib-exts --fail-at-end \
                                -pl '!benchmarks' ${MAVEN_SKIP} ${MAVEN_SKIP_TESTS} -Ddruid.console.skip=false -T1C
                        '''
                    }
                    post {
                        always { buildArtifacts(env.STAGE_NAME) }
                        cleanup { resetWs() }
                    }
                }

                stage('web-console') {
                    agent {
                        docker {
                            image "${IMAGE_JDK8}-${DOCKER_TAG}"
                            args "${DOCKER_AGENT_ARGS}"
                            registryUrl "${RT_REGISTRY_URL}"
                            registryCredentialsId "${RT_REGISTRY_CREDS}"
                            label "${DOCKER_AGENT_LABEL}"
                        }
                    }
                    steps {
                        setupMavenSettings()
                        sh script:'''
                            ${MVN} test -pl 'web-console'
                        '''
                    }
                    post {
                        always { buildArtifacts(env.STAGE_NAME) }
                        cleanup { resetWs() }
                        success {
                            sh script:'''
                                cd web-console
                                { for i in 1 2 3; do npm run codecov && break || sleep 15; done }
                            '''
                        }
                    }
                }

                stage('docs') {
                    agent {
                        docker {
                            image "${IMAGE_JDK8}"
                            args "${DOCKER_AGENT_ARGS}"
                            registryUrl "${RT_REGISTRY_URL}"
                            registryCredentialsId "${RT_REGISTRY_CREDS}"
                            label "${DOCKER_AGENT_LABEL}"
                        }
                    }
                    steps {
                        sh script:'''
                            (cd website && npm install)
                            (cd website && npm run lint && npm run spellcheck) || { echo "

                            If there are spell check errors:

                            1) Suppressing False Positives: Edit website/.spelling to add suppressions. Instructions
                            are at the top of the file and explain how to suppress false positives either globally or
                            within a particular file.

                            2) Running Spell Check Locally: cd website && npm install && npm run spellcheck

                            For more information, refer to: https://www.npmjs.com/package/markdown-spellcheck

                            " && false; }
                        '''
                    }
                    post {
                        always { buildArtifacts(env.STAGE_NAME) }
                        cleanup { resetWs() }
                    }
                }

                stage('(openjdk11) processing module test') {
                    agent {docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "jenkinsOnDemand" }}
                    environment {
                        MAVEN_PROJECTS="processing"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="true"
                    }
                    steps { processingModuleTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) processing module test (SQL Compatibility)') {
                    agent {docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "jenkinsOnDemand" }}
                    environment {
                        MAVEN_PROJECTS="processing"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="false"
                    }
                    steps { processingModuleTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) indexing modules test') {
                    agent {docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "jenkinsOnDemand" }}
                    environment {
                        MAVEN_PROJECTS="indexing-hadoop,indexing-service,extensions-core/kafka-indexing-service,extensions-core/kinesis-indexing-service"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="true"
                    }
                    steps { processingModuleTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) indexing modules test (SQL Compatibility)') {
                    agent {docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "jenkinsOnDemand" }}
                    environment {
                        MAVEN_PROJECTS="indexing-hadoop,indexing-service,extensions-core/kafka-indexing-service,extensions-core/kinesis-indexing-service"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="false"
                    }
                    steps { processingModuleTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) server module test') {
                    agent {docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "jenkinsOnDemand" }}
                    environment {
                        MAVEN_PROJECTS="server"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="true"
                    }
                    steps { processingModuleTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) server module test (SQL Compatibility)') {
                    agent {docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "jenkinsOnDemand" }}
                    environment {
                        MAVEN_PROJECTS="server"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="false"
                    }
                    steps { processingModuleTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) other modules test') {
                    agent {docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "jenkinsOnDemand" }}
                    environment {
                        MAVEN_PROJECTS="!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests,!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="true"
                    }
                    steps { processingModuleTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) other modules test (SQL Compatibility)') {
                    agent {docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "jenkinsOnDemand" }}
                    environment {
                        MAVEN_PROJECTS="!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests,!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="false"
                    }
                    steps { processingModuleTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }
            }
        }
        stage('Integration tests') {
            when {
                expression { !params.SKIP_JENKINS_INTEGRATION_TESTS && !params.SKIP_ALL_JENKINS_TESTS }
                beforeAgent true
            }
            environment {
                DOCKER_AGENT_ARGS = "-u root:root --runtime=sysbox-runc -e RUN_DOCKER=1"
                DOCKER_AGENT_LABEL = "jenkinsOnDemand"
            }
            parallel {
                // example of integration tests stage:
                stage('(Compile=openjdk8, Run=openjdk8) imply query integration tests') {
                    agent {
                        docker {
                            image "${IMAGE_JDK8}-${DOCKER_TAG}"
                            args "${DOCKER_AGENT_ARGS}"
                            registryUrl "${RT_REGISTRY_URL}"
                            registryCredentialsId "${RT_REGISTRY_CREDS}"
                            label "${DOCKER_AGENT_LABEL}"
                        }
                    }
                    steps {
                        // integration tests here:
                        // jvm runtime is specified here:
                        implyQueryIntegrationTests('-Djvm.runtime=8')
                    }
                    post {
                        // collect artifacts (docker logs and task logs):
                        always { buildArtifacts(env.STAGE_NAME) }
                        // clean untracked files from cloned repo:
                        cleanup { resetWs() }
                    }
                }

                // here is short format of integration tests stage:
                stage('(Compile=openjdk8, Run=openjdk8) imply ingest service integration tests') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { implyIngestServiceIntegrationTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                // here is short format of integration tests stage:
                stage('(Compile=openjdk8, Run=openjdk8) imply keycloak security integration tests') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image "${IMAGE_JDK8}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { implyKeycloakSecurityIntegrationTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) s3 deep storage test') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { s3DeepStorageTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) azure deep storage test') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { azureDeepStorageTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) hdfs deep storage test') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { hdfsDeepStorageTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) gcs deep storage test') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { gcsDeepStorageTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) kinesis deep storage test') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { kinesisDeepStorageTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                // -Djvm.runtime=11

                stage('(Compile=openjdk8, Run=openjdk11) imply query integration tests') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { implyQueryIntegrationTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) imply ingest service integration tests') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { implyIngestServiceIntegrationTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) imply keycloak security integration tests') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image "${IMAGE_JDK8}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { implyKeycloakSecurityIntegrationTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) s3 deep storage test') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { s3DeepStorageTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) azure deep storage test') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { azureDeepStorageTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) hdfs deep storage test') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { hdfsDeepStorageTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) gcs deep storage test') {
                    agent {docker {image "${IMAGE_JDK8}-${DOCKER_TAG}";args "${DOCKER_AGENT_ARGS}";registryUrl "${RT_REGISTRY_URL}";registryCredentialsId "${RT_REGISTRY_CREDS}";label "${DOCKER_AGENT_LABEL}"}}
                    steps { gcsDeepStorageTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }
            } // end of parallel
        } // end of stage('Checks')

        stage("Build and publish") {
            when {
                anyOf {
                    expression { (env.CHANGE_BRANCH ?: env.BRANCH_NAME) ==~ env.ACTIVE_DEV_BRANCH_REGEX }
                    expression { (env.CHANGE_BRANCH ?: env.BRANCH_NAME) ==~ (env.RELEASE_BRANCH_PREFIX_REGEX + env.RELEASE_BRANCH_SUFFIX_REGEX) }
                    expression { params.PUBLISH_ON_ANY_BRANCH }
                }
                beforeAgent true
            }

            matrix {
                axes {
                    axis {
                        name 'BUILD_PROFILE'
                        values 'dist', 'imply-saas'
                    }
                }
                stages {
                    stage('profile build and upload') {
                        environment {
                            GIT_ASKPASS = "/tmp/askpass.sh"
                            GITHUB_NETRC = "/tmp/github-netrc"
                            STAGING_BASE_DIR= "/tmp/druid-build/stage"
                            TMP_DIR = "/tmp/druid-build/tmp"
                            BUILD_DIR = "/tmp/druid-build"
                            ARTIFACT_NAME = sh(script:"""#!/bin/bash
                                if [ "${BUILD_PROFILE}" == "imply-saas" ]; then
                                    echo "druid-saas"
                                else
                                    echo "druid"
                                fi
                            """, returnStdout: true).trim()
                            ARTIFACTORY_BUILD_NAME = sh(script:"""#!/bin/bash
                                if [ "${BUILD_PROFILE}" == "imply-saas" ]; then
                                    echo "druid.saas"
                                else
                                    echo "druid"
                                fi
                            """, returnStdout: true).trim()
                        }
                        agent {
                            docker {
                                image 'docker/buildabear:20200923'
                                args '-u root:root'
                                label 'jenkinsOnDemand'
                                registryUrl "${RT_REGISTRY_URL}"
                                registryCredentialsId "${RT_REGISTRY_CREDS}"
                            }
                        }
                        stages {
                            stage('prepare to build') {
                                steps {
                                    script {
                                        def gitCredsId = scm.getUserRemoteConfigs()[0].getCredentialsId()
                                        withCredentials([usernamePassword(credentialsId: gitCredsId, usernameVariable: 'GIT_CREDS_USR', passwordVariable: 'GIT_CREDS_PSW')]) {
                                            sh script:'''#!/bin/bash -eux
                                                echo -e '#!/bin/bash\nPATH=/opt/maven/apache-maven-3.5.4/bin:$PATH exec mvn -B $@' > /usr/bin/mvn
                                                chmod u+x /usr/bin/mvn

                                                cat <<EOF > "${GIT_ASKPASS}"
                                                #!/bin/sh
                                                case "\\$1" in
                                                    Username*) echo "${GIT_CREDS_USR}" ;;
                                                    Password*) echo "${GIT_CREDS_PSW}" ;;
                                                esac
                                                EOF

                                                cat <<EOF > "${GITHUB_NETRC}"
                                                machine api.github.com
                                                login ${GIT_CREDS_USR}
                                                password ${GIT_CREDS_PSW}
                                                EOF

                                                chmod u+x "${GIT_ASKPASS}"
                                                chmod 400 "${GITHUB_NETRC}"
                                            '''.replaceAll(/\n\s+/, "\n"),
                                            label: 'prepare for build'
                                        }
                                    }
                                }
                            }
                            stage('build') {
                                steps {
                                    setupMavenSettings()
                                    sh script:'''#!/bin/bash -eux
                                        COMMIT_SHA=$(git rev-parse HEAD)
                                        if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                            UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                        else
                                            COMMIT_SHA=$(git rev-parse HEAD^)
                                            if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                                UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                            else
                                                echo "head branch commit SHA is not found"
                                                exit 1
                                            fi
                                        fi

                                        NETTY_UPSTREAM_NAME="netty"
                                        NETTY_UPSTREAM_ORG=${NETTY_UPSTREAM_ORG:-"implydata"}
                                        NETTY_UPSTREAM_REPO="https://github.com/$NETTY_UPSTREAM_ORG/$NETTY_UPSTREAM_NAME.git"
                                        NETTY_UPSTREAM_VERSION=${NETTY_UPSTREAM_VERSION:-"3.10.6.Final-iap2"}
                                        NETTY_UPSTREAM_COMMITISH=${NETTY_UPSTREAM_COMMITISH:-"netty-$NETTY_UPSTREAM_VERSION"}
                                        NETTY_UPSTREAM_DIR="$TMP_DIR/$NETTY_UPSTREAM_NAME.git"

                                        DERBYTOOLS_VERSION="10.11.1.1"

                                        # Imply includes additional hadoop libraries (e.g., hadoop-aws)
                                        HADOOP_VERSION="2.8.5"

                                        # This should match the version used in druid
                                        MYSQL_CONNECTOR_VERSION="5.1.48"

                                        # Install our patched version of Netty.
                                        git clone -b "$NETTY_UPSTREAM_COMMITISH" --single-branch --depth 1 "$NETTY_UPSTREAM_REPO" "$NETTY_UPSTREAM_DIR"
                                        (cd "$NETTY_UPSTREAM_DIR" && git checkout "$NETTY_UPSTREAM_COMMITISH")
                                        (cd "$NETTY_UPSTREAM_DIR" && mvn install -DskipTests)

                                        cd $WORKSPACE

                                        # remove snapshot suffix from pom files
                                        INIT_DRUID_VERSION=$(mvn org.apache.maven.plugins:maven-help-plugin:3.2.0:evaluate -Dexpression=project.version -q -DforceStdout)
                                        DRUID_VERSION=${INIT_DRUID_VERSION%%-SNAPSHOT}
                                        mvn versions:set -DnewVersion=$DRUID_VERSION  -DgenerateBackupPoms=false

                                        current_java_version=$(java -version 2>&1 >/dev/null | grep 'version' | awk '{print $3}')
                                        echo "Compiling Druid with current Java version set. Java version=$current_java_version"

                                        mvn clean install \
                                            -Dnetty3.version="$NETTY_UPSTREAM_VERSION" \
                                            -Dhadoop.compile.version="$HADOOP_VERSION" \
                                            -Dmysql.version="$MYSQL_CONNECTOR_VERSION" \
                                            -Danimal.sniffer.skip=true \
                                            -Dcheckstyle.skip=true \
                                            -Denforcer.skip=true \
                                            -Dforbiddenapis.skip=true \
                                            -Djacoco.skip=true \
                                            -Dmaven.javadoc.skip=true \
                                            -Dpmd.skip=true \
                                            -Dspotbugs.skip=true \
                                            -DskipTests \
                                            -T1C \
                                            -Dtar \
                                            -P${BUILD_PROFILE}

                                        echo "Built Druid, version: $DRUID_VERSION"

                                        # Stage Druid
                                        export STAGING_DIR=${STAGING_BASE_DIR}/${BUILD_PROFILE}
                                        mkdir -p "$STAGING_DIR/dist"
                                        tar -C "$TMP_DIR" -xzf "$WORKSPACE"/distribution/target/apache-druid-*-bin.tar.gz
                                        mv "$TMP_DIR"/apache-druid-* "$STAGING_DIR/dist/druid"

                                        # Fetch the MySQL JDBC driver from Maven Central
                                        MYSQL_CONNECTOR_LOCATION="$STAGING_DIR/dist/druid/extensions/mysql-metadata-storage/mysql-connector-java-${MYSQL_CONNECTOR_VERSION}.jar"
                                        mkdir -p "$STAGING_DIR/dist/druid/extensions/mysql-metadata-storage"
                                        curl -o "$MYSQL_CONNECTOR_LOCATION" --retry 10 "https://repo1.maven.org/maven2/mysql/mysql-connector-java/${MYSQL_CONNECTOR_VERSION}/mysql-connector-java-${MYSQL_CONNECTOR_VERSION}.jar"

                                        if [ "$(sha1sum "$MYSQL_CONNECTOR_LOCATION" | awk '{print $1}')" != "9140be77aafa5050bf4bb936d560cbacb5a6b5c1" ]
                                        then
                                          echo "$MYSQL_CONNECTOR_LOCATION: checksum mismatch" >&2
                                          exit 1
                                        fi

                                        # Add hadoop-aws
                                        (cd "$STAGING_DIR"/dist/druid && java -classpath "lib/*" org.apache.druid.cli.Main tools pull-deps -h "org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}")

                                        # Add derbytools to lib
                                        DERBYTOOLS_LOCATION="$STAGING_DIR/dist/druid/lib/derbytools-$DERBYTOOLS_VERSION.jar"
                                        curl -Lo "$DERBYTOOLS_LOCATION" "https://search.maven.org/remotecontent?filepath=org/apache/derby/derbytools/$DERBYTOOLS_VERSION/derbytools-$DERBYTOOLS_VERSION.jar"

                                        if [ "$(sha1sum "$DERBYTOOLS_LOCATION" | awk '{print $1}')" != "10a124a8962c6f8ea70a368d711ce6883889ca34" ]
                                        then
                                          echo "$DERBYTOOLS_LOCATION: checksum mismatch" >&2
                                          exit 1
                                        fi

                                        # Remove unsupported open-source extensions
                                        for extension in druid-pac4j druid-ranger-security materialized-view-maintenance materialized-view-selection; do
                                            rm -rf "${STAGING_DIR}/dist/druid/extensions/${extension}"
                                        done

                                        # Put druid on a diet
                                        perl - <<'EOT'
                                        use strict;
                                        use File::Basename;

                                        my $dir = "$ENV{STAGING_DIR}/dist/druid";
                                        chdir $dir or die "chdir $dir: $!";

                                        my %jars;
                                        for my $jar (qx!find ./lib -name '*.jar'!, qx!find ./extensions -name '*.jar'!, qx!find ./hadoop-dependencies -name '*.jar'!) {
                                          chomp $jar;
                                          my $jarname = basename($jar);
                                          if (exists $jars{$jarname}) {
                                            my $depth = $jar =~ tr !/!/! - 1;
                                            my $dots = "";
                                            for my $x (1..$depth) {
                                              $dots .= "../";
                                            }
                                            system("ln", "-sf", "${dots}$jars{$jarname}", $jar);
                                          } else {
                                            $jars{$jarname} = $jar;
                                          }
                                        }
                                        EOT

                                        tar -C $STAGING_DIR/dist -czf $WORKSPACE/${ARTIFACT_NAME}-${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}.tar.gz druid

                                        echo "${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}" > $WORKSPACE/build.version

                                    '''.replaceAll(/\n\s+/, "\n"),
                                    label: 'build druid'
                                } // end of steps
                            } // end of stage('build')

                            stage('upload') {
                                steps {
                                    script {
                                        def release_version = "None"
                                        def headBranch = env.CHANGE_BRANCH ?: env.BRANCH_NAME
                                        if(headBranch ==~ (env.RELEASE_BRANCH_PREFIX_REGEX + env.RELEASE_BRANCH_SUFFIX_REGEX)) {
                                            release_version = (headBranch =~ env.RELEASE_BRANCH_SUFFIX_REGEX).getAt(0)
                                        } else if(headBranch ==~ env.ACTIVE_DEV_BRANCH_REGEX) {
                                            release_version = sh(
                                                script: '''#!/bin/bash -e
                                                    curl -s --netrc-file ${GITHUB_NETRC} \
                                                    -H "Accept: application/vnd.github.v3.raw" \
                                                    https://api.github.com/repos/implydata/imply-release/contents/monthly-release.version |  \
                                                    tee /dev/stderr
                                                ''',
                                                returnStdout: true
                                            ).trim()
                                        }
                                        def artifact_build_version = sh(
                                            script: 'cat build.version',
                                            returnStdout: true
                                        ).trim()
                                        def repo_url = sh(
                                            script: 'git config --get remote.origin.url',
                                            returnStdout: true
                                        ).trim()
                                        rtUpload (
                                            serverId: "${RT_SERVER_ID}",
                                            spec: """{
                                                "files": [
                                                    {
                                                      "pattern": "${ARTIFACT_NAME}-${artifact_build_version}.tar.gz",
                                                      "target": "tgz-local/${ARTIFACT_NAME}/",
                                                      "props": "release.version=${release_version};BVT=Pass;build.url=${BUILD_URL};vcs.url=${repo_url};vcs.branch=${headBranch};build.version=${artifact_build_version}"
                                                    }
                                                ]
                                            }""",
                                            buildName: "${ARTIFACTORY_BUILD_NAME}",
                                            buildNumber: "${BUILD_NUMBER}"
                                        )
                                    }
                                }
                            } // end of stage('upload')
                        } // end of stages
                        post {
                            cleanup {
                                resetWs()
                            }
                        }
                    } // end of stage('profile build and upload')
                } // end of stages
            } // end of matrix
        } // end of stage("Build and publish")
    } // end of stages
    post {
        failure {
            // notify about failed build
            script {
                def failedStageNames = getFailedStages( currentBuild ).collect { it.displayName }.findAll { !(it =~ /^(Unit|Integration|Matrix)/) }
                if(!currentBuild.previousCompletedBuild || currentBuild.previousCompletedBuild?.result == "SUCCESS") {
                    if(isEligibleToNotify()) { notify(false, failedStageNames) }
                } else {
                    def failedBuildSilencePeriodInHours = 0
                    def failedBuildSilencePeriodInMillis = 1000 * 60 * 60 * failedBuildSilencePeriodInHours
                    def nowInMillis = new Date().getTime()
                    def previousCompletedBuildStarted = currentBuild.previousCompletedBuild?.startTimeInMillis ?: nowInMillis
                    def previousCompletedBuildDuration = currentBuild.previousCompletedBuild?.duration ?: 0
                    def previousCompletedBuildFinished = previousCompletedBuildStarted + previousCompletedBuildDuration
                    if(previousCompletedBuildFinished < nowInMillis-failedBuildSilencePeriodInMillis) {
                        if(isEligibleToNotify()) { notify(false, failedStageNames) }
                    }
                }
            }
        }
        success {
            // notify about fixed build
            script {
                if(!!currentBuild.previousCompletedBuild && currentBuild.previousCompletedBuild?.result != "SUCCESS") {
                    if(isEligibleToNotify()) { notify(true, []) }
                }
            }
        }
    }
}