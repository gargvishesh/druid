import org.jenkinsci.plugins.pipeline.modeldefinition.Utils

def MVN                             = "mvn -B"
def MVN_OPTS                        = "export MAVEN_OPTS=\"-Xms4g -Xmx8g -XX:MaxDirectMemorySize=2048m\""
def MAVEN_SKIP                      = "-Danimal.sniffer.skip=true -Dcheckstyle.skip=true -Ddruid.console.skip=true -Denforcer.skip=true -Dforbiddenapis.skip=true -Dmaven.javadoc.skip=true -Dpmd.skip=true -Dspotbugs.skip=true"
def MAVEN_SKIP_TESTS                = "-DskipTests -Djacoco.skip=true"
def M2_CACHE_IMAGE                  = "018895040333.dkr.ecr.us-east-1.amazonaws.com/druid-m2-cache"
def BUILD_CACHE_IMAGE               = "018895040333.dkr.ecr.us-east-1.amazonaws.com/druid-build-cache"
def AWS_CREDS_ID                    = "aws"
def AZURE_CREDS_ID                  = "azure_credentials"
def GCS_CREDS_ID                    = "gcs-bucket-qa"
def BUILD_CACHE_TAG                 = env.BUILD_TAG.toLowerCase().replaceAll(~/[^a-z0-9-]/,'')
def MVN_SETTINGS_FILE               = 'maven-artifactory-settings'
def COMMIT_STATUS_INTERVAL          = 60
def TRAVIS_UNTIL_SMTH_MAX_ATTEMPTS  = 10
def TRAVIS_UNTIL_PASS_MAX_ATTEMPTS  = 200
def TRAVIS_SLUG_ID                  = "travis-ci"
def MVN_INSTALL_CACHE_EXCLUDE       = "web-console"
def ARTIFACTORY_SERVER_ID           = "repo-qa-imply-io"
def ARTIFACTORY_REPO                = "tgz-local"
def ACTIVE_DEV_BRANCH_REGEX         = /^0\.20\.0-iap$/
def RELEASE_BRANCH_PREFIX_REGEX     = /^release\//
def RELEASE_BRANCH_SUFFIX_REGEX     = /\d{4}\.\d{2}([-.]\d+)?$/


// global variables
String REPO_URL
String REPO_NAME
String REPO_OWNER
String HEAD_COMMIT_SHA
String HEAD_BRANCH_NAME
boolean SKIP_PUBLISH_STAGES

properties([
    parameters([
        booleanParam(name: 'PUBLISH_ON_ANY_BRANCH', defaultValue: false, description: 'Build and publish to artifactory regardless branch name'),
        booleanParam(name: 'SKIP_ALL_JENKINS_TESTS', defaultValue: false, description: 'Skip all jenkins tests defined in the jenkinsfile. Does not apply to travis tests'),
        booleanParam(name: 'SKIP_JENKINS_INTEGRATION_TESTS', defaultValue: false, description: 'Skip integration tests defined in the jenkinsfile. Does not apply to travis tests'),
        string(name: 'BUILD_DOCKER_IMAGE', defaultValue: '269875963461.dkr.ecr.us-east-1.amazonaws.com/buildabear:20200923', description: 'ECR image with dependencies')
    ])
])

def dockerInitCleanup = {
    sh script: "for container in \$(docker ps -q); do docker stop \$container; done"
    sh script: "docker container prune -f"
    sh script: "docker volume prune -f"
    sh script: "docker network prune -f"
}

def workspaceCleanup = {
    sh script: "sudo git clean -fdx", label: "Remove everything but files from git"
}

def withArtifactorySettings = { body ->
    withCredentials([file(credentialsId: MVN_SETTINGS_FILE, variable: 'MVN_SETTINGS_PATH')]) {
        sh script: '''
            mkdir -p ~/.m2
            ln -sf $MVN_SETTINGS_PATH ~/.m2/settings.xml
        ''',
        label: 'setting mvn settings symlink'
        try {
            body()
        } finally {
            sh script: 'rm -f ~/.m2/settings.xml', label: 'mvn settings symlink cleanup'
        }
    }
}

def withAWSCredentials = { body ->
    withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: AWS_CREDS_ID, secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
        body()
    }
}

def withAzureCredentials = { body ->
    withCredentials([usernamePassword(credentialsId: AZURE_CREDS_ID, usernameVariable: 'AZURE_ACCOUNT', passwordVariable: 'AZURE_KEY')]) {
        body()
    }
}

def withGCSCredentials = { body ->
    withCredentials([file(credentialsId: GCS_CREDS_ID, variable: 'GC_KEY')]) {
        body()
    }
}

def withCurrentGitCredentials = { body ->
    withCredentials([usernamePassword(credentialsId: scm.getUserRemoteConfigs()[0].getCredentialsId(), passwordVariable: 'GIT_PASSWORD', usernameVariable: 'GIT_USERNAME')]) {
        body(env.GIT_USERNAME, env.GIT_PASSWORD)
    }
}

def isTimeoutException = { err ->
    err instanceof org.jenkinsci.plugins.workflow.steps.FlowInterruptedException &&
    err.causes.collect {it.getClass()}.contains(org.jenkinsci.plugins.workflow.steps.TimeoutStepExecution.ExceededTimeout)
}

def isShellFailedException = { err ->
    err instanceof hudson.AbortException &&
    "${err}" =~ /script\s+returned\s+exit\s+code/
}

def shellOutput = { exec ->
    sh(script: exec, returnStdout: true, label: "getting output from shell: ${exec}").trim()
}

def mavenInstall = {
    sh script: """
        ${MVN_OPTS} && ${MVN} clean install -q -ff ${MAVEN_SKIP} ${MAVEN_SKIP_TESTS} -T 1C
    """,
    label: "maven install"
}

def skipStageIf = { boolean expression, stageName, body ->
    def config = [:]
    body.resolveStrategy = Closure.OWNER_FIRST
    body.delegate = config

    if (expression) {
        echo "Skipping ${stageName}..."
        Utils.markStageSkippedForConditional(stageName)
    } else {
        body()
    }
}

def retryOnNonEmpty = { wsPath, pause, maxAttempts, body ->
    def outputFilePath = "${wsPath}@tmp/.retryOutput"
    def attemptNumber = 1
    def linesCount = "0"
    sh script: """
        mkdir -p "\$(dirname ${outputFilePath})"
        rm -f ${outputFilePath}
        touch ${outputFilePath}
    """, label: 'prepare output file'

    while(true) {
        try {
            body(outputFilePath)
            break
        } catch(err) {
            if (isShellFailedException(err)) {
                linesCount = shellOutput("cat ${outputFilePath} | wc -l | tee /dev/stderr")
                if (linesCount == "0") {
                    echo "Attempt failed and output file is empty. Failing"
                    throw err
                }
                if (attemptNumber >= maxAttempts) {
                    echo "All ${maxAttempts} attempt failed"
                    throw err
                }
                echo "sleeping ${pause} seconds before next attempt"
                sleep pause
                attemptNumber = attemptNumber + 1
                continue
            } else {
                throw err
            }
        }
    }
}

def copyFilesByPattern = { filesPattern, destDir ->
    sh script: """#!/bin/bash -x
        shopt -s globstar
        if  ls ${filesPattern}; then
            for fname in ${filesPattern}; do
                fname_root=\$(echo \${fname} | cut -d "/" -f1)
                if ! [[ "\$fname_root" = "${destDir}" ]]; then
                    mkdir -p \$(dirname -- "${destDir}/\${fname}")
                    cp \$fname ${destDir}/\${fname}
                fi
            done
        fi
    """
}

def inspectException = { body ->
    try {
        body()
    } catch(err) {
        echo "Exception: ${err}"
        def causeClasses = err.causes.collect {it.getClass()}
        echo "Exception causes: ${causeClasses}"
        throw err
    }
}

def artifactsWrapper = { body ->
    try {
        body()
    } finally {
        def stageArtifactsDirPath = "stage_${env.STAGE_NAME.replaceAll(~/[^A-Za-z0-9_-]/,'_')}"
        sh script: "sudo mkdir -p /root/shared"
        sh script: "mkdir -p ${stageArtifactsDirPath}"
        // copy logs
        sh script: "sudo bash -c '[ -d \"/root/shared/logs\" ] && cp -R /root/shared/logs ${stageArtifactsDirPath}/ || true'"
        sh script: "sudo bash -c '[ -d \"/root/shared/tasklogs\" ] && cp -R /root/shared/tasklogs ${stageArtifactsDirPath}/ || true'"
        // copy test reports
        copyFilesByPattern("**/target/surefire-reports/*.xml", stageArtifactsDirPath)
        copyFilesByPattern("**/target/failsafe-reports/*.xml", stageArtifactsDirPath)
        // copy top-level jacoco reports
        copyFilesByPattern("**/target/*.exec", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/*.html", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/*.xml", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/*.csv", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/jacoco-resources/*", stageArtifactsDirPath)
        // copy detailed jacoco reports
        copyFilesByPattern("**/jacoco/**/*.html", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/**/*.xml", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/**/*.csv", stageArtifactsDirPath)
        // fixate artifacts
        archiveArtifacts artifacts: "${stageArtifactsDirPath}/**", allowEmptyArchive: true
    }
}

def heavyNode = { body ->
    node('jenkinsOnDemand') {
        artifactsWrapper {
            body()
        }
    }
}

def lightweightNode = { body ->
    node('jenkinsOnDemandMultiExec') {
        artifactsWrapper {
            body()
        }
    }
}

def ecrLogin = { imageURL ->
    def registryHost = (imageURL =~ /^[0-9a-z.-]+/).getAt(0)
    withAWSCredentials {
        sh script: """
            PATH=/home/jenkins/bin:/home/jenkins/.local/bin:\$PATH \
            aws ecr get-login-password --region us-east-1 | \
            docker login --username AWS --password-stdin ${registryHost}
        """
    }
}

def withM2Cache = { body ->
    def destination = "${env.WORKSPACE}@tmp/.m2Cache"
    def buildContext = "${env.WORKSPACE}@tmp/.m2CacheBuildContext"
    ecrLogin(M2_CACHE_IMAGE)
    try {
        timeout(time: 10, unit: 'MINUTES') {
            sh script: """
                docker pull ${M2_CACHE_IMAGE}:latest && \
                rm -rf ${buildContext} && mkdir -p ${buildContext} && \
                echo "FROM ${M2_CACHE_IMAGE}:latest" > ${buildContext}/Dockerfile && \
                DOCKER_BUILDKIT=1 docker build --output type=local,dest=${destination} ${buildContext}
            """,
            label: "pull ${M2_CACHE_IMAGE}:latest to local .m2 repository"
        }
    } catch(err) {
        if (isTimeoutException(err) || isShellFailedException(err)) {
            sh script: "mkdir -p ${destination}/.m2", label: "creating empty .m2 dir anyway"
        } else {
            throw err
        }
    } finally {
        sh script: "sudo chmod a+w ${destination}/.m2", label: "setting .m2 permissions"
    }
    body(destination)
}

def uploadM2Cache = { cacheDirPath ->
    def dockerfilePath = "${cacheDirPath}/m2CacheDockerfile"
    ecrLogin(M2_CACHE_IMAGE)
    sh script: """
        cd ${cacheDirPath}
        echo "FROM scratch" > ${dockerfilePath}
        echo "ADD .m2 /.m2" >> ${dockerfilePath}
        docker build -t ${M2_CACHE_IMAGE}:${BUILD_CACHE_TAG} -f ${dockerfilePath} .
        docker tag ${M2_CACHE_IMAGE}:${BUILD_CACHE_TAG} ${M2_CACHE_IMAGE}:latest
        docker push ${M2_CACHE_IMAGE}:${BUILD_CACHE_TAG}
        docker push ${M2_CACHE_IMAGE}:latest
        docker rmi ${M2_CACHE_IMAGE}:${BUILD_CACHE_TAG} ${M2_CACHE_IMAGE}:latest
    """
}

def downloadInstallCache = { wsPath ->
    def buildContext = "${wsPath}@tmp/.cacheBuildContext"
    ecrLogin(BUILD_CACHE_IMAGE)
    dir(wsPath) {
        try {
            timeout(time: 10, unit: 'MINUTES') {
                sh script: """
                    rm -rf ${buildContext}
                    mkdir -p ${buildContext}
                    echo "FROM ${BUILD_CACHE_IMAGE}:${BUILD_CACHE_TAG}" > ${buildContext}/Dockerfile
                    DOCKER_BUILDKIT=1 docker build --output type=local,dest=\$(pwd) ${buildContext}
                """,
                label: "pull ${BUILD_CACHE_IMAGE}:${BUILD_CACHE_TAG} to workspace"
            }
        } catch(err) {
            if (isTimeoutException(err)) {
                mavenInstall()
            } else {
                throw err
            }
        }
    }
}

def uploadInstallCache = { wsPath ->
    def contextPath = "${wsPath}@tmp/.buildcachecontext"
    def dockerfileName = "BuildCacheDockerfile"

    sh script: """
        rm -rf ${contextPath}
        mkdir -p ${contextPath}

        cd ${wsPath}
        git status --ignored --porcelain | \
            egrep '^\\!\\!' | \
            egrep -v '${MVN_INSTALL_CACHE_EXCLUDE}' | \
            sed 's|/\$||' | \
            awk -F'\\!\\! ' '{print \$2}'  | \
            tee -a ${contextPath}/artifacts.txt
        for aname in \$(cat ${contextPath}/artifacts.txt | grep -F '/')
        do
            mkdir -p \$(dirname -- "${contextPath}/root/\${aname}")
        done
        cat ${contextPath}/artifacts.txt | \
            xargs -I BUILDSOURCE mv BUILDSOURCE ${contextPath}/root/BUILDSOURCE

        cd ${contextPath}
        echo "FROM scratch" > ${dockerfileName}
        echo "COPY root/ /" >> ${dockerfileName}
        docker build  -t ${BUILD_CACHE_IMAGE}:${BUILD_CACHE_TAG} -f ${dockerfileName} .
        docker push ${BUILD_CACHE_IMAGE}:${BUILD_CACHE_TAG}
        docker rmi ${BUILD_CACHE_IMAGE}:${BUILD_CACHE_TAG}
    """,
    label: 'upload target files'
}

def buildDockerBinaryImage = { fromImage ->
    writeFile file: "${env.WORKSPACE}@tmp/DockerBinaryImageDockerfile", text: """\
FROM ${fromImage}
ARG DOCKER_VERSION=19.03.8
RUN set -ex \
    && curl -fsSLO https://download.docker.com/linux/static/stable/x86_64/docker-\${DOCKER_VERSION}.tgz \
    && mv docker-\${DOCKER_VERSION}.tgz docker.tgz \
    && tar xzvf docker.tgz \
    && mv docker/docker /usr/local/bin/docker \
    && rm -r docker docker.tgz
RUN curl -s -L \
    "https://github.com/docker/compose/releases/download/1.26.2/docker-compose-\$(uname -s)-\$(uname -m)" \
    -o /usr/local/bin/docker-compose \
    && chmod +x /usr/local/bin/docker-compose
RUN curl -sL https://aka.ms/InstallAzureCLIDeb | bash
RUN curl -s "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" \
    && unzip awscliv2.zip \
    && ./aws/install \
    && rm awscliv2.zip \
    && rm -rf aws
RUN echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" \
    | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list \
    && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg \
    |  apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - \
    && apt-get update -y \
    && apt-get install google-cloud-sdk -y
RUN curl -s https://curl.haxx.se/ca/cacert.pem > /etc/ssl/certs/cacert.pem
ENV SSL_CERT_FILE=/etc/ssl/certs/cacert.pem
"""
    def imageName = "${fromImage}-dbin-${env.STAGE_NAME.replaceAll(~/[^a-z0-9]/,'')}${env.BUILD_ID}"
    docker.build(imageName, "${env.WORKSPACE}@tmp -f ${env.WORKSPACE}@tmp/DockerBinaryImageDockerfile")
    return imageName
}

def integrationTestsWrapper = { stageName, jdkVersion, envMap, body ->
    stage(stageName) {
        skipStageIf(params.SKIP_JENKINS_INTEGRATION_TESTS, env.STAGE_NAME) {
            heavyNode {
                dockerInitCleanup()
                sh script: "sudo rm -rf /root/shared"
                sh script: "sudo mkdir -p /root/shared"
                withM2Cache { cacheDir ->
                    checkout scm
                    workspaceCleanup()
                    downloadInstallCache(env.WORKSPACE)
                    docker.image(buildDockerBinaryImage("maven:3.6.3-jdk-${jdkVersion}")).inside("""\
                        -v ${cacheDir}/.m2:/root/.m2 \
                        -v /var/run/docker.sock:/var/run/docker.sock \
                        -v /root/shared:/root/shared \
                        -e DOCKER_IP=127.0.0.1 \
                        --net host \
                        -u root:root \
                        """) {
                        withArtifactorySettings {
                            withEnv(envMap) {
                                sh script: """
                                    echo "deb http://security.debian.org/debian-security jessie/updates main" >> /etc/apt/sources.list
                                    apt-get update -y
                                    apt-get install dnsutils -y
                                    apt-get install openssl=1.0.1t-1+deb8u12 -y --allow-downgrades
                                """
                                try {
                                    timeout(time: 90, unit: 'MINUTES') {
                                        body()
                                    }
                                } finally {
                                    sh script: """#!/bin/bash -ex
                                        if  ls ~/shared/logs/*.log; then
                                            for v in ~/shared/logs/*.log; do
                                                echo \$v logtail ========================
                                                tail -100 \$v
                                            done
                                        fi
                                        docker ps -a
                                        for v in broker middlemanager overlord router coordinator historical; do
                                            echo \$v dmesg ========================
                                            docker ps --format {{.Names}} | grep -q druid-\$v && docker exec druid-\$v sh -c 'dmesg | tail -3' || true
                                        done
                                    """
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}

def s3DeepStorageTests = { stageName, jdkVersion, envMap ->
    integrationTestsWrapper(stageName, jdkVersion, envMap) {
        withAWSCredentials {
            def cloudpath = UUID.randomUUID().toString()
            sh script: """
                aws s3 sync \
                ${WORKSPACE}/integration-tests/src/test/resources/data/batch_index/json/ s3://druid-qa/${cloudpath} \
                --exclude "*" --include "wikipedia_index_data*.json"
            """
            writeFile file: "jenkins/s3-config", text: "druid_storage_type=s3\ndruid_storage_bucket=druid-qa\ndruid_storage_baseKey=${cloudpath}\ndruid_s3_accessKey=${AWS_ACCESS_KEY_ID}\ndruid_s3_secretKey=${AWS_SECRET_ACCESS_KEY}\nAWS_REGION=us-east-1\ndruid_extensions_loadList=[\"druid-s3-extensions\",\"druid-hdfs-storage\"]"
            try {
                sh script: """
                    ${MVN_OPTS}
                    ${MVN} verify -P integration-tests -pl integration-tests \
                    -Dgroups=s3-deep-storage -Doverride.config.path=${WORKSPACE}/jenkins/s3-config \${JVM_RUNTIME} -Ddruid.test.config.cloudBucket=druid-qa -Ddruid.test.config.cloudPath=${cloudpath}/ -Dstart.hadoop.docker=true \
                    -ff ${MAVEN_SKIP} -Djacoco.skip=true
                """, label: "s3-deep-storage with ${env.JVM_RUNTIME}"
            }
            finally {
                sh script: "aws s3 rm s3://druid-qa/${cloudpath}  --recursive"
            }
        }
    }
}

def kinesisDeepStorageTests = { stageName, jdkVersion, envMap ->
    integrationTestsWrapper(stageName, jdkVersion, envMap) {
        lock('awsResource') {
            withAWSCredentials {
                writeFile file: "jenkins/kinesis-config", text: "druid_kinesis_accessKey=${AWS_ACCESS_KEY_ID}\ndruid_kinesis_secretKey=${AWS_SECRET_ACCESS_KEY}\nAWS_REGION=us-east-1\ndruid_extensions_loadList=[\"druid-kinesis-indexing-service\"]"
                sh script: """
                    ${MVN_OPTS}
                    ${MVN} verify -P integration-tests -pl integration-tests \
                    -Dgroups=kinesis-index -Doverride.config.path=${WORKSPACE}/jenkins/kinesis-config \${JVM_RUNTIME} -Ddruid.test.config.streamEndpoint=kinesis.us-east-1.amazonaws.com -Dstart.hadoop.docker=true \
                    -ff ${MAVEN_SKIP} -Djacoco.skip=true
                """, label: "kinesis-deep-storage with ${env.JVM_RUNTIME}"
            }
        }
    }
}

def azureDeepStorageTests = { stageName, jdkVersion, envMap ->
    integrationTestsWrapper(stageName, jdkVersion, envMap) {
        withAzureCredentials {
            def containerName = UUID.randomUUID().toString()
            sh script: """
                az storage container create -n ${containerName} \
                --public-access blob \
                --account-name ${AZURE_ACCOUNT} --account-key ${AZURE_KEY}
            """
            writeFile file: "jenkins/azure-config", text: "druid_storage_type=azure\ndruid_azure_account=$AZURE_ACCOUNT\ndruid_azure_key=$AZURE_KEY\ndruid_azure_container=${containerName}\ndruid_extensions_loadList=[\"druid-azure-extensions\",\"druid-hdfs-storage\"]"
            try {
                sh script: """
                    az storage blob upload-batch \
                    --account-name ${AZURE_ACCOUNT} --account-key ${AZURE_KEY} \
                    -d ${containerName} \
                    --source ${WORKSPACE}/integration-tests/src/test/resources/data/batch_index/json/ --pattern "wikipedia_index_data*.json"
                """
                sh script: """
                    ${MVN_OPTS}
                    ${MVN} verify -P integration-tests -pl integration-tests \
                    -Dgroups=azure-deep-storage -Doverride.config.path=${WORKSPACE}/jenkins/azure-config \${JVM_RUNTIME} -Ddruid.test.config.cloudBucket=${containerName} -Ddruid.test.config.cloudPath= -Dstart.hadoop.docker=true \
                    -ff ${MAVEN_SKIP} -Djacoco.skip=true
                """, label: "azure-deep-storage with ${env.JVM_RUNTIME}"
            }
            finally {
                sh script: """
                     az storage container delete -n ${containerName}\
                     --account-name ${AZURE_ACCOUNT} --account-key ${AZURE_KEY}
                """
            }
        }
    }
}

def gcsDeepStorageTests = { stageName, jdkVersion, envMap ->
    integrationTestsWrapper(stageName, jdkVersion, envMap) {
        withGCSCredentials {
            def cloudpath = "gcs-test-${UUID.randomUUID().toString()}/"
            def bucket = "imply-qa-testing"

            writeFile file: "${WORKSPACE}/jenkins/gcs-config", text: "druid_storage_type=google\ndruid_google_bucket=${bucket}\ndruid_google_prefix=${cloudpath}\ndruid_extensions_loadList=[\"druid-google-extensions\",\"druid-hdfs-storage\"]\nGOOGLE_APPLICATION_CREDENTIALS=/shared/docker/credentials/creds.json"
            sh script: """
                mkdir -p ${WORKSPACE}/jenkins/gcs
                cp ${env.GC_KEY} ${WORKSPACE}/jenkins/gcs/creds.json
                chmod 764 ${WORKSPACE}/jenkins/gcs/creds.json
                gsutil -o Credentials:gs_service_key_file=${WORKSPACE}/jenkins/gcs/creds.json cp ${WORKSPACE}/integration-tests/src/test/resources/data/batch_index/json/wikipedia_index_data*.json gs://${bucket}/${cloudpath}
                """, label: "copy gcs creds"
            try {
                sh script: MVN_OPTS + "\n" + MVN +
                    ' verify -P integration-tests -pl integration-tests' +
                    " -Doverride.config.path=${WORKSPACE}/jenkins/gcs-config -Dresource.file.dir.path=${WORKSPACE}/jenkins/gcs" +
                    " -Dgroups=gcs-deep-storage \${JVM_RUNTIME} -Ddruid.test.config.cloudBucket=${bucket} -Ddruid.test.config.cloudPath=${cloudpath} -Dstart.hadoop.docker=true" +
                    " -ff ${MAVEN_SKIP} -Djacoco.skip=true", label: "gcs-deep-storage with ${env.JVM_RUNTIME}"
            } finally {
                sh script: "gsutil -o Credentials:gs_service_key_file=${WORKSPACE}/jenkins/gcs/creds.json rm -r gs://${bucket}/${cloudpath}"
            }

        }
    }
}

def hdfsDeepStorageTests = { stageName, jdkVersion, envMap ->
    integrationTestsWrapper(stageName, jdkVersion, envMap) {
        writeFile file: "${WORKSPACE}/jenkins/hdfs-config", text: "druid_storage_type=hdfs\n" +
            "druid_storage_storageDirectory=/druid/segments\n" +
            "druid_extensions_loadList=[\"druid-hdfs-storage\"]\n" +
            "druid_indexer_logs_type=hdfs\n" +
            "druid_indexer_logs_directory=/druid/indexing-logs"
        sh script: MVN_OPTS + "\n" + MVN +
            ' verify -P integration-tests -pl integration-tests' +
            " -Doverride.config.path=${WORKSPACE}/jenkins/hdfs-config" +
            ' -Dgroups=hdfs-deep-storage -Dstart.hadoop.docker=true ${JVM_RUNTIME} -Ddruid.test.config.extraDatasourceNameSuffix="" -Dit.test=ITHdfsToHdfsParallelIndexTest' +
            " -ff ${MAVEN_SKIP} -Djacoco.skip=true", label: "hdfs-deep-storage with ${env.JVM_RUNTIME}"
    }
}

stage('Maven install') {
    skipStageIf(params.SKIP_ALL_JENKINS_TESTS, env.STAGE_NAME) {
        heavyNode {
            withM2Cache { cacheDir ->
                checkout scm
                workspaceCleanup()
                docker.image('maven:3.6.3-jdk-8').inside("""\
                    --memory=8g --memory-reservation=4g \
                    -e HOME=/tmp -e _JAVA_OPTIONS=-Duser.home=/tmp -v ${cacheDir}/.m2:/tmp/.m2 \
                    """ ) {
                    withArtifactorySettings {
                        mavenInstall()
                    }
                }
                uploadM2Cache(cacheDir)
                uploadInstallCache(env.WORKSPACE)
            }
        }
    }
}

stage("Checks") {
    skipStageIf(params.SKIP_ALL_JENKINS_TESTS, env.STAGE_NAME) {
        parallel "security vulnerabilities": {
            stage("security vulnerabilities") {
                lightweightNode {
                    withM2Cache { cacheDir ->
                        docker.image("maven:3.6.3-jdk-8").inside(
                            "--memory=8g --memory-reservation=4g -v ${cacheDir}/.m2:/root/.m2 -u root:root") {
                            checkout scm
                            withArtifactorySettings {
                                retryOnNonEmpty(env.WORKSPACE, 30, 3) { outputFile ->
                                    sh script: """#!/bin/bash
                                        set -o pipefail

                                        ${MVN} dependency-check:check 2>&1 | \
                                        tee >(grep -F 'Failed to request component-reports' >> ${outputFile}) || \
                                        { echo "The OWASP dependency check has found security vulnerabilities. Please use a newer version
                                        of the dependency that does not have vulnerabilities. If the analysis has false positives,
                                        they can be suppressed by adding entries to owasp-dependency-check-suppressions.xml (for more
                                        information, see https://jeremylong.github.io/DependencyCheck/general/suppression.html).
                                        " && false; }
                                    """
                                }
                            }
                        }
                    }
                }
            }
        },
        "(Compile=openjdk8, Run=openjdk8) s3 deep storage test": {
            s3DeepStorageTests("(Compile=openjdk8, Run=openjdk8) s3 deep storage test", 8, ['JVM_RUNTIME=-Djvm.runtime=8'])
        },
        "(Compile=openjdk8, Run=openjdk8) kinesis deep storage test": {
            kinesisDeepStorageTests("(Compile=openjdk8, Run=openjdk8) kinesis deep storage test", 8, ['JVM_RUNTIME=-Djvm.runtime=8'])
        },
        "(Compile=openjdk8, Run=openjdk8) azure deep storage test": {
            azureDeepStorageTests("(Compile=openjdk8, Run=openjdk8) azure deep storage test", 8, ['JVM_RUNTIME=-Djvm.runtime=8'])
        },
        "(Compile=openjdk8, Run=openjdk8) hdfs deep storage test": {
            hdfsDeepStorageTests("(Compile=openjdk8, Run=openjdk8) hdfs deep storage test", 8, ['JVM_RUNTIME=-Djvm.runtime=8'])
        },
        "(Compile=openjdk8, Run=openjdk8) gcs deep storage test": {
            gcsDeepStorageTests("(Compile=openjdk8, Run=openjdk8) gcs deep storage test", 8, ['JVM_RUNTIME=-Djvm.runtime=8'])
        },

        "(Compile=openjdk8, Run=openjdk11) s3 deep storage test": {
            s3DeepStorageTests("(Compile=openjdk8, Run=openjdk11) s3 deep storage test", 8, ['JVM_RUNTIME=-Djvm.runtime=11'])
        },
        "(Compile=openjdk8, Run=openjdk11) azure deep storage test": {
            azureDeepStorageTests("(Compile=openjdk8, Run=openjdk11) azure deep storage test", 8, ['JVM_RUNTIME=-Djvm.runtime=11'])
        },
        "(Compile=openjdk8, Run=openjdk11) hdfs deep storage test": {
            hdfsDeepStorageTests("(Compile=openjdk8, Run=openjdk11) hdfs deep storage test", 8, ['JVM_RUNTIME=-Djvm.runtime=11'])
        },
        "(Compile=openjdk8, Run=openjdk11) gcs deep storage test": {
            gcsDeepStorageTests("(Compile=openjdk8, Run=openjdk11) gcs deep storage test", 8, ['JVM_RUNTIME=-Djvm.runtime=11'])
        }
    }
}

stage('Build and publish') {
    heavyNode {
        stage('checkout') {
            checkout scm
            workspaceCleanup()

            def commitSHA = shellOutput('git rev-parse HEAD')
            if(!shellOutput("git branch origin/${BRANCH_NAME} -r --contains ${commitSHA}")) {
                commitSHA = shellOutput('git rev-parse HEAD^')
                if(!shellOutput("git branch origin/${BRANCH_NAME} -r --contains ${commitSHA}")) {
                    error("head branch commit SHA is not found")
                }
            }

            HEAD_COMMIT_SHA = commitSHA
            REPO_URL = shellOutput('git config --get remote.origin.url')
            REPO_NAME = REPO_URL.tokenize(/(\/|:)/).last().replaceFirst(/\.git$/, '')
            REPO_OWNER = REPO_URL.tokenize(/(\/|:)/)[-2]
            HEAD_BRANCH_NAME = env.CHANGE_BRANCH ?: env.BRANCH_NAME
            SKIP_PUBLISH_STAGES = !(HEAD_BRANCH_NAME ==~ ACTIVE_DEV_BRANCH_REGEX || HEAD_BRANCH_NAME ==~ (RELEASE_BRANCH_PREFIX_REGEX + RELEASE_BRANCH_SUFFIX_REGEX) || params.PUBLISH_ON_ANY_BRANCH)
        }

        stage('poll commit status') {
            skipStageIf(SKIP_PUBLISH_STAGES, env.STAGE_NAME) {
                def untilSmthAttemptsNumber = 1
                def untilPassAttemptsNumber = 1

                withCurrentGitCredentials { gitUser, gitPassword ->
                    while(true) {
                        def statuses = readJSON(
                            text: sh(
                                script: """
                                    curl -s -u ${gitUser}:${gitPassword} \
                                    https://api.github.com/repos/${REPO_OWNER}/${REPO_NAME}/commits/${HEAD_COMMIT_SHA}/check-suites |  \
                                    tee /dev/stderr | \
                                    jq -rM '.check_suites[]? | select(.app.slug == "${TRAVIS_SLUG_ID}") | {status: .status, conclusion: .conclusion}' | \
                                    jq -rsM .
                                """,
                                returnStdout: true,
                                label: 'getting travis ci status of commit'
                            ).trim(),
                            returnPojo: true
                        )

                        if(statuses.every {it.status == 'completed' && it.conclusion == 'success'}) {
                            echo "Success: all travis checks are passed for commit: ${HEAD_COMMIT_SHA}"
                            break
                        }

                        if(statuses.any {it.conclusion == 'failure' || it.conclusion == 'cancelled'}) {
                            error("some of travis checks are failed or cancelled")
                        }

                        if(untilSmthAttemptsNumber >= TRAVIS_UNTIL_SMTH_MAX_ATTEMPTS) {
                            error("travis statuses are not found after ${TRAVIS_UNTIL_SMTH_MAX_ATTEMPTS} attempts")
                        }
                        if(untilPassAttemptsNumber >= TRAVIS_UNTIL_PASS_MAX_ATTEMPTS) {
                            error("travis passed statuses are not found after ${TRAVIS_UNTIL_PASS_MAX_ATTEMPTS} attempts")
                        }

                        if(!statuses) {
                            untilSmthAttemptsNumber = untilSmthAttemptsNumber + 1
                        } else {
                            untilPassAttemptsNumber = untilPassAttemptsNumber + 1
                        }

                        echo "sleeping ${COMMIT_STATUS_INTERVAL} seconds before next attempt"
                        sleep COMMIT_STATUS_INTERVAL
                    }
                }
            }
        }

        withCurrentGitCredentials { gitUser, gitPassword ->
            stage('build') {
                skipStageIf(SKIP_PUBLISH_STAGES, env.STAGE_NAME) {
                    ecrLogin(params.BUILD_DOCKER_IMAGE)
                    docker.image(params.BUILD_DOCKER_IMAGE).inside(
                        """ -u root:root \
                            -e GIT_CREDS_PSW=${gitPassword} \
                            -e GIT_ASKPASS=/tmp/askpass.sh \
                            -e GIT_CREDS_USR=${gitUser} \
                            -e BUILD_DIR=/tmp/druid-build \
                            -e STAGING_DIR=/tmp/druid-build/stage \
                            -e TMP_DIR=/tmp/druid-build/tmp \
                            -e DRUID_COMPILE_JAVA_VERSION='' \
                            -e UPSTREAM_DIR=${env.WORKSPACE} \
                            -e UPSTREAM_COMMIT_SHA=${HEAD_COMMIT_SHA}"""
                        ) {
                        withArtifactorySettings {
                            sh script:'''#!/bin/bash -eux
                                echo -e '#!/bin/bash\nPATH=/opt/maven/apache-maven-3.5.4/bin:$PATH exec mvn -B $@' > /usr/bin/mvn
                                chmod u+x /usr/bin/mvn

                                cat <<EOF > "${GIT_ASKPASS}"
                                #!/bin/sh
                                case "\\$1" in
                                    Username*) echo "\\$GIT_CREDS_USR" ;;
                                    Password*) echo "\\$GIT_CREDS_PSW" ;;
                                esac
                                EOF

                                chmod u+x "${GIT_ASKPASS}"
                            '''.replaceAll(/\n\s+/, "\n"),
                            label: 'prepare for build'

                            sh script:'''#!/bin/bash -eux
                                NETTY_UPSTREAM_NAME="netty"
                                NETTY_UPSTREAM_ORG=${NETTY_UPSTREAM_ORG:-"implydata"}
                                NETTY_UPSTREAM_REPO="https://github.com/$NETTY_UPSTREAM_ORG/$NETTY_UPSTREAM_NAME.git"
                                NETTY_UPSTREAM_VERSION=${NETTY_UPSTREAM_VERSION:-"3.10.6.Final-iap1"}
                                NETTY_UPSTREAM_COMMITISH=${NETTY_UPSTREAM_COMMITISH:-"netty-$NETTY_UPSTREAM_VERSION"}
                                NETTY_UPSTREAM_DIR="$TMP_DIR/$NETTY_UPSTREAM_NAME.git"

                                CLARITY_EMITTER_UPSTREAM_NAME="clarity-emitter"
                                CLARITY_EMITTER_UPSTREAM_ORG=${CLARITY_EMITTER_UPSTREAM_ORG:-"implydata"}
                                CLARITY_EMITTER_UPSTREAM_REPO="https://github.com/$CLARITY_EMITTER_UPSTREAM_ORG/$CLARITY_EMITTER_UPSTREAM_NAME.git"
                                CLARITY_EMITTER_UPSTREAM_COMMITISH=${CLARITY_EMITTER_UPSTREAM_COMMITISH:-"master"}
                                CLARITY_EMITTER_UPSTREAM_DIR="$TMP_DIR/$CLARITY_EMITTER_UPSTREAM_NAME.git"

                                IMPLY_DRUID_SECURITY_UPSTREAM_NAME="imply-druid-security"
                                IMPLY_DRUID_SECURITY_UPSTREAM_ORG=${IMPLY_DRUID_SECURITY_UPSTREAM_ORG:-"implydata"}
                                IMPLY_DRUID_SECURITY_UPSTREAM_REPO="https://github.com/$IMPLY_DRUID_SECURITY_UPSTREAM_ORG/$IMPLY_DRUID_SECURITY_UPSTREAM_NAME.git"
                                IMPLY_DRUID_SECURITY_UPSTREAM_COMMITISH=${IMPLY_DRUID_SECURITY_UPSTREAM_COMMITISH:-"master"}
                                IMPLY_DRUID_SECURITY_UPSTREAM_DIR="$TMP_DIR/$IMPLY_DRUID_SECURITY_UPSTREAM_NAME.git"

                                DERBYTOOLS_VERSION="10.11.1.1"

                                # Imply includes additional hadoop libraries (e.g., hadoop-aws)
                                HADOOP_VERSION="2.8.5"

                                # This should match the version used in druid
                                MYSQL_CONNECTOR_VERSION="5.1.48"

                                # Install our patched version of Netty.
                                git clone -b "$NETTY_UPSTREAM_COMMITISH" --single-branch --depth 1 "$NETTY_UPSTREAM_REPO" "$NETTY_UPSTREAM_DIR"
                                (cd "$NETTY_UPSTREAM_DIR" && git checkout "$NETTY_UPSTREAM_COMMITISH")
                                (cd "$NETTY_UPSTREAM_DIR" && mvn install -DskipTests)

                                cd $UPSTREAM_DIR

                                # remove snapshot suffix from pom files
                                mvn versions:set -DremoveSnapshot -DgenerateBackupPoms=false

                                run_druid_mvn_install()
                                {
                                  mvn install \
                                  -Dnetty3.version="$NETTY_UPSTREAM_VERSION" \
                                  -Dhadoop.compile.version="$HADOOP_VERSION" \
                                  -Dmysql.version="$MYSQL_CONNECTOR_VERSION" \
                                  -Danimal.sniffer.skip=true \
                                  -Dcheckstyle.skip=true \
                                  -Denforcer.skip=true \
                                  -Dforbiddenapis.skip=true \
                                  -Djacoco.skip=true \
                                  -Dmaven.javadoc.skip=true \
                                  -Dpmd.skip=true \
                                  -Dspotbugs.skip=true \
                                  -DskipTests \
                                  -T1C \
                                  -Dtar
                                }

                                if [ -z "$DRUID_COMPILE_JAVA_VERSION" ]
                                then
                                      current_java_version=$(java -version 2>&1 >/dev/null | grep 'version' | awk '{print $3}')
                                      echo "\$DRUID_COMPILE_JAVA_VERSION is not set. Compiling Druid with current Java version set. Java version=$current_java_version"
                                      (
                                        cd "$UPSTREAM_DIR" && \
                                        run_druid_mvn_install
                                      )
                                else
                                      echo "Compiling Druid with Java version DRUID_COMPILE_JAVA_VERSION=$DRUID_COMPILE_JAVA_VERSION"
                                      (
                                        sudo update-java-alternatives -s zulu-"${DRUID_COMPILE_JAVA_VERSION}"-amd64 && \
                                        export JAVA_HOME="/usr/lib/jvm/zulu-${DRUID_COMPILE_JAVA_VERSION}-amd64" && \
                                        export PATH="$PATH:$JAVA_HOME" && \
                                        cd "$UPSTREAM_DIR" && \
                                        run_druid_mvn_install
                                      )
                                fi

                                DRUID_VERSION=$(cd "$UPSTREAM_DIR" && mvn help:evaluate -Dexpression=project.version | egrep -v '^[\\[D]' | fgrep -v 'Download')
                                echo "Built Druid, version: $DRUID_VERSION"

                                # Stage Druid
                                mkdir -p "$STAGING_DIR/dist"
                                tar -C "$TMP_DIR" -xzf "$UPSTREAM_DIR"/distribution/target/apache-druid-*-bin.tar.gz
                                mv "$TMP_DIR"/apache-druid-* "$STAGING_DIR/dist/druid"

                                # Fetch the MySQL JDBC driver from Maven Central
                                MYSQL_CONNECTOR_LOCATION="$STAGING_DIR/dist/druid/extensions/mysql-metadata-storage/mysql-connector-java-${MYSQL_CONNECTOR_VERSION}.jar"
                                mkdir -p "$STAGING_DIR/dist/druid/extensions/mysql-metadata-storage"
                                curl -o "$MYSQL_CONNECTOR_LOCATION" --retry 10 "https://repo1.maven.org/maven2/mysql/mysql-connector-java/${MYSQL_CONNECTOR_VERSION}/mysql-connector-java-${MYSQL_CONNECTOR_VERSION}.jar"

                                if [ "$(sha1sum "$MYSQL_CONNECTOR_LOCATION" | awk '{print $1}')" != "9140be77aafa5050bf4bb936d560cbacb5a6b5c1" ]
                                then
                                  echo "$MYSQL_CONNECTOR_LOCATION: checksum mismatch" >&2
                                  exit 1
                                fi

                                # Add materialized-view-maintenance and materialized-view-selection
                                if [ -d "${UPSTREAM_DIR}/extensions-contrib/materialized-view-maintenance/target" ]
                                then
                                    mkdir -p "${STAGING_DIR}/dist/druid/extensions/materialized-view-maintenance"
                                    mv  "${UPSTREAM_DIR}"/extensions-contrib/materialized-view-maintenance/target/materialized-view-maintenance* \
                                        "${STAGING_DIR}"/dist/druid/extensions/materialized-view-maintenance
                                fi
                                if [ -d "${UPSTREAM_DIR}/extensions-contrib/materialized-view-selection/target" ]
                                then
                                    mkdir -p "${STAGING_DIR}/dist/druid/extensions/materialized-view-selection"
                                    mv  "${UPSTREAM_DIR}"/extensions-contrib/materialized-view-selection/target/materialized-view-selection* \
                                        "${STAGING_DIR}"/dist/druid/materialized-view-selection
                                fi

                                # Add hadoop-aws
                                (cd "$STAGING_DIR"/dist/druid && java -classpath "lib/*" org.apache.druid.cli.Main tools pull-deps -h "org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}")

                                # Add derbytools to lib
                                DERBYTOOLS_LOCATION="$STAGING_DIR/dist/druid/lib/derbytools-$DERBYTOOLS_VERSION.jar"
                                curl -Lo "$DERBYTOOLS_LOCATION" "https://search.maven.org/remotecontent?filepath=org/apache/derby/derbytools/$DERBYTOOLS_VERSION/derbytools-$DERBYTOOLS_VERSION.jar"

                                if [ "$(sha1sum "$DERBYTOOLS_LOCATION" | awk '{print $1}')" != "10a124a8962c6f8ea70a368d711ce6883889ca34" ]
                                then
                                  echo "$DERBYTOOLS_LOCATION: checksum mismatch" >&2
                                  exit 1
                                fi

                                # Add clarity-emitter to extensions
                                git clone -b "$CLARITY_EMITTER_UPSTREAM_COMMITISH" --single-branch --depth 1 "$CLARITY_EMITTER_UPSTREAM_REPO" "$CLARITY_EMITTER_UPSTREAM_DIR"
                                (cd "$CLARITY_EMITTER_UPSTREAM_DIR" && git checkout "$CLARITY_EMITTER_UPSTREAM_COMMITISH")
                                (cd "$CLARITY_EMITTER_UPSTREAM_DIR" && mvn package -DskipTests -Ddruid.version="$DRUID_VERSION")

                                tar -C "$STAGING_DIR/dist/druid/extensions" -xzf "$CLARITY_EMITTER_UPSTREAM_DIR"/http/target/clarity-emitter-http-*.tar.gz
                                tar -C "$STAGING_DIR/dist/druid/extensions" -xzf "$CLARITY_EMITTER_UPSTREAM_DIR"/kafka/target/clarity-emitter-kafka-*.tar.gz

                                # Add imply-druid-security to extensions
                                git clone -b "$IMPLY_DRUID_SECURITY_UPSTREAM_COMMITISH" --single-branch --depth 1 "$IMPLY_DRUID_SECURITY_UPSTREAM_REPO" "$IMPLY_DRUID_SECURITY_UPSTREAM_DIR"
                                (cd "$IMPLY_DRUID_SECURITY_UPSTREAM_DIR" && git checkout "$IMPLY_DRUID_SECURITY_UPSTREAM_COMMITISH")
                                (cd "$IMPLY_DRUID_SECURITY_UPSTREAM_DIR" && mvn package -DskipTests -Ddruid.version="$DRUID_VERSION")

                                mkdir -p "$STAGING_DIR/dist/druid/extensions/imply-druid-security"
                                cp "$IMPLY_DRUID_SECURITY_UPSTREAM_DIR"/target/imply-druid-security-*.jar "$STAGING_DIR/dist/druid/extensions/imply-druid-security"

                                # Remove unsupported open-source extensions
                                for extension in druid-pac4j druid-ranger-security materialized-view-maintenance materialized-view-selection; do
                                    rm -rf "${STAGING_DIR}/dist/druid/extensions/${extension}"
                                done

                                # Put druid on a diet
                                perl - <<'EOT'
                                use strict;
                                use File::Basename;

                                my $dir = "$ENV{STAGING_DIR}/dist/druid";
                                chdir $dir or die "chdir $dir: $!";

                                my %jars;
                                for my $jar (qx!find ./lib -name '*.jar'!, qx!find ./extensions -name '*.jar'!, qx!find ./hadoop-dependencies -name '*.jar'!) {
                                  chomp $jar;
                                  my $jarname = basename($jar);
                                  if (exists $jars{$jarname}) {
                                    my $depth = $jar =~ tr !/!/! - 1;
                                    my $dots = "";
                                    for my $x (1..$depth) {
                                      $dots .= "../";
                                    }
                                    system("ln", "-sf", "${dots}$jars{$jarname}", $jar);
                                  } else {
                                    $jars{$jarname} = $jar;
                                  }
                                }
                                EOT

                                tar -C $STAGING_DIR/dist -czf $UPSTREAM_DIR/druid-${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}.tar.gz druid
                            '''.replaceAll(/\n\s+/, "\n"),
                            label: 'build druid'
                        }
                    }
                }
            }

            stage('upload') {
                skipStageIf(SKIP_PUBLISH_STAGES, env.STAGE_NAME) {
                    def release_version = "None"
                    if(HEAD_BRANCH_NAME ==~ (RELEASE_BRANCH_PREFIX_REGEX + RELEASE_BRANCH_SUFFIX_REGEX)) {
                        release_version = (HEAD_BRANCH_NAME =~ RELEASE_BRANCH_SUFFIX_REGEX).getAt(0)[0]
                    } else if(HEAD_BRANCH_NAME ==~ ACTIVE_DEV_BRANCH_REGEX){
                        release_version = shellOutput("""
                                curl -s -u ${gitUser}:${gitPassword} -H "Accept: application/vnd.github.v3.raw" \
                                https://api.github.com/repos/implydata/imply-release/contents/monthly-release.version |  \
                                tee /dev/stderr
                            """
                        )
                    }
                    def build_version = shellOutput('''#!/bin/bash -ex
                            ls druid-*.tar.gz | head -n 1 | sed -E "s/druid-(.+)\\.tar\\.gz/\\1/"
                        '''
                    )
                    rtUpload (
                        serverId: ARTIFACTORY_SERVER_ID,
                        spec: """{
                            "files": [
                                {
                                  "pattern": "druid-*.tar.gz",
                                  "target": "${ARTIFACTORY_REPO}/${REPO_NAME}/",
                                  "props": "release.version=${release_version};BVT=Pass;build.url=${BUILD_URL};vcs.revision=${HEAD_COMMIT_SHA};vcs.url=${REPO_URL};vcs.branch=${HEAD_BRANCH_NAME};build.version=${build_version}"
                                }
                            ]
                        }""",
                        buildName: "${REPO_NAME}",
                        buildNumber: "${BUILD_NUMBER}"
                    )
                }
            }
        }
    }
}
