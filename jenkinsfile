def setupMavenSettings() {
    script {
        withCredentials([file(credentialsId: 'maven-artifactory-settings', variable: 'MVN_SETTINGS_PATH')]) {
            sh script: '''
                mkdir -p ~/.m2
                cp -f $MVN_SETTINGS_PATH ~/.m2/settings.xml
            ''',
               label: 'setup maven settings'
        }
    }
}

def markBvtAsFail(artifactName, version) {
    script {
        withCredentials(
                [usernamePassword(
                        credentialsId: 'repo.cnc.imply.io',
                        usernameVariable: 'ARTIFACTORY_USER',
                        passwordVariable: 'ARTIFACTORY_PASSWORD'
                )]
        ) {
            def UPDATE_PROPS_URL = "https://repo.cnc.imply.io/artifactory/api/metadata/tgz-local/${artifactName}/${artifactName}-${version}.tar.gz"
            sh(script: """
                curl -u "${ARTIFACTORY_USER}":"${ARTIFACTORY_PASSWORD}" --location --request PATCH "${UPDATE_PROPS_URL}" \
                --header 'Content-Type: application/json' \
                --data-raw '{
                    "props":{
                        "BVT": "Fail"
                    }
                }'"""
            )
        }
    }
}

def mavenInstall() {
    setupMavenSettings()
    sh script: '''        
        ${MVN} clean install -q -ff -pl \'!distribution\' ${MAVEN_SKIP} ${MAVEN_SKIP_TESTS} -T1C
        ${MVN} install -q -ff -pl \'distribution\' ${MAVEN_SKIP} ${MAVEN_SKIP_TESTS}
    ''',
       label: 'mvn install'
}

def dockerCleanup() {
    sh  script: 'docker system prune -f -a --volumes || true',
        label: 'cleanup docker data'
}

def mavenDockerBackup() {
    rtDockerPull(
            serverId: "${RT_SERVER_ID}",
            image: "${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}",
            sourceRepo: 'docker-local'
    )
    sh script: '''#!/bin/bash -e
        TEMP_DIR=$(mktemp -d)
        git status --ignored --porcelain | \
            grep -E '^\\!\\!' | sed 's|/$||' | awk -F'\\!\\! ' '{print $2}' | grep -v -E '^web-console' | \
        while read mvnresult; do mkdir -p ${TEMP_DIR}/$(dirname -- $mvnresult); cp -R $mvnresult ${TEMP_DIR}/$mvnresult; done
        tar -czf /tmp/mvnresult.tar.gz -C ${TEMP_DIR} .
        tar -czf /tmp/m2repository.tar.gz -C ~/.m2 repository
        rm -rf ${TEMP_DIR}

        cd /tmp
        echo "FROM ${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}" >> Dockerfile
        echo "COPY m2repository.tar.gz /tmp/m2repository.tar.gz" >> Dockerfile
        echo "COPY mvnresult.tar.gz /tmp/mvnresult.tar.gz" >> Dockerfile
        docker build -t ${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}-${DOCKER_TAG} .
    ''',
       label: 'build temporary docker image for the build'
    rtDockerPush(
            serverId: "${RT_SERVER_ID}",
            image: "${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}-${DOCKER_TAG}",
            targetRepo: 'docker-local'
    )
}

def mavenCacheRestore() {
    setupMavenSettings()
    sh script: '''#!/bin/bash -e
        tar -xzf /tmp/mvnresult.tar.gz --no-same-owner --no-overwrite-dir -C ./
        mkdir -p ~/.m2
        tar -xzf /tmp/m2repository.tar.gz --no-same-owner --no-overwrite-dir -C ~/.m2/
    ''',
       label: 'restore build cache'
}

def prepareIfconfig() {
    sh script: '''
        echo -e '#!/bin/bash\nPATH=/sbin:$PATH exec ifconfig eth0 $@' > /usr/bin/ifconfig
        chmod +x /usr/bin/ifconfig
        hash -r
    ''',
       label: "prepare ifconfig"
}

def getInstanceId() {
    sh script: '''#!/bin/bash -x
        curl -s http://169.254.169.254/latest/meta-data/instance-id
    ''',
       label: 'print instance id'
}

def buildArtifacts(stageName) {
    script {
        def stageArtifactsDirPath = "stage_${stageName.replaceAll(~/[^A-Za-z0-9_-]/,'_')}"
        withEnv(["stageArtifactsDirPath=${stageArtifactsDirPath}"]) {
            // copy logs
            sh script: '''#!/bin/bash -x
                mkdir -p ${stageArtifactsDirPath}

                shopt -s globstar

                urlencode_path() {
                    python -c 'import urllib, sys; print urllib.quote(sys.argv[1], sys.argv[2])' "$1" "/"
                }

                copy_files_by_pattern() {
                    if  ls ${1}; then
                        for fname in ${1}; do
                            fname_root=$(echo "${fname}" | cut -d "/" -f1)
                            if ! [[ "$fname_root" = "${2}" ]]; then
                                if [[ -f "$fname" ]]; then
                                    dest=${2}/$(urlencode_path "${fname}")
                                    mkdir -p $(dirname -- "$dest")
                                    cp "$fname" "$dest"
                                fi
                            fi
                        done
                    fi
                }

                if [ -d ~/shared ]; then
                    artifacts_path=$(pwd)/${stageArtifactsDirPath}
                    pushd ~/shared
                    copy_files_by_pattern 'logs/**' $artifacts_path
                    copy_files_by_pattern 'tasklogs/**' $artifacts_path
                    popd
                fi

                # copy dockerd logs
                artifacts_path=$(pwd)/${stageArtifactsDirPath}
                pushd /var/tmp
                copy_files_by_pattern 'dockerd.log' $artifacts_path
                popd

                # copy test reports
                copy_files_by_pattern '**/target/surefire-reports/*.xml' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/target/failsafe-reports/*.xml' ${stageArtifactsDirPath}

                # copy top-level jacoco reports
                copy_files_by_pattern '**/target/*.exec' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/*.html' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/*.xml' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/*.csv' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/jacoco-resources/*' ${stageArtifactsDirPath}
                # copy detailed jacoco reports
                copy_files_by_pattern '**/jacoco/**/*.html' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/**/*.xml' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/**/*.csv' ${stageArtifactsDirPath}
                # inspection results
                copy_files_by_pattern 'inspection-results/*.xml' ${stageArtifactsDirPath}
            '''
        }
        // fixate artifacts
        archiveArtifacts artifacts: "${stageArtifactsDirPath}/**", allowEmptyArchive: true
        // cleanup tmp artifacts dir
        sh script: "rm -rf ${stageArtifactsDirPath}"
    }
}

def resetWs() {
    sh script: "git clean -fdx", label: "Clean up everything but files from git"
}

def isEligibleToNotify() {
    return  !env.CHANGE_ID \
            && (env.BRANCH_NAME ==~ env.ACTIVE_DEV_BRANCH_REGEX || env.BRANCH_NAME ==~ (env.RELEASE_BRANCH_PREFIX_REGEX+env.RELEASE_BRANCH_SUFFIX_REGEX))
}

def implyQueryIntegrationTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        retry(2) {
            try {
                sh script: """
                    \${MVN} -pl integration-tests process-resources && \${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
                    -Dgroups=query \
                    -Dit.indexer=middleManager \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.extraDatasourceNameSuffix="" \
                    -Doverride.config.path=../../integration-tests-imply/docker/environment-configs/test-groups/prepopulated-data \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                   label: "imply query integration tests with ${jvmRuntimeOpt}"
            } finally {
                dockerCleanup()
            }
        }
    }
}

def implyKeycloakSecurityIntegrationTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    getInstanceId()
    script {
        retry(3) {
            try {
                sh script: """
                    \${MVN} -pl integration-tests process-resources && ${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
                    -Dgroups=keycloak-security \
                    -Dit.indexer=middleManager \
                    -Doverride.config.path=../../integration-tests-imply/docker/environment-configs/test-groups/keycloak-security \
                    ${jvmRuntimeOpt} \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                   label: "imply keycloak security integration tests with ${jvmRuntimeOpt}"
            } finally {
                dockerCleanup()
            }
        }
    }
}

def implyAsyncDownloadIntegrationTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        retry(2) {
            try {
                sh script: """
                    \${MVN} -pl integration-tests process-resources && ${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
                    -Dgroups=async-download \
                    -Dit.indexer=middleManager \
                    -Doverride.config.path=../../integration-tests-imply/docker/environment-configs/test-groups/async-download \
                    ${jvmRuntimeOpt} \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                   label: "imply async query result download integration tests with ${jvmRuntimeOpt}"
            } finally {
                dockerCleanup()
            }
        }
    }
}

def implyS3Tests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'aws', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
            def cloudpath = UUID.randomUUID().toString()
            writeFile   file: "jenkins/s3-config",
                        text: "druid_storage_type=s3\ndruid_storage_bucket=druid-qa\ndruid_storage_baseKey=${cloudpath}\ndruid_s3_accessKey=${AWS_ACCESS_KEY_ID}\ndruid_s3_secretKey=${AWS_SECRET_ACCESS_KEY}\nAWS_REGION=us-east-1\ndruid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"imply-sql-async\",\"druid-s3-extensions\"]\ndruid_query_async_storage_type=s3\ndruid_query_async_storage_s3_bucket=druid-qa\ndruid_query_async_storage_s3_prefix=${cloudpath}/async-test/\ndruid_query_async_storage_s3_tempDir=/shared/async-tmp-results\ndruid_metadata_storage_type=mysql\ndruid_metadata_storage_connector_connectURI=jdbc:mysql://druid-metadata-storage/druid\ndruid_metadata_storage_connector_user=druid\ndruid_metadata_storage_connector_password=diurd"
            retry(2) {
                try {
                    sh script: """
                        \${MVN} -pl integration-tests process-resources && ${MVN} verify -P integration-tests-imply -pl integration-tests-imply \
                        -Dgroups=imply-s3 \
                        -Doverride.config.path=\${WORKSPACE}/jenkins/s3-config \
                        ${jvmRuntimeOpt} \
                        -Ddruid.test.config.cloudBucket=druid-qa \
                        -Ddruid.test.config.cloudPath=${cloudpath}/ \
                        -ff \${MAVEN_SKIP} -Djacoco.skip=true
                    """,
                       label: "imply-s3 with ${jvmRuntimeOpt}"
                }
                finally {
                    dockerCleanup()
                }
            }
        }
    }
}

def s3DeepStorageTests(jvmRuntimeOpt, hadoopProfile) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'aws', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
            def cloudpath = UUID.randomUUID().toString()
            writeFile   file: "jenkins/s3-config",
                        text: "druid_storage_type=s3\n" +
                              "druid_storage_bucket=druid-qa\n" +
                              "druid_storage_baseKey=${cloudpath}\n" +
                              "druid_s3_accessKey=${AWS_ACCESS_KEY_ID}\n" +
                              "druid_s3_secretKey=${AWS_SECRET_ACCESS_KEY}\n" +
                              "AWS_REGION=us-east-1\n" +
                              "druid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"druid-hdfs-storage\",\"druid-s3-extensions\",\"druid-parquet-extensions\",\"druid-avro-extensions\",\"druid-protobuf-extensions\",\"druid-orc-extensions\",\"druid-kafka-indexing-service\"]"
            retry(2) {
                try {
                    sh script: """
                        aws s3 sync \
                        ./integration-tests/src/test/resources/data/batch_index/json/ s3://druid-qa/${cloudpath} \
                        --exclude "*" --include "wikipedia_index_data*.json"
                    """,
                       label: "uploading data to bucket"
                    sh script: """
                        \${MVN} verify -P integration-tests -pl integration-tests \
                        -Dgroups=s3-deep-storage \
                        -Doverride.config.path=\${WORKSPACE}/jenkins/s3-config \
                        ${jvmRuntimeOpt} \
                        -Ddruid.test.config.cloudBucket=druid-qa \
                        -Ddruid.test.config.cloudPath=${cloudpath}/ \
                        -Ddocker.build.hadoop=true \
                        -Dstart.hadoop.docker=true \
                        -ff \${MAVEN_SKIP} -Djacoco.skip=true \
                        ${hadoopProfile}
                    """,
                       label: "s3-deep-storage with ${jvmRuntimeOpt}" + (hadoopProfile != '' ? " and ${hadoopProfile}" : "")
                }
                finally {
                    dockerCleanup()
                    sh script: "aws s3 rm s3://druid-qa/${cloudpath} --recursive"
                }
            }
        }
    }
}

def kinesisDeepStorageTests(jvmRuntimeOpt, hadoopProfile) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId:  'aws', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
            lock('awsResource') {
                writeFile   file: "jenkins/kinesis-config",
                            text: "druid_kinesis_accessKey=${AWS_ACCESS_KEY_ID}\ndruid_kinesis_secretKey=${AWS_SECRET_ACCESS_KEY}\nAWS_REGION=us-east-1\ndruid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"druid-kinesis-indexing-service\"]"
                retry(2) {
                    try {
                        sh script: """
                            \${MVN} verify -P integration-tests -pl integration-tests \
                            -Dgroups=kinesis-index \
                            -Doverride.config.path=\${WORKSPACE}/jenkins/kinesis-config \
                            ${jvmRuntimeOpt} \
                            -Ddruid.test.config.streamEndpoint=kinesis.us-east-1.amazonaws.com \
                            -Ddocker.build.hadoop=true \
                            -Dstart.hadoop.docker=true \
                            -ff \${MAVEN_SKIP} -Djacoco.skip=true \
                            ${hadoopProfile}
                        """,
                           label: "kinesis-deep-storage with ${jvmRuntimeOpt}" + (hadoopProfile != '' ? " and ${hadoopProfile}" : "")
                    } finally {
                        dockerCleanup()
                    }
                }
            }
        }
    }
}

def azureDeepStorageTests(jvmRuntimeOpt, hadoopProfile) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        withCredentials([usernamePassword(credentialsId: 'azure_credentials', usernameVariable: 'AZURE_ACCOUNT', passwordVariable: 'AZURE_KEY')]) {
            def containerName = UUID.randomUUID().toString()
            writeFile   file: "jenkins/azure-config",
                        text: "druid_storage_type=azure\n" +
                              "druid_azure_account=${AZURE_ACCOUNT}\n" +
                              "druid_azure_key=${AZURE_KEY}\n" +
                              "druid_azure_container=${containerName}\n" +
                              "druid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"druid-hdfs-storage\",\"druid-azure-extensions\",\"druid-parquet-extensions\",\"druid-avro-extensions\",\"druid-protobuf-extensions\",\"druid-orc-extensions\",\"druid-kafka-indexing-service\"]"
            retry(2) {
                try {
                    sh script: """
                        az storage container create -n ${containerName} \
                        --public-access blob \
                        --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY}
                    """,
                       label: "creating storage container"
                    sh script: """
                        az storage blob upload-batch \
                        --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY} \
                        -d ${containerName} \
                        --source ./integration-tests/src/test/resources/data/batch_index/json/ --pattern "wikipedia_index_data*.json"
                    """,
                       label: "uploading data to storage container"
                    sh script: """
                        \${MVN} verify -P integration-tests -pl integration-tests \
                        -Dgroups=azure-deep-storage \
                        -Doverride.config.path=\${WORKSPACE}/jenkins/azure-config \
                        ${jvmRuntimeOpt} \
                        -Ddruid.test.config.cloudBucket=${containerName} \
                        -Ddruid.test.config.cloudPath= \
                        -Ddocker.build.hadoop=true \
                        -Dstart.hadoop.docker=true \
                        -ff \${MAVEN_SKIP} -Djacoco.skip=true \
                        ${hadoopProfile}
                    """,
                       label: "azure-deep-storage with ${jvmRuntimeOpt}" + (hadoopProfile != '' ? " and ${hadoopProfile}" : "")
                }
                finally {
                    dockerCleanup()
                    sh script: """
                        az storage container delete -n ${containerName}\
                        --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY}
                    """
                }
            }
        }
    }
}

def gcsDeepStorageTests(jvmRuntimeOpt, hadoopProfile) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        withCredentials([file(credentialsId: 'gcs-bucket-qa', variable: 'GC_KEY')]) {
            def cloudpath = "gcs-test-${UUID.randomUUID().toString()}/"
            def bucket = "imply-qa-testing"
            writeFile   file: "jenkins/gcs-config",
                        text: "druid_storage_type=google\ndruid_google_bucket=${bucket}\ndruid_google_prefix=${cloudpath}\ndruid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"druid-hdfs-storage\",\"druid-google-extensions\",\"druid-parquet-extensions\",\"druid-avro-extensions\",\"druid-protobuf-extensions\",\"druid-orc-extensions\",\"druid-kafka-indexing-service\"]\nGOOGLE_APPLICATION_CREDENTIALS=/shared/docker/credentials/creds.json"
            retry(3) {
                try {
                    sh script: """
                        mkdir -p jenkins/gcs
                        cp -f \${GC_KEY} jenkins/gcs/creds.json
                        chmod 764 jenkins/gcs/creds.json
                        gsutil \
                            -o Credentials:gs_service_key_file=\${WORKSPACE}/jenkins/gcs/creds.json \
                            cp \${WORKSPACE}/integration-tests/src/test/resources/data/batch_index/json/wikipedia_index_data*.json \
                            gs://${bucket}/${cloudpath}
                    """,
                       label: "copying gcs creds"
                    sh script: """
                        \${MVN} verify -P integration-tests -pl integration-tests \
                        -Doverride.config.path=\${WORKSPACE}/jenkins/gcs-config \
                        -Dresource.file.dir.path=\${WORKSPACE}/jenkins/gcs \
                        -Dgroups=gcs-deep-storage \
                        ${jvmRuntimeOpt} \
                        -Ddruid.test.config.cloudBucket=${bucket} \
                        -Ddruid.test.config.cloudPath=${cloudpath} \
                        -Ddocker.build.hadoop=true \
                        -Dstart.hadoop.docker=true \
                        -ff \${MAVEN_SKIP} -Djacoco.skip=true \
                        ${hadoopProfile} \
                    """,
                       label: "gcs-deep-storage with ${jvmRuntimeOpt}" + (hadoopProfile != '' ? " and ${hadoopProfile}" : "")
                } finally {
                    dockerCleanup()
                    sh script: "gsutil -o Credentials:gs_service_key_file=\${WORKSPACE}/jenkins/gcs/creds.json rm -r gs://${bucket}/${cloudpath}"
                }
            }
        }
    }
}

def hdfsDeepStorageTests(jvmRuntimeOpt, hadoopProfile) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    writeFile   file:   "jenkins/hdfs-config",
                text:   "druid_storage_type=hdfs\n" +
                        "druid_storage_storageDirectory=/druid/segments\n" +
                        "druid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"druid-hdfs-storage\",\"druid-parquet-extensions\",\"druid-avro-extensions\",\"druid-protobuf-extensions\",\"druid-orc-extensions\",\"druid-kafka-indexing-service\"]"
    script {
        retry(2) {
            try {
                sh script: """
                    \${MVN} verify -P integration-tests -pl integration-tests \
                    -Doverride.config.path=\${WORKSPACE}/jenkins/hdfs-config \
                    -Dgroups=hdfs-deep-storage \
                    -Ddocker.build.hadoop=true \
                    -Dstart.hadoop.docker=true \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.extraDatasourceNameSuffix="" \
                    -Dit.test=ITHdfsToHdfsParallelIndexTest \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true \
                    ${hadoopProfile}
                """,
                   label: "hdfs-deep-storage with ${jvmRuntimeOpt}" + (hadoopProfile != '' ? " and ${hadoopProfile}" : "")
            } finally {
                dockerCleanup()
            }
        }
    }
}

@NonCPS
def cancelPreviousBuilds() {
    println("Checking to see if any previous builds need to be aborted.")
    def jobName = env.JOB_NAME
    def buildNumber = env.BUILD_NUMBER.toInteger()
    /* Get job name */
    def currentJob = Jenkins.instance.getItemByFullName(jobName)
    /* Iterating over the builds for specific job */
    for (def build : currentJob.builds) {
        def exec = build.getExecutor()
        /* If there is a build that is currently running and it's not current build */
        if (build.isBuilding() && build.number.toInteger() != buildNumber && exec != null) {
            println("Initiating interrupt of previous build #${build.number}")
            /* Then stop it */
            exec.interrupt(
                    Result.ABORTED,
                    new CauseOfInterruption.UserInterruption("Aborted by #${currentBuild.number}")
            )
            println("Aborted previously running build #${build.number}")
        }
    }
}

pipeline {
    options {
        timeout(time: 4, unit: 'HOURS')
        buildDiscarder(logRotator(artifactDaysToKeepStr: '15', artifactNumToKeepStr: '10', daysToKeepStr: '30', numToKeepStr: '20'))
    }

    parameters {
        booleanParam(name: 'SKIP_ALL_JENKINS_TESTS', defaultValue: false, description: 'Skip all jenkins tests defined in the jenkinsfile')
        booleanParam(name: 'SKIP_JENKINS_INTEGRATION_TESTS', defaultValue: false, description: 'Skip integration tests defined in the jenkinsfile')
        booleanParam(name: 'PUBLISH_ON_ANY_BRANCH', defaultValue: false, description: 'Build and publish to artifactory regardless branch name')
    }

    agent none

    environment {
        MVN = "mvn -B"
        MAVEN_SKIP = "-Pskip-static-checks -Dweb.console.skip=true -Dmaven.javadoc.skip=true"
        MAVEN_SKIP_TESTS = "-Pskip-tests"
        DOCKER_IP = "127.0.0.1"
        APACHE_ARCHIVE_MIRROR_HOST = "https://repo.cnc.imply.io/artifactory/archive-apache-org-remote"
        COMPOSE_HTTP_TIMEOUT = "600"

        ACTIVE_DEV_BRANCH_REGEX = '^monthly$'
        RELEASE_BRANCH_PREFIX_REGEX = '^release\\/'
        RELEASE_BRANCH_SUFFIX_REGEX = '[^/]+$'

        IMAGE_JDK8 = "docker/druid-ci-jdk8:1647252999"
        IMAGE_JDK11 = "docker/druid-ci-jdk11:1620827975"
        IMAGE_JDK15 = "docker/druid-ci-jdk15:1625878101"
        DOCKER_TAG = "${BUILD_TAG}".replaceAll(~/[^A-Za-z0-9_-]/,'_')
        RT_REGISTRY_URL = "https://repo.cnc.imply.io/artifactory"
        RT_REGISTRY_CREDS = "repo.qa.imply.io"
        RT_SERVER_ID = "repo-qa-imply-io"
        RT_DOCKER_REPOSITORY_PREFIX = "repo.cnc.imply.io"
        DOCKER_REGISTRY_MIRROR = "https://registry-mirror.cnc.imply.io:443"
        DOCKER_AGENT_LABEL = "ubuntu-sysbox-1621268334"

    }

    stages {
        stage('Cancel old builds, check labels') {
            steps {
                script {
                    // Do not cancel previous monthly builds
                    if (env.BRANCH_NAME != 'monthly') {
                        cancelPreviousBuilds()
                    }
                    if (env.CHANGE_ID && pullRequest.labels.contains("Don't Build")) {
                        currentBuild.result = 'ABORTED'
                        error("Tagged Don't Build")
                    }
                    // Set initial value of JAVA_CODE_CHANGES here because value set in env block cannot be changed
                    env.JAVA_CODE_CHANGES = "True"
                    env.DOC_CODE_CHANGES  =  "True"
                    env.CONSOLE_CODE_CHANGES = "True"
                }
            }
        }

        stage('Maven install') {
            when {
                expression { !params.SKIP_ALL_JENKINS_TESTS }
                beforeAgent true
            }
            environment {
                DOCKER_AGENT_ARGS = "-u root:root --runtime=sysbox-runc -e RUN_DOCKER=1"
            }
            parallel {
                stage('(openjdk8) maven install') {
                    agent {docker {image "${IMAGE_JDK8}"; args "${DOCKER_AGENT_ARGS}"; label "${DOCKER_AGENT_LABEL}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"}}
                    environment { CURRENT_DOCKER_IMAGE="${IMAGE_JDK8}" }
                    steps {
                        mavenInstall()
                        mavenDockerBackup()
                    }
                    post {
                        cleanup {
                            resetWs()
                            dockerCleanup()
                        }
                    }
                }
                stage('(openjdk11) maven install') {
                    agent {docker {image "${IMAGE_JDK11}"; args "${DOCKER_AGENT_ARGS}"; label "${DOCKER_AGENT_LABEL}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"}}
                    environment { CURRENT_DOCKER_IMAGE="${IMAGE_JDK11}" }
                    steps {
                        mavenInstall()
                        mavenDockerBackup()

                    }
                    post {
                        cleanup {
                            resetWs()
                            dockerCleanup()
                        }
                    }
                }
            }
        }
        stage('set build version') {
            agent {
                docker {
                    image 'docker/buildabear:20200923'
                    args '-u root:root'
                    label "${DOCKER_AGENT_LABEL}"
                    registryUrl "${RT_REGISTRY_URL}"
                    registryCredentialsId "${RT_REGISTRY_CREDS}"
                }
            }
            steps {
                script {
                    setupMavenSettings()
                    env.BUILD_VERSION = sh(
                            script: '''#!/bin/bash -eux
                            echo -e '#!/bin/bash\\nPATH=/opt/maven/apache-maven-3.5.4/bin:$PATH exec mvn -B $@' > /usr/bin/mvn
                            chmod u+x /usr/bin/mvn


                            COMMIT_SHA=$(git rev-parse HEAD)
                            if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                            else
                                COMMIT_SHA=$(git rev-parse HEAD^)
                                if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                    UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                else
                                    echo "head branch commit SHA is not found"
                                    exit 1
                                fi
                            fi

                            cd $WORKSPACE

                            INIT_DRUID_VERSION=$(mvn org.apache.maven.plugins:maven-help-plugin:3.2.0:evaluate -Dexpression=project.version -q -DforceStdout)
                            DRUID_VERSION=${INIT_DRUID_VERSION%%-SNAPSHOT}
                            mvn versions:set -DnewVersion=$DRUID_VERSION  -DgenerateBackupPoms=false
                            echo "${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}"''',
                            returnStdout: true).tokenize().last().trim()
                    echo "${BUILD_VERSION}"
                }
            }
        }

        stage("Build and publish pre test") {
            when {
                anyOf {
                    expression { (env.CHANGE_BRANCH ?: env.BRANCH_NAME) ==~ env.ACTIVE_DEV_BRANCH_REGEX }
                    expression { (env.CHANGE_BRANCH ?: env.BRANCH_NAME) ==~ (env.RELEASE_BRANCH_PREFIX_REGEX + env.RELEASE_BRANCH_SUFFIX_REGEX) }
                    expression { params.PUBLISH_ON_ANY_BRANCH }
                }
                beforeAgent true
            }
            matrix {
                axes {
                    axis {
                        name 'BUILD_PROFILE'
                        values 'dist', 'imply-saas', 'dist-hadoop2'
                    }
                }
                stages {
                    stage('profile build and upload') {
                        environment {
                            GIT_ASKPASS = "/tmp/askpass.sh"
                            GITHUB_NETRC = "/tmp/github-netrc"
                            STAGING_BASE_DIR= "/tmp/druid-build/stage"
                            TMP_DIR = "/tmp/druid-build/tmp"
                            PLAYWRIGHT_BROWSERS_PATH = "/tmp/druid-build/ms-playwright"
                            BUILD_DIR = "/tmp/druid-build"
                            ARTIFACT_NAME = sh(script:"""#!/bin/bash
                                if [ "${BUILD_PROFILE}" == "imply-saas" ]; then
                                    echo "druid-saas"
                                elif [ "${BUILD_PROFILE}" == "dist-hadoop2" ]; then
                                    echo "druid-hadoop2"
                                else
                                    echo "druid"
                                fi
                            """, returnStdout: true).trim()
                            ARTIFACTORY_BUILD_NAME = sh(script:"""#!/bin/bash
                                if [ "${BUILD_PROFILE}" == "imply-saas" ]; then
                                    echo "druid.saas"
                                elif [ "${BUILD_PROFILE}" == "dist-hadoop2" ]; then
                                    echo "druid.hadoop2"
                                else
                                    echo "druid"
                                fi
                            """, returnStdout: true).trim()
                        }
                        agent {
                            docker {
                                image 'docker/buildabear:20200923'
                                args '-u root:root'
                                label "${DOCKER_AGENT_LABEL}"
                                registryUrl "${RT_REGISTRY_URL}"
                                registryCredentialsId "${RT_REGISTRY_CREDS}"
                            }
                        }
                        stages {
                            stage('prepare to build') {
                                steps {
                                    script {
                                        def gitCredsId = scm.getUserRemoteConfigs()[0].getCredentialsId()
                                        withCredentials([usernamePassword(credentialsId: gitCredsId, usernameVariable: 'GIT_CREDS_USR', passwordVariable: 'GIT_CREDS_PSW')]) {
                                            sh script:'''#!/bin/bash -eux
                                                echo -e '#!/bin/bash\nPATH=/opt/maven/apache-maven-3.5.4/bin:$PATH exec mvn -B $@' > /usr/bin/mvn
                                                chmod u+x /usr/bin/mvn

                                                cat <<EOF > "${GIT_ASKPASS}"
                                                #!/bin/sh
                                                case "\\$1" in
                                                    Username*) echo "${GIT_CREDS_USR}" ;;
                                                    Password*) echo "${GIT_CREDS_PSW}" ;;
                                                esac
                                                EOF

                                                cat <<EOF > "${GITHUB_NETRC}"
                                                machine api.github.com
                                                login ${GIT_CREDS_USR}
                                                password ${GIT_CREDS_PSW}
                                                EOF

                                                chmod u+x "${GIT_ASKPASS}"
                                                chmod 400 "${GITHUB_NETRC}"
                                            '''.replaceAll(/\n\s+/, "\n"),
                                               label: 'prepare for build'
                                        }
                                    }
                                }
                            }
                            stage('build') {
                                steps {
                                    setupMavenSettings()
                                    sh script:'''#!/bin/bash -eux
                                        COMMIT_SHA=$(git rev-parse HEAD)
                                        if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                            UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                        else
                                            COMMIT_SHA=$(git rev-parse HEAD^)
                                            if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                                UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                            else
                                                echo "head branch commit SHA is not found"
                                                exit 1
                                            fi
                                        fi

                                        # Matching the version from pom.xml
                                        DERBYTOOLS_VERSION="10.14.2.0"

                                        # Imply includes additional hadoop libraries (e.g., hadoop-aws)
                                        if [ "${BUILD_PROFILE}" == "dist-hadoop2" ]; then
                                            HADOOP_VERSION="2.8.5"
                                            HADOOP_PROFILE="hadoop2"
                                        else
                                            HADOOP_VERSION="3.3.5"
                                            HADOOP_PROFILE="hadoop3"
                                        fi

                                        MARIADB_CONNECTOR_VERSION="2.7.3"

                                        cd $WORKSPACE

                                        # remove snapshot suffix from pom files
                                        INIT_DRUID_VERSION=$(mvn org.apache.maven.plugins:maven-help-plugin:3.2.0:evaluate -Dexpression=project.version -q -DforceStdout)
                                        DRUID_VERSION=${INIT_DRUID_VERSION%%-SNAPSHOT}
                                        mvn versions:set -DnewVersion=$DRUID_VERSION  -DgenerateBackupPoms=false

                                        current_java_version=$(java -version 2>&1 >/dev/null | grep 'version' | awk '{print $3}')
                                        echo "Compiling Druid with current Java version set. Java version=$current_java_version"

                                        mvn clean install \
                                            -Dhadoop.compile.version="$HADOOP_VERSION" \
                                            -Danimal.sniffer.skip=true \
                                            -Dcheckstyle.skip=true \
                                            -Denforcer.skip=true \
                                            -Dforbiddenapis.skip=true \
                                            -Djacoco.skip=true \
                                            -Dmaven.javadoc.skip=true \
                                            -Dpmd.skip=true \
                                            -Dspotbugs.skip=true \
                                            -DskipTests \
                                            -Dtar \
                                            -P${BUILD_PROFILE}
                                            -P${HADOOP_PROFILE}

                                        echo "Built Druid, version: $DRUID_VERSION"

                                        # Stage Druid
                                        export STAGING_DIR=${STAGING_BASE_DIR}/${BUILD_PROFILE}
                                        mkdir -p "$STAGING_DIR/dist"
                                        mkdir -p "$TMP_DIR"
                                        tar -C "$TMP_DIR" -xzf "$WORKSPACE"/distribution/target/apache-druid-*-bin.tar.gz
                                        mv "$TMP_DIR"/apache-druid-* "$STAGING_DIR/dist/druid"

                                        # Fetch the MariaDB Client from mariadb.com
                                        MARIADB_CONNECTOR_LOCATION="$STAGING_DIR/dist/druid/extensions/mysql-metadata-storage/mariadb-java-client-${MARIADB_CONNECTOR_VERSION}.jar"
                                        #mkdir -p "$STAGING_DIR/dist/druid/extensions/mysql-metadata-storage"
                                        curl -o "$MARIADB_CONNECTOR_LOCATION" --retry 10 "https://downloads.mariadb.com/Connectors/java/connector-java-${MARIADB_CONNECTOR_VERSION}/mariadb-java-client-${MARIADB_CONNECTOR_VERSION}.jar"

                                        if [ "$(sha1sum "$MARIADB_CONNECTOR_LOCATION" | awk '{print $1}')" != "4a2edc05bd882ad19371d2615c2635dccf8d74f0" ]
                                        then
                                          echo "$MARIADB_CONNECTOR_LOCATION: checksum mismatch" >&2
                                          exit 1
                                        fi

                                        # Add hadoop-aws
                                        (cd "$STAGING_DIR"/dist/druid && java -classpath "lib/*" org.apache.druid.cli.Main tools pull-deps -h "org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}")

                                        # Add derbytools to lib
                                        DERBYTOOLS_LOCATION="$STAGING_DIR/dist/druid/lib/derbytools-$DERBYTOOLS_VERSION.jar"
                                        curl -Lo "$DERBYTOOLS_LOCATION" "https://search.maven.org/remotecontent?filepath=org/apache/derby/derbytools/$DERBYTOOLS_VERSION/derbytools-$DERBYTOOLS_VERSION.jar"

                                        if [ "$(sha1sum "$DERBYTOOLS_LOCATION" | awk '{print $1}')" != "338d5a54b4089c80414fe0ecb3899d521da69b26" ]
                                        then
                                          echo "$DERBYTOOLS_LOCATION: checksum mismatch" >&2
                                          exit 1
                                        fi

                                        # Remove unsupported open-source extensions
                                        for extension in druid-pac4j druid-ranger-security materialized-view-maintenance materialized-view-selection; do
                                            rm -rf "${STAGING_DIR}/dist/druid/extensions/${extension}"
                                        done

                                        # Put druid on a diet
                                        perl - <<'EOT'
                                        use strict;
                                        use File::Basename;

                                        my $dir = "$ENV{STAGING_DIR}/dist/druid";
                                        chdir $dir or die "chdir $dir: $!";

                                        my %jars;
                                        for my $jar (qx!find ./lib -name '*.jar'!, qx!find ./extensions -name '*.jar'!, qx!find ./hadoop-dependencies -name '*.jar'!) {
                                          chomp $jar;
                                          my $jarname = basename($jar);
                                          if (exists $jars{$jarname}) {
                                            my $depth = $jar =~ tr !/!/! - 1;
                                            my $dots = "";
                                            for my $x (1..$depth) {
                                              $dots .= "../";
                                            }
                                            system("ln", "-sf", "${dots}$jars{$jarname}", $jar);
                                          } else {
                                            $jars{$jarname} = $jar;
                                          }
                                        }
                                        EOT

                                        tar -C $STAGING_DIR/dist -czf $WORKSPACE/${ARTIFACT_NAME}-${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}.tar.gz druid

                                        echo "${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}" > $WORKSPACE/build.version

                                    '''.replaceAll(/\n\s+/, "\n"),
                                       label: 'build druid and verify'
                                } // end of steps
                            } // end of stage('build')
                            stage('prepare upload') {
                                steps {
                                    setupMavenSettings()
                                    sh script:'''#!/bin/bash -eux
                                        COMMIT_SHA=$(git rev-parse HEAD)
                                        if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                            UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                        else
                                            COMMIT_SHA=$(git rev-parse HEAD^)
                                            if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                                UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                            else
                                                echo "head branch commit SHA is not found"
                                                exit 1
                                            fi
                                        fi

                                        cd $WORKSPACE

                                        # remove snapshot suffix from pom files
                                        INIT_DRUID_VERSION=$(mvn org.apache.maven.plugins:maven-help-plugin:3.2.0:evaluate -Dexpression=project.version -q -DforceStdout)
                                        DRUID_VERSION=${INIT_DRUID_VERSION%%-SNAPSHOT}
                                        mvn versions:set -DnewVersion=$DRUID_VERSION  -DgenerateBackupPoms=false

                                        # Stage Druid
                                        export STAGING_DIR=${STAGING_BASE_DIR}/${BUILD_PROFILE}


                                        echo "${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}" > $WORKSPACE/build.version

                                    '''.replaceAll(/\n\s+/, "\n"),
                                       label: 'build druid and verify'
                                } // end of steps
                            } // end of stage('build')

                            stage('upload') {
                                steps {
                                    script {
                                        def release_version = "None"
                                        def headBranch = env.CHANGE_BRANCH ?: env.BRANCH_NAME
                                        if(headBranch ==~ (env.RELEASE_BRANCH_PREFIX_REGEX + env.RELEASE_BRANCH_SUFFIX_REGEX)) {
                                            release_version = (headBranch =~ env.RELEASE_BRANCH_SUFFIX_REGEX).getAt(0)
                                        } else if(headBranch ==~ env.ACTIVE_DEV_BRANCH_REGEX) {
                                            release_version = sh(
                                                    script: '''#!/bin/bash -e
                                                    curl -s --netrc-file ${GITHUB_NETRC} \
                                                    -H "Accept: application/vnd.github.v3.raw" \
                                                    https://api.github.com/repos/implydata/imply-release/contents/monthly-release.version |  \
                                                    tee /dev/stderr
                                                ''',
                                                    returnStdout: true
                                            ).trim()
                                        }
                                        def artifact_build_version = sh(
                                                script: 'cat build.version',
                                                returnStdout: true
                                        ).trim()
                                        def repo_url = sh(
                                                script: 'git config --get remote.origin.url',
                                                returnStdout: true
                                        ).trim()
                                        rtUpload (
                                                serverId: "${RT_SERVER_ID}",
                                                spec: """{
                                                "files": [
                                                    {
                                                      "pattern": "${ARTIFACT_NAME}-${artifact_build_version}.tar.gz",
                                                      "target": "tgz-local/${ARTIFACT_NAME}/",
                                                      "props": "release.version=${release_version};BVT=InProgress;build.url=${BUILD_URL};vcs.url=${repo_url};vcs.branch=${headBranch};build.version=${artifact_build_version}"
                                                    }
                                                ]
                                            }""",
                                                buildName: "${ARTIFACTORY_BUILD_NAME}",
                                                buildNumber: "${BUILD_NUMBER}"
                                        )
                                        // this is used to know whether or not to mark as failure. And avoid errors for something that fails before the artifact is uploaded.
                                        env.BUILD_UPLOAD_SUCCESS = true
                                    }
                                }
                            } // end of stage('upload')
                        } // end of stages
                        post {
                            cleanup {
                                resetWs()
                            }
                        }
                    } // end of stage('profile build and upload')
                } // end of stages
            } // end of matrix
        } // end of stage("Build and publish")

        stage('Tests - phase 2') {
            when {
                expression { !params.SKIP_JENKINS_INTEGRATION_TESTS && !params.SKIP_ALL_JENKINS_TESTS && env.JAVA_CODE_CHANGES == "True" }
                beforeAgent true
            }
            environment {
                DOCKER_AGENT_ARGS = "-u root:root --runtime=sysbox-runc -e RUN_DOCKER=1"
            }
            parallel {
                // example of integration tests stage:
                stage('(Compile=openjdk8, Run=openjdk8) imply query integration tests') {
                    agent {
                        docker {
                            image "${IMAGE_JDK8}-${DOCKER_TAG}"
                            args "${DOCKER_AGENT_ARGS}"
                            registryUrl "${RT_REGISTRY_URL}"
                            registryCredentialsId "${RT_REGISTRY_CREDS}"
                            label "${DOCKER_AGENT_LABEL}"
                        }
                    }
                    steps {
                        // integration tests here:
                        // jvm runtime is specified here:
                        implyQueryIntegrationTests('-Djvm.runtime=8')
                    }
                    post {
                        // collect artifacts (docker logs and task logs):
                        always { buildArtifacts(env.STAGE_NAME) }
                        // clean untracked files from cloned repo:
                        cleanup { resetWs() }
                    }
                }

                // here is short format of integration tests stage:
                stage('(Compile=openjdk8, Run=openjdk8) imply keycloak security integration tests') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { implyKeycloakSecurityIntegrationTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) imply async query result download integration tests') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { implyAsyncDownloadIntegrationTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) imply s3 tests') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { implyS3Tests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) s3 deep storage test') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { s3DeepStorageTests('-Djvm.runtime=8', '') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) s3 deep storage test with Hadoop2') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { s3DeepStorageTests('-Djvm.runtime=8', '-PHadoop2') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) azure deep storage test') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { azureDeepStorageTests('-Djvm.runtime=8', '') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) azure deep storage test with Hadoop2') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { azureDeepStorageTests('-Djvm.runtime=8', '-PHadoop2') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) hdfs deep storage test') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { hdfsDeepStorageTests('-Djvm.runtime=8', '') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) hdfs deep storage test with Hadoop2') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { hdfsDeepStorageTests('-Djvm.runtime=8', '-PHadoop2') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) gcs deep storage test') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { gcsDeepStorageTests('-Djvm.runtime=8', '') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) gcs deep storage test with Hadoop2') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { gcsDeepStorageTests('-Djvm.runtime=8', '-PHadoop2') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) kinesis deep storage test') {
                    when {
                        // kinesis deep storage test currently running in a downstream pipeline as it is flaky
                        // Jenkins link - https://ci.cnc.imply.io/job/monthly-flaky-tests-main/
                        // Jenkins file path in repo - Jenkins/jenkinsfile-monthly-flaky-tests
                        expression { "disabling in main jenkins pipeline and added it to monthly-flaky-tests"  == "OK" }
                    }
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { kinesisDeepStorageTests('-Djvm.runtime=8', '') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) kinesis deep storage test with Hadoop2') {
                    when {
                        // kinesis deep storage test currently running in a downstream pipeline as it is flaky
                        // Jenkins link - https://ci.cnc.imply.io/job/monthly-flaky-tests-main/
                        // Jenkins file path in repo - Jenkins/jenkinsfile-monthly-flaky-tests
                        expression { "disabling in main jenkins pipeline and added it to monthly-flaky-tests"  == "OK" }
                    }
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { kinesisDeepStorageTests('-Djvm.runtime=8', '-PHadoop2') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }


                // -Djvm.runtime=11

                stage('(Compile=openjdk8, Run=openjdk11) imply query integration tests') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { implyQueryIntegrationTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) imply keycloak security integration tests') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { implyKeycloakSecurityIntegrationTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) s3 deep storage test') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { s3DeepStorageTests('-Djvm.runtime=11', '') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) s3 deep storage test with Hadoop2') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { s3DeepStorageTests('-Djvm.runtime=11', '-PHadoop2') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) azure deep storage test') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { azureDeepStorageTests('-Djvm.runtime=11', '') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) azure deep storage test with Hadoop2') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { azureDeepStorageTests('-Djvm.runtime=11', '-PHadoop2') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) hdfs deep storage test') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { hdfsDeepStorageTests('-Djvm.runtime=11', '') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) hdfs deep storage test with Hadoop2') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { hdfsDeepStorageTests('-Djvm.runtime=11', '-PHadoop2') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) gcs deep storage test') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { gcsDeepStorageTests('-Djvm.runtime=11', '') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) gcs deep storage test with Hadoop2') {
                    agent { docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "${DOCKER_AGENT_ARGS}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    steps { gcsDeepStorageTests('-Djvm.runtime=11', '-PHadoop2') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }
            } // end of parallel
        } // end of stage('Checks')

        stage("Tag verified builds") {
            when {
                anyOf {
                    expression { (env.CHANGE_BRANCH ?: env.BRANCH_NAME) ==~ env.ACTIVE_DEV_BRANCH_REGEX }
                    expression { (env.CHANGE_BRANCH ?: env.BRANCH_NAME) ==~ (env.RELEASE_BRANCH_PREFIX_REGEX + env.RELEASE_BRANCH_SUFFIX_REGEX) }
                    expression { params.PUBLISH_ON_ANY_BRANCH }
                }
                beforeAgent true
            }

            matrix {
                axes {
                    axis {
                        name 'BUILD_PROFILE'
                        values 'dist', 'imply-saas', 'dist-hadoop2'
                    }
                }
                stages {
                    stage('profile tag BVT Pass') {
                        environment {
                            GIT_ASKPASS = "/tmp/askpass.sh"
                            GITHUB_NETRC = "/tmp/github-netrc"
                            STAGING_BASE_DIR= "/tmp/druid-build/stage"
                            TMP_DIR = "/tmp/druid-build/tmp"
                            BUILD_DIR = "/tmp/druid-build"
                            ARTIFACT_NAME = sh(script:"""#!/bin/bash
                                if [ "${BUILD_PROFILE}" == "imply-saas" ]; then
                                    echo "druid-saas"
                                elif [ "${BUILD_PROFILE}" == "dist-hadoop2" ]; then
                                    echo "druid-hadoop2"
                                else
                                    echo "druid"
                                fi
                            """, returnStdout: true).trim()
                        }
                        agent {
                            docker {
                                image 'docker/buildabear:20200923'
                                args '-u root:root'
                                label "${DOCKER_AGENT_LABEL}"
                                registryUrl "${RT_REGISTRY_URL}"
                                registryCredentialsId "${RT_REGISTRY_CREDS}"
                            }
                        }
                        stages {
                            stage('Set jenkins-verified property') {
                                steps {
                                    script {
                                        def release_version = "None"
                                        def headBranch = env.CHANGE_BRANCH ?: env.BRANCH_NAME
                                        if(headBranch ==~ (env.RELEASE_BRANCH_PREFIX_REGEX + env.RELEASE_BRANCH_SUFFIX_REGEX)) {
                                            release_version = (headBranch =~ env.RELEASE_BRANCH_SUFFIX_REGEX).getAt(0)
                                        } else if(headBranch ==~ env.ACTIVE_DEV_BRANCH_REGEX) {
                                            release_version = sh(
                                                    script: '''#!/bin/bash -e
                                                    curl -s --netrc-file ${GITHUB_NETRC} \
                                                    -H "Accept: application/vnd.github.v3.raw" \
                                                    https://api.github.com/repos/implydata/imply-release/contents/monthly-release.version |  \
                                                    tee /dev/stderr
                                                ''',
                                                    returnStdout: true
                                            ).trim()
                                        }
                                        echo "setting BVT=Pass for tgz-local/${ARTIFACT_NAME}/${ARTIFACT_NAME}-${BUILD_VERSION}.tar.gz"
                                        rtSetProps (
                                                serverId: "${RT_SERVER_ID}",
                                                spec: """{
                                                "files": [
                                                    {
                                                      "pattern": "tgz-local/${ARTIFACT_NAME}/${ARTIFACT_NAME}-${BUILD_VERSION}.tar.gz",
                                                      "props": "BVT=InProgress"
                                                    }
                                                ]
                                            }""",
                                                props: 'BVT=Pass',
                                                failNoOp: true
                                        )
                                    }
                                }
                            } // end of stage('Set BVT=pass property')
                        } // end of stages
                        post {
                            cleanup {
                                resetWs()
                            }
                        }
                    } // end of stage('profile build and upload')
                } // end of stages
            } // end of matrix
        } // end of stage("Tag verified builds")
    } // end of stages
    post {
        failure {
            script {
                if (isEligibleToNotify()) {
                    library 'imply-shared-library'
                    failedDruidBVTNotification(currentBuild)
                }
            }
            node('jenkinsOnDemand') {
                // notify about failed build
                script {
                    echo "checking to see if should set BVT as Fail for ${BUILD_VERSION}"
                    if (env.BUILD_UPLOAD_SUCCESS == null || !env.BUILD_UPLOAD_SUCCESS) {
                        echo "Not setting BVT as Fail"
                        return
                    }
                    echo "Setting BVT as Fail"
                    markBvtAsFail('druid', "${BUILD_VERSION}")
                    markBvtAsFail('druid-saas', "${BUILD_VERSION}")
                    markBvtAsFail('druid-hadoop2', "{$BUILD_VERSION}")
                }
            }
        }
        success {
            // notify about fixed build
            script {
                if (isEligibleToNotify()) {
                    library 'imply-shared-library'
                    fixedDruidBVTNotification(currentBuild)
                }
            }
            // trigger falky tests if the build is successful for monthly branch
            script {
                if (env.BRANCH_NAME == 'monthly') {
                    build(job: 'monthly-flaky-tests-main', wait: false)
                }
            }
        }
    }
}

