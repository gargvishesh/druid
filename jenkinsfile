//////////
////////// System part
//////////


import io.jenkins.blueocean.rest.impl.pipeline.PipelineNodeGraphVisitor
import io.jenkins.blueocean.rest.impl.pipeline.FlowNodeWrapper
import org.jenkinsci.plugins.workflow.support.steps.build.RunWrapper

// Get information about all stages, including the failure causes.
// Returns a list of maps: [[id, displayName, result]]
// The 'errors' member is a list of unique exceptions.

@NonCPS
List<Map> getStageResults( RunWrapper build ) {

    // Get all pipeline nodes that represent stages
    def visitor = new PipelineNodeGraphVisitor( build.rawBuild )
    def stages = visitor.pipelineNodes.findAll{ it.type == FlowNodeWrapper.NodeType.STAGE || it.type == FlowNodeWrapper.NodeType.PARALLEL }

    return stages.collect{ stage ->
        return [
            id: stage.id,
            displayName: stage.displayName,
            result: "${stage.status.result}",
        ]
    }
}

// Get information of all failed stages
@NonCPS
List<Map> getFailedStages( RunWrapper build ) {
    return getStageResults( build ).findAll{ it.result == 'FAILURE' }
}

def setupMavenSettings() {
    script {
        withCredentials([file(credentialsId: 'maven-artifactory-settings', variable: 'MVN_SETTINGS_PATH')]) {
            sh script: '''
                mkdir -p ~/.m2
                cp -f $MVN_SETTINGS_PATH ~/.m2/settings.xml
            ''',
            label: 'setup maven settings'
        }
    }
}

def mavenInstall() {
    setupMavenSettings()
    sh  script: '${MVN} clean install -q -ff ${MAVEN_SKIP} ${MAVEN_SKIP_TESTS} -T 1C',
        label: 'mvn install'
}

def prepareIfconfig() {
    sh script: '''
        echo -e '#!/bin/bash\nPATH=/sbin:$PATH exec ifconfig eth0 $@' > /usr/bin/ifconfig
        chmod +x /usr/bin/ifconfig
        hash -r
    ''',
    label: "prepare ifconfig"
}

def imageJDK8() {
    return 'docker/druid-ci-jdk8:latest'
}

def imageJDK11() {
    return 'docker/druid-ci-jdk11:latest'
}

def rtRegistryUrl() {
    return 'https://repo.qa.imply.io/artifactory'
}

def rtRegistryCredentialsId() {
    return 'repo.qa.imply.io'
}

def dockerAgentArgs() {
    return '-u root:root --runtime=sysbox-runc -e DOCKER_REGISTRY_MIRROR=https://registry-mirror.qa.imply.io:443'
}

def dockerAgentLabel() {
    return 'jenkinsOnDemand'
}

def copyFilesByPattern(filesPattern, destDir) {
    sh script: """#!/bin/bash -x
        shopt -s globstar
        if  ls ${filesPattern}; then
            for fname in ${filesPattern}; do
                fname_root=\$(echo \${fname} | cut -d "/" -f1)
                if ! [[ "\$fname_root" = "${destDir}" ]]; then
                    mkdir -p \$(dirname -- "${destDir}/\${fname}")
                    cp \$fname ${destDir}/\${fname}
                fi
            done
        fi
    """
}

def buildArtifacts(stageName) {
    script {
        def stageArtifactsDirPath = "stage_${stageName.replaceAll(~/[^A-Za-z0-9_-]/,'_')}"
        sh script: "mkdir -p ${stageArtifactsDirPath}"
        // copy logs
        sh script: """#!/bin/bash -x
            [ -d ~/shared/logs ] && cp -R ~/shared/logs ${stageArtifactsDirPath}/ || true
            [ -d ~/shared/tasklogs ] && cp -R ~/shared/tasklogs ${stageArtifactsDirPath}/ || true
        """
        // copy test reports
        copyFilesByPattern("**/target/surefire-reports/*.xml", stageArtifactsDirPath)
        copyFilesByPattern("**/target/failsafe-reports/*.xml", stageArtifactsDirPath)
        // copy top-level jacoco reports
        copyFilesByPattern("**/target/*.exec", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/*.html", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/*.xml", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/*.csv", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/jacoco-resources/*", stageArtifactsDirPath)
        // copy detailed jacoco reports
        copyFilesByPattern("**/jacoco/**/*.html", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/**/*.xml", stageArtifactsDirPath)
        copyFilesByPattern("**/jacoco/**/*.csv", stageArtifactsDirPath)
        // fixate artifacts
        archiveArtifacts artifacts: "${stageArtifactsDirPath}/**", allowEmptyArchive: true
    }
}

def resetWs() {
    sh script: "git clean -fdx", label: "Clean up everything but files from git"
}

def activeDevBranchRegex() {
    return '^(\\d+\\.\\d+\\.\\d+-iap|master)$'
}

def releaseBranchPrefixRegex() {
    return '^release\\/'
}

def releaseBranchSuffixRegex() {
    return '[^/]+$'
}

def releaseBranchRegex() {
    return releaseBranchPrefixRegex() + releaseBranchSuffixRegex()
}

def headBranchName() {
    return env.CHANGE_BRANCH ?: env.BRANCH_NAME
}

def isEligibleToNotify() {
    return  !(env.BRANCH_NAME == "master") \
            && !env.CHANGE_ID \
            && (env.BRANCH_NAME ==~ activeDevBranchRegex() || env.BRANCH_NAME ==~ releaseBranchRegex())
}

def notify(fixed, failedStageNames) {
    def slackMessageColor = fixed ? '#33CC33' : '#FF3333'
    def titleBeginning = fixed ? 'Fixed' : 'Failed'

    def artifactSlackTip = fixed ? "" : "\ncheck artifacts (docker logs): ${env.BUILD_URL}artifact"
    def artifactEmailTip = fixed ? "" : "<br/>check artifacts (docker logs) :${env.BUILD_URL}artifact"

    def failedStageSlackTip = fixed ? "" : "\nFailed stages:\n" + failedStageNames.collect { "* ${it}" }.join("\n")
    def failedStageEmailTip = fixed ? "" : "<br/>Failed stages:<br/>" + failedStageNames.collect { "* ${it}" }.join("<br/>")

    def slackTarget = "#javabeans"
    def emailTarget = "eng.druid@imply.io"

    try {
        slackSend   color: slackMessageColor,
                    channel: slackTarget,
                    message: """
                        *${titleBeginning} implydata/druid build: ${env.BRANCH_NAME}#${env.BUILD_NUMBER}*
                        classic link: ${env.BUILD_URL}
                        blueocean link: ${env.RUN_DISPLAY_URL} ${artifactSlackTip} ${failedStageSlackTip}
                    """.replaceAll(/\n\s+/, "\n")
    } catch (err) {
        echo "Couldn't send slack notification"
        echo "Exception: ${err}"
    }

    try {
        mail    body: """
                    classic link: ${env.BUILD_URL}
                    <br/>blueocean link: ${env.RUN_DISPLAY_URL} ${artifactEmailTip} ${failedStageEmailTip}
                """.replaceAll(/\n\s+/, "\n"),
                cc: '', bcc: '', replyTo: '',
                from: 'jenkins@qa.imply.io',
                mimeType: 'text/html',
                subject: "${titleBeginning} implydata/druid build: ${env.BRANCH_NAME}#${env.BUILD_NUMBER}",
                to: emailTarget
    } catch (err) {
        echo "Couldn't send email notification"
        echo "Exception: ${err}"
    }
}

def implyQueryIntegrationTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    sh script: """
        \${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
        -Dgroups=query \
        -Dit.indexer=middleManager \
        ${jvmRuntimeOpt} \
        -Ddruid.test.config.extraDatasourceNameSuffix="" \
        -ff \${MAVEN_SKIP} -Djacoco.skip=true
    """,
    label: "imply query integration tests with ${jvmRuntimeOpt}"
}

def implyIngestServiceIntegrationTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    sh script: """
        \${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
        -Dgroups=ingest-service \
        -Dit.indexer=middleManager \
        ${jvmRuntimeOpt} \
        -ff \${MAVEN_SKIP} -Djacoco.skip=true
    """,
    label: "imply ingest service integration tests with ${jvmRuntimeOpt}"
}

def s3DeepStorageTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    script {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'aws', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
            def cloudpath = UUID.randomUUID().toString()
            sh script: """
                aws s3 sync \
                ./integration-tests/src/test/resources/data/batch_index/json/ s3://druid-qa/${cloudpath} \
                --exclude "*" --include "wikipedia_index_data*.json"
            """
            writeFile   file: "jenkins/s3-config",
                        text: "druid_storage_type=s3\ndruid_storage_bucket=druid-qa\ndruid_storage_baseKey=${cloudpath}\ndruid_s3_accessKey=${AWS_ACCESS_KEY_ID}\ndruid_s3_secretKey=${AWS_SECRET_ACCESS_KEY}\nAWS_REGION=us-east-1\ndruid_extensions_loadList=[\"druid-s3-extensions\",\"druid-hdfs-storage\"]"
            try {
                sh script: """
                    \${MVN} verify -P integration-tests -pl integration-tests \
                    -Dgroups=s3-deep-storage \
                    -Doverride.config.path=\${WORKSPACE}/jenkins/s3-config \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.cloudBucket=druid-qa \
                    -Ddruid.test.config.cloudPath=${cloudpath}/ \
                    -Ddocker.build.hadoop=true \
                    -Dstart.hadoop.docker=true \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "s3-deep-storage with ${jvmRuntimeOpt}"
            }
            finally {
                sh script: "aws s3 rm s3://druid-qa/${cloudpath}  --recursive"
            }
        }
    }
}

def kinesisDeepStorageTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    script {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId:  'aws', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
            lock('awsResource') {
                writeFile   file: "jenkins/kinesis-config",
                            text: "druid_kinesis_accessKey=${AWS_ACCESS_KEY_ID}\ndruid_kinesis_secretKey=${AWS_SECRET_ACCESS_KEY}\nAWS_REGION=us-east-1\ndruid_extensions_loadList=[\"druid-kinesis-indexing-service\"]"
                sh script: """
                    \${MVN} verify -P integration-tests -pl integration-tests \
                    -Dgroups=kinesis-index \
                    -Doverride.config.path=\${WORKSPACE}/jenkins/kinesis-config \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.streamEndpoint=kinesis.us-east-1.amazonaws.com \
                    -Ddocker.build.hadoop=true \
                    -Dstart.hadoop.docker=true \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "kinesis-deep-storage with ${jvmRuntimeOpt}"
            }
        }
    }
}

def azureDeepStorageTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    script {
        withCredentials([usernamePassword(credentialsId: 'azure_credentials', usernameVariable: 'AZURE_ACCOUNT', passwordVariable: 'AZURE_KEY')]) {
            def containerName = UUID.randomUUID().toString()
            sh script: """
                az storage container create -n ${containerName} \
                --public-access blob \
                --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY}
            """
            writeFile   file: "jenkins/azure-config",
                        text: "druid_storage_type=azure\ndruid_azure_account=${AZURE_ACCOUNT}\ndruid_azure_key=${AZURE_KEY}\ndruid_azure_container=${containerName}\ndruid_extensions_loadList=[\"druid-azure-extensions\",\"druid-hdfs-storage\"]"
            try {
                sh script: """
                    az storage blob upload-batch \
                    --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY} \
                    -d ${containerName} \
                    --source ./integration-tests/src/test/resources/data/batch_index/json/ --pattern "wikipedia_index_data*.json"
                """
                sh script: """
                    \${MVN} verify -P integration-tests -pl integration-tests \
                    -Dgroups=azure-deep-storage \
                    -Doverride.config.path=\${WORKSPACE}/jenkins/azure-config \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.cloudBucket=${containerName} \
                    -Ddruid.test.config.cloudPath= \
                    -Ddocker.build.hadoop=true \
                    -Dstart.hadoop.docker=true \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "azure-deep-storage with ${jvmRuntimeOpt}"
            }
            finally {
                sh script: """
                    az storage container delete -n ${containerName}\
                    --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY}
                """
            }
        }
    }
}

def gcsDeepStorageTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    script {
        withCredentials([file(credentialsId: 'gcs-bucket-qa', variable: 'GC_KEY')]) {
            def cloudpath = "gcs-test-${UUID.randomUUID().toString()}/"
            def bucket = "imply-qa-testing"

            writeFile   file: "jenkins/gcs-config",
                        text: "druid_storage_type=google\ndruid_google_bucket=${bucket}\ndruid_google_prefix=${cloudpath}\ndruid_extensions_loadList=[\"druid-google-extensions\",\"druid-hdfs-storage\"]\nGOOGLE_APPLICATION_CREDENTIALS=/shared/docker/credentials/creds.json"
            sh script: """
                mkdir -p jenkins/gcs
                cp \${GC_KEY} jenkins/gcs/creds.json
                chmod 764 jenkins/gcs/creds.json
                gsutil \
                    -o Credentials:gs_service_key_file=\${WORKSPACE}/jenkins/gcs/creds.json \
                    cp \${WORKSPACE}/integration-tests/src/test/resources/data/batch_index/json/wikipedia_index_data*.json \
                    gs://${bucket}/${cloudpath}
                """,
                label: "copy gcs creds"
            try {
                sh script: """
                    \${MVN} verify -P integration-tests -pl integration-tests \
                    -Doverride.config.path=\${WORKSPACE}/jenkins/gcs-config \
                    -Dresource.file.dir.path=\${WORKSPACE}/jenkins/gcs \
                    -Dgroups=gcs-deep-storage \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.cloudBucket=${bucket} \
                    -Ddruid.test.config.cloudPath=${cloudpath} \
                    -Ddocker.build.hadoop=true \
                    -Dstart.hadoop.docker=true \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "gcs-deep-storage with ${jvmRuntimeOpt}"
            } finally {
                sh script: "gsutil -o Credentials:gs_service_key_file=\${WORKSPACE}/jenkins/gcs/creds.json rm -r gs://${bucket}/${cloudpath}"
            }
        }
    }
}

def hdfsDeepStorageTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    writeFile   file:   "jenkins/hdfs-config",
                text:   "druid_storage_type=hdfs\n" +
                        "druid_storage_storageDirectory=/druid/segments\n" +
                        "druid_extensions_loadList=[\"druid-hdfs-storage\"]\n" +
                        "druid_indexer_logs_type=hdfs\n" +
                        "druid_indexer_logs_directory=/druid/indexing-logs"
    sh script: """
        \${MVN} verify -P integration-tests -pl integration-tests \
        -Doverride.config.path=\${WORKSPACE}/jenkins/hdfs-config \
        -Dgroups=hdfs-deep-storage \
        -Ddocker.build.hadoop=true \
        -Dstart.hadoop.docker=true \
        ${jvmRuntimeOpt} \
        -Ddruid.test.config.extraDatasourceNameSuffix="" \
        -Dit.test=ITHdfsToHdfsParallelIndexTest \
        -ff \${MAVEN_SKIP} -Djacoco.skip=true
    """,
    label: "hdfs-deep-storage with ${jvmRuntimeOpt}"
}


//////////
////////// Pipeline part
//////////

pipeline {
    options {
        timeout(time: 2, unit: 'HOURS')
        buildDiscarder(logRotator(artifactDaysToKeepStr: '15', artifactNumToKeepStr: '10', daysToKeepStr: '30', numToKeepStr: '20'))
    }

    parameters {
        booleanParam(name: 'SKIP_ALL_JENKINS_TESTS', defaultValue: false, description: 'Skip all jenkins tests defined in the jenkinsfile')
        booleanParam(name: 'SKIP_JENKINS_INTEGRATION_TESTS', defaultValue: false, description: 'Skip integration tests defined in the jenkinsfile')
        booleanParam(name: 'PUBLISH_ON_ANY_BRANCH', defaultValue: false, description: 'Build and publish to artifactory regardless branch name')
    }

    agent none

    environment {
        MVN = "mvn -B"
        MAVEN_SKIP = "-Danimal.sniffer.skip=true -Dcheckstyle.skip=true -Ddruid.console.skip=true -Denforcer.skip=true -Dforbiddenapis.skip=true -Dmaven.javadoc.skip=true -Dpmd.skip=true -Dspotbugs.skip=true"
        MAVEN_SKIP_TESTS = "-DskipTests -Djacoco.skip=true"
        MAVEN_OPTS = "-Xms4g -Xmx8g -XX:MaxDirectMemorySize=2048m"
        DOCKER_IP = "127.0.0.1"
        ZK_VERSION = "3.5"
    }

    stages {
        stage('Checks') {
            when {
                expression { !params.SKIP_ALL_JENKINS_TESTS }
                beforeAgent true
            }
            parallel {
                stage('security vulnerabilities') {
                    agent {
                        docker {
                            image 'maven:3.6.3-jdk-8'
                            args '--memory=8g --memory-reservation=4g -u root:root'
                            label 'jenkinsOnDemandMultiExec'
                        }
                    }
                    steps {
                        mavenInstall()
                        sh script: '''#!/bin/bash
                            set -o pipefail

                            ${MVN} dependency-check:aggregate -pl '!integration-tests,!integration-tests-imply,!api-contract' || \
                            { echo "The OWASP dependency check has found security vulnerabilities. Please use a newer version
                            of the dependency that does not have vulnerabilities. If the analysis has false positives,
                            they can be suppressed by adding entries to owasp-dependency-check-suppressions.xml (for more
                            information, see https://jeremylong.github.io/DependencyCheck/general/suppression.html).
                            " && false; }
                        ''',
                        label: 'dependency check'
                    }
                }

                // example of integration tests stage:
                stage('(Compile=openjdk8, Run=openjdk8) imply query integration tests') {
                    when {
                        expression { !params.SKIP_JENKINS_INTEGRATION_TESTS } // skip on demand (based on parameter)
                        beforeAgent true
                    }
                    agent {
                        docker {
                            // docker image:
                            image imageJDK8()
                            // docker run arguments:
                            args dockerAgentArgs()
                            // configure artifactory registry credentials:
                            registryUrl rtRegistryUrl()
                            registryCredentialsId rtRegistryCredentialsId()
                            // specify jenkins agent node where container should start:
                            label dockerAgentLabel()
                        }
                    }
                    steps {
                        // integration tests here:
                        // jvm runtime is specified here:
                        implyQueryIntegrationTests('-Djvm.runtime=8')
                    }
                    post {
                        // collect artifacts (docker logs and task logs):
                        always { buildArtifacts(env.STAGE_NAME) }
                        // clean untracked files from cloned repo:
                        cleanup { resetWs() }
                    }
                }

                // here is short format of integration tests stage:
                stage('(Compile=openjdk8, Run=openjdk8) imply ingest service integration tests') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { implyIngestServiceIntegrationTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) s3 deep storage test') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { s3DeepStorageTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) azure deep storage test') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { azureDeepStorageTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) hdfs deep storage test') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { hdfsDeepStorageTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) gcs deep storage test') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { gcsDeepStorageTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk8) kinesis deep storage test') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { kinesisDeepStorageTests('-Djvm.runtime=8') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                // -Djvm.runtime=11

                stage('(Compile=openjdk8, Run=openjdk11) imply query integration tests') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { implyQueryIntegrationTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) imply ingest service integration tests') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { implyIngestServiceIntegrationTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) s3 deep storage test') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { s3DeepStorageTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) azure deep storage test') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { azureDeepStorageTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) hdfs deep storage test') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { hdfsDeepStorageTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(Compile=openjdk8, Run=openjdk11) gcs deep storage test') {
                    when { expression { !params.SKIP_JENKINS_INTEGRATION_TESTS }; beforeAgent true }
                    agent {docker {image imageJDK8();args dockerAgentArgs();registryUrl rtRegistryUrl();registryCredentialsId rtRegistryCredentialsId();label dockerAgentLabel()}}
                    steps { gcsDeepStorageTests('-Djvm.runtime=11') }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }
            } // end of parallel
        } // end of stage('Checks')

        stage("Build and publish") {
            when {
                anyOf {
                    expression { headBranchName() ==~ activeDevBranchRegex() }
                    expression { headBranchName() ==~ releaseBranchRegex() }
                    expression { params.PUBLISH_ON_ANY_BRANCH }
                }
                beforeAgent true
            }

            matrix {
                axes {
                    axis {
                        name 'BUILD_PROFILE'
                        values 'dist', 'imply-saas'
                    }
                }
                stages {
                    stage('profile build and upload') {
                        environment {
                            GIT_ASKPASS = "/tmp/askpass.sh"
                            GITHUB_NETRC = "/tmp/github-netrc"
                            STAGING_BASE_DIR= "/tmp/druid-build/stage"
                            TMP_DIR = "/tmp/druid-build/tmp"
                            BUILD_DIR = "/tmp/druid-build"
                            ARTIFACT_NAME = sh(script:"""#!/bin/bash
                                if [ "${BUILD_PROFILE}" == "imply-saas" ]; then
                                    echo "druid-saas"
                                else
                                    echo "druid"
                                fi
                            """, returnStdout: true).trim()
                            ARTIFACTORY_BUILD_NAME = sh(script:"""#!/bin/bash
                                if [ "${BUILD_PROFILE}" == "imply-saas" ]; then
                                    echo "druid.saas"
                                else
                                    echo "druid"
                                fi
                            """, returnStdout: true).trim()
                        }
                        agent {
                            docker {
                                image 'docker/buildabear:20200923'
                                args '-u root:root'
                                label 'jenkinsOnDemand'
                                registryUrl rtRegistryUrl()
                                registryCredentialsId rtRegistryCredentialsId()
                            }
                        }
                        stages {
                            stage('prepare to build') {
                                steps {
                                    script {
                                        def gitCredsId = scm.getUserRemoteConfigs()[0].getCredentialsId()
                                        withCredentials([usernamePassword(credentialsId: gitCredsId, usernameVariable: 'GIT_CREDS_USR', passwordVariable: 'GIT_CREDS_PSW')]) {
                                            sh script:'''#!/bin/bash -eux
                                                echo -e '#!/bin/bash\nPATH=/opt/maven/apache-maven-3.5.4/bin:$PATH exec mvn -B $@' > /usr/bin/mvn
                                                chmod u+x /usr/bin/mvn

                                                cat <<EOF > "${GIT_ASKPASS}"
                                                #!/bin/sh
                                                case "\\$1" in
                                                    Username*) echo "${GIT_CREDS_USR}" ;;
                                                    Password*) echo "${GIT_CREDS_PSW}" ;;
                                                esac
                                                EOF

                                                cat <<EOF > "${GITHUB_NETRC}"
                                                machine api.github.com
                                                login ${GIT_CREDS_USR}
                                                password ${GIT_CREDS_PSW}
                                                EOF

                                                chmod u+x "${GIT_ASKPASS}"
                                                chmod 400 "${GITHUB_NETRC}"
                                            '''.replaceAll(/\n\s+/, "\n"),
                                            label: 'prepare for build'
                                        }
                                    }
                                }
                            }
                            stage('build') {
                                steps {
                                    setupMavenSettings()
                                    sh script:'''#!/bin/bash -eux
                                        COMMIT_SHA=$(git rev-parse HEAD)
                                        if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                            UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                        else
                                            COMMIT_SHA=$(git rev-parse HEAD^)
                                            if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                                UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                            else
                                                echo "head branch commit SHA is not found"
                                                exit 1
                                            fi
                                        fi

                                        NETTY_UPSTREAM_NAME="netty"
                                        NETTY_UPSTREAM_ORG=${NETTY_UPSTREAM_ORG:-"implydata"}
                                        NETTY_UPSTREAM_REPO="https://github.com/$NETTY_UPSTREAM_ORG/$NETTY_UPSTREAM_NAME.git"
                                        NETTY_UPSTREAM_VERSION=${NETTY_UPSTREAM_VERSION:-"3.10.6.Final-iap2"}
                                        NETTY_UPSTREAM_COMMITISH=${NETTY_UPSTREAM_COMMITISH:-"netty-$NETTY_UPSTREAM_VERSION"}
                                        NETTY_UPSTREAM_DIR="$TMP_DIR/$NETTY_UPSTREAM_NAME.git"

                                        DERBYTOOLS_VERSION="10.11.1.1"

                                        # Imply includes additional hadoop libraries (e.g., hadoop-aws)
                                        HADOOP_VERSION="2.8.5"

                                        # This should match the version used in druid
                                        MYSQL_CONNECTOR_VERSION="5.1.48"

                                        # Install our patched version of Netty.
                                        git clone -b "$NETTY_UPSTREAM_COMMITISH" --single-branch --depth 1 "$NETTY_UPSTREAM_REPO" "$NETTY_UPSTREAM_DIR"
                                        (cd "$NETTY_UPSTREAM_DIR" && git checkout "$NETTY_UPSTREAM_COMMITISH")
                                        (cd "$NETTY_UPSTREAM_DIR" && mvn install -DskipTests)

                                        cd $WORKSPACE

                                        # remove snapshot suffix from pom files
                                        INIT_DRUID_VERSION=$(mvn org.apache.maven.plugins:maven-help-plugin:3.2.0:evaluate -Dexpression=project.version -q -DforceStdout)
                                        DRUID_VERSION=${INIT_DRUID_VERSION%%-SNAPSHOT}
                                        mvn versions:set -DnewVersion=$DRUID_VERSION  -DgenerateBackupPoms=false

                                        current_java_version=$(java -version 2>&1 >/dev/null | grep 'version' | awk '{print $3}')
                                        echo "Compiling Druid with current Java version set. Java version=$current_java_version"

                                        mvn clean install \
                                            -Dnetty3.version="$NETTY_UPSTREAM_VERSION" \
                                            -Dhadoop.compile.version="$HADOOP_VERSION" \
                                            -Dmysql.version="$MYSQL_CONNECTOR_VERSION" \
                                            -Danimal.sniffer.skip=true \
                                            -Dcheckstyle.skip=true \
                                            -Denforcer.skip=true \
                                            -Dforbiddenapis.skip=true \
                                            -Djacoco.skip=true \
                                            -Dmaven.javadoc.skip=true \
                                            -Dpmd.skip=true \
                                            -Dspotbugs.skip=true \
                                            -DskipTests \
                                            -T1C \
                                            -Dtar \
                                            -P${BUILD_PROFILE}

                                        echo "Built Druid, version: $DRUID_VERSION"

                                        # Stage Druid
                                        export STAGING_DIR=${STAGING_BASE_DIR}/${BUILD_PROFILE}
                                        mkdir -p "$STAGING_DIR/dist"
                                        tar -C "$TMP_DIR" -xzf "$WORKSPACE"/distribution/target/apache-druid-*-bin.tar.gz
                                        mv "$TMP_DIR"/apache-druid-* "$STAGING_DIR/dist/druid"

                                        # Fetch the MySQL JDBC driver from Maven Central
                                        MYSQL_CONNECTOR_LOCATION="$STAGING_DIR/dist/druid/extensions/mysql-metadata-storage/mysql-connector-java-${MYSQL_CONNECTOR_VERSION}.jar"
                                        mkdir -p "$STAGING_DIR/dist/druid/extensions/mysql-metadata-storage"
                                        curl -o "$MYSQL_CONNECTOR_LOCATION" --retry 10 "https://repo1.maven.org/maven2/mysql/mysql-connector-java/${MYSQL_CONNECTOR_VERSION}/mysql-connector-java-${MYSQL_CONNECTOR_VERSION}.jar"

                                        if [ "$(sha1sum "$MYSQL_CONNECTOR_LOCATION" | awk '{print $1}')" != "9140be77aafa5050bf4bb936d560cbacb5a6b5c1" ]
                                        then
                                          echo "$MYSQL_CONNECTOR_LOCATION: checksum mismatch" >&2
                                          exit 1
                                        fi

                                        # Add hadoop-aws
                                        (cd "$STAGING_DIR"/dist/druid && java -classpath "lib/*" org.apache.druid.cli.Main tools pull-deps -h "org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}")

                                        # Add derbytools to lib
                                        DERBYTOOLS_LOCATION="$STAGING_DIR/dist/druid/lib/derbytools-$DERBYTOOLS_VERSION.jar"
                                        curl -Lo "$DERBYTOOLS_LOCATION" "https://search.maven.org/remotecontent?filepath=org/apache/derby/derbytools/$DERBYTOOLS_VERSION/derbytools-$DERBYTOOLS_VERSION.jar"

                                        if [ "$(sha1sum "$DERBYTOOLS_LOCATION" | awk '{print $1}')" != "10a124a8962c6f8ea70a368d711ce6883889ca34" ]
                                        then
                                          echo "$DERBYTOOLS_LOCATION: checksum mismatch" >&2
                                          exit 1
                                        fi

                                        # Remove unsupported open-source extensions
                                        for extension in druid-pac4j druid-ranger-security materialized-view-maintenance materialized-view-selection; do
                                            rm -rf "${STAGING_DIR}/dist/druid/extensions/${extension}"
                                        done

                                        # Put druid on a diet
                                        perl - <<'EOT'
                                        use strict;
                                        use File::Basename;

                                        my $dir = "$ENV{STAGING_DIR}/dist/druid";
                                        chdir $dir or die "chdir $dir: $!";

                                        my %jars;
                                        for my $jar (qx!find ./lib -name '*.jar'!, qx!find ./extensions -name '*.jar'!, qx!find ./hadoop-dependencies -name '*.jar'!) {
                                          chomp $jar;
                                          my $jarname = basename($jar);
                                          if (exists $jars{$jarname}) {
                                            my $depth = $jar =~ tr !/!/! - 1;
                                            my $dots = "";
                                            for my $x (1..$depth) {
                                              $dots .= "../";
                                            }
                                            system("ln", "-sf", "${dots}$jars{$jarname}", $jar);
                                          } else {
                                            $jars{$jarname} = $jar;
                                          }
                                        }
                                        EOT

                                        tar -C $STAGING_DIR/dist -czf $WORKSPACE/${ARTIFACT_NAME}-${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}.tar.gz druid

                                        echo "${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}" > $WORKSPACE/build.version
                                        echo "${UPSTREAM_COMMIT_SHA:0:8}" > $WORKSPACE/upstreamCommitSha

                                    '''.replaceAll(/\n\s+/, "\n"),
                                    label: 'build druid'
                                } // end of steps
                            } // end of stage('build')

                            stage('upload') {
                                steps {
                                    script {
                                        def release_version = "None"
                                        if(headBranchName() ==~ releaseBranchRegex()) {
                                            release_version = (headBranchName() =~ releaseBranchSuffixRegex()).getAt(0)
                                        } else if(headBranchName() ==~ activeDevBranchRegex()){
                                            release_version = sh(
                                                script: '''#!/bin/bash -e
                                                    curl -s --netrc-file ${GITHUB_NETRC} \
                                                    -H "Accept: application/vnd.github.v3.raw" \
                                                    https://api.github.com/repos/implydata/imply-release/contents/monthly-release.version |  \
                                                    tee /dev/stderr
                                                ''',
                                                returnStdout: true
                                            ).trim()
                                        }
                                        def artifact_build_version = sh(
                                            script: 'cat build.version',
                                            returnStdout: true
                                        ).trim()
                                        def upstream_commit_sha = sh(
                                            script: 'cat upstreamCommitSha',
                                            returnStdout: true
                                        ).trim()
                                        def repo_url = sh(
                                            script: 'git config --get remote.origin.url',
                                            returnStdout: true
                                        ).trim()
                                        rtUpload (
                                            serverId: 'repo-qa-imply-io',
                                            spec: """{
                                                "files": [
                                                    {
                                                      "pattern": "${ARTIFACT_NAME}-${artifact_build_version}.tar.gz",
                                                      "target": "tgz-local/${ARTIFACT_NAME}/",
                                                      "props": "release.version=${release_version};BVT=Pass;build.url=${BUILD_URL};vcs.revision=${upstream_commit_sha};vcs.url=${repo_url};vcs.branch=${headBranchName()};build.version=${artifact_build_version}"
                                                    }
                                                ]
                                            }""",
                                            buildName: "${ARTIFACTORY_BUILD_NAME}",
                                            buildNumber: "${BUILD_NUMBER}"
                                        )
                                    }
                                }
                            } // end of stage('upload')
                        } // end of stages
                        post {
                            cleanup {
                                resetWs()
                            }
                        }
                    } // end of stage('profile build and upload')
                } // end of stages
            } // end of matrix
        } // end of stage("Build and publish")
    } // end of stages
    post {
        failure {
            // notify about failed build
            script {
                def failedStageNames = getFailedStages( currentBuild ).collect { it.displayName }.findAll { !(it =~ /^(Checks|Matrix)/) }
                if(!currentBuild.previousCompletedBuild || currentBuild.previousCompletedBuild?.result == "SUCCESS") {
                    if(isEligibleToNotify()) { notify(false, failedStageNames) }
                } else {
                    def failedBuildSilencePeriodInHours = 0
                    def failedBuildSilencePeriodInMillis = 1000 * 60 * 60 * failedBuildSilencePeriodInHours
                    def nowInMillis = new Date().getTime()
                    def previousCompletedBuildStarted = currentBuild.previousCompletedBuild?.startTimeInMillis ?: nowInMillis
                    def previousCompletedBuildDuration = currentBuild.previousCompletedBuild?.duration ?: 0
                    def previousCompletedBuildFinished = previousCompletedBuildStarted + previousCompletedBuildDuration
                    if(previousCompletedBuildFinished < nowInMillis-failedBuildSilencePeriodInMillis) {
                        if(isEligibleToNotify()) { notify(false, failedStageNames) }
                    }
                }
            }
        }
        success {
            // notify about fixed build
            script {
                if(!!currentBuild.previousCompletedBuild && currentBuild.previousCompletedBuild?.result != "SUCCESS") {
                    if(isEligibleToNotify()) { notify(true, []) }
                }
            }
        }
    }
}