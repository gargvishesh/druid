def setupMavenSettings() {
    script {
        withCredentials([file(credentialsId: 'maven-artifactory-settings', variable: 'MVN_SETTINGS_PATH')]) {
            sh script: '''
                mkdir -p ~/.m2
                cp -f $MVN_SETTINGS_PATH ~/.m2/settings.xml
            ''',
            label: 'setup maven settings'
        }
    }
}

def mavenInstall() {
    setupMavenSettings()
    sh script: '''
        ${MVN} clean install -q -ff -pl \'!distribution\' ${MAVEN_SKIP} ${MAVEN_SKIP_TESTS} -T1C
        ${MVN} install -q -ff -pl \'distribution\' ${MAVEN_SKIP} ${MAVEN_SKIP_TESTS}
    ''',
    label: 'mvn install'
}

def dockerCleanup() {
    sh  script: 'docker system prune -f -a --volumes || true',
        label: 'cleanup docker data'
}

def mavenDockerBackup() {
    rtDockerPull(
        serverId: "${RT_SERVER_ID}",
        image: "${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}",
        sourceRepo: 'docker-local'
    )
    sh script: '''#!/bin/bash -e
        TEMP_DIR=$(mktemp -d)
        git status --ignored --porcelain | \
            grep -E '^\\!\\!' | sed 's|/$||' | awk -F'\\!\\! ' '{print $2}' | grep -v -E '^web-console' | \
        while read mvnresult; do mkdir -p ${TEMP_DIR}/$(dirname -- $mvnresult); cp -R $mvnresult ${TEMP_DIR}/$mvnresult; done
        tar -czf /tmp/mvnresult.tar.gz -C ${TEMP_DIR} .
        tar -czf /tmp/m2repository.tar.gz -C ~/.m2 repository
        rm -rf ${TEMP_DIR}

        cd /tmp
        echo "FROM ${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}" >> Dockerfile
        echo "COPY m2repository.tar.gz /tmp/m2repository.tar.gz" >> Dockerfile
        echo "COPY mvnresult.tar.gz /tmp/mvnresult.tar.gz" >> Dockerfile
        docker build -t ${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}-${DOCKER_TAG} .
    ''',
    label: 'build temporary docker image for the build'
    rtDockerPush(
        serverId: "${RT_SERVER_ID}",
        image: "${RT_DOCKER_REPOSITORY_PREFIX}/${CURRENT_DOCKER_IMAGE}-${DOCKER_TAG}",
        targetRepo: 'docker-local'
    )
}

def mavenCacheRestore() {
    setupMavenSettings()
    sh script: '''#!/bin/bash -e
        tar -xzf /tmp/mvnresult.tar.gz --no-same-owner --no-overwrite-dir -C ./
        mkdir -p ~/.m2
        tar -xzf /tmp/m2repository.tar.gz --no-same-owner --no-overwrite-dir -C ~/.m2/
    ''',
    label: 'restore build cache'
}

def prepareIfconfig() {
    sh script: '''
        echo -e '#!/bin/bash\nPATH=/sbin:$PATH exec ifconfig eth0 $@' > /usr/bin/ifconfig
        chmod +x /usr/bin/ifconfig
        hash -r
    ''',
    label: "prepare ifconfig"
}

def getInstanceId() {
    sh script: '''#!/bin/bash -x
        curl -s http://169.254.169.254/latest/meta-data/instance-id
    ''',
    label: 'print instance id'
}

def buildArtifacts(stageName) {
    script {
        def stageArtifactsDirPath = "stage_${stageName.replaceAll(~/[^A-Za-z0-9_-]/,'_')}"
        withEnv(["stageArtifactsDirPath=${stageArtifactsDirPath}"]) {
            // copy logs
            sh script: '''#!/bin/bash -x
                mkdir -p ${stageArtifactsDirPath}

                shopt -s globstar

                urlencode_path() {
                    python -c 'import urllib, sys; print urllib.quote(sys.argv[1], sys.argv[2])' "$1" "/"
                }

                copy_files_by_pattern() {
                    if  ls ${1}; then
                        for fname in ${1}; do
                            fname_root=$(echo "${fname}" | cut -d "/" -f1)
                            if ! [[ "$fname_root" = "${2}" ]]; then
                                if [[ -f "$fname" ]]; then
                                    dest=${2}/$(urlencode_path "${fname}")
                                    mkdir -p $(dirname -- "$dest")
                                    cp "$fname" "$dest"
                                fi
                            fi
                        done
                    fi
                }

                if [ -d ~/shared ]; then
                    artifacts_path=$(pwd)/${stageArtifactsDirPath}
                    pushd ~/shared
                    copy_files_by_pattern 'logs/**' $artifacts_path
                    copy_files_by_pattern 'tasklogs/**' $artifacts_path
                    popd
                fi

                # copy dockerd logs
                artifacts_path=$(pwd)/${stageArtifactsDirPath}
                pushd /var/tmp
                copy_files_by_pattern 'dockerd.log' $artifacts_path
                popd

                # copy test reports
                copy_files_by_pattern '**/target/surefire-reports/*.xml' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/target/failsafe-reports/*.xml' ${stageArtifactsDirPath}

                # copy top-level jacoco reports
                copy_files_by_pattern '**/target/*.exec' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/*.html' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/*.xml' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/*.csv' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/jacoco-resources/*' ${stageArtifactsDirPath}
                # copy detailed jacoco reports
                copy_files_by_pattern '**/jacoco/**/*.html' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/**/*.xml' ${stageArtifactsDirPath}
                copy_files_by_pattern '**/jacoco/**/*.csv' ${stageArtifactsDirPath}
                # inspection results
                copy_files_by_pattern 'inspection-results/*.xml' ${stageArtifactsDirPath}
            '''
        }
        // fixate artifacts
        archiveArtifacts artifacts: "${stageArtifactsDirPath}/**", allowEmptyArchive: true
        // cleanup tmp artifacts dir
        sh script: "rm -rf ${stageArtifactsDirPath}"
    }
}

def resetWs() {
    sh script: "git clean -fdx", label: "Clean up everything but files from git"
}

def isEligibleToNotify() {
    return  !env.CHANGE_ID \
            && (env.BRANCH_NAME ==~ env.ACTIVE_DEV_BRANCH_REGEX || env.BRANCH_NAME ==~ (env.RELEASE_BRANCH_PREFIX_REGEX+env.RELEASE_BRANCH_SUFFIX_REGEX))
}

def implyQueryIntegrationTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        retry(2) {
            try {
                sh script: """
                    \${MVN} -pl integration-tests process-resources && \${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
                    -Dgroups=query \
                    -Dit.indexer=middleManager \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.extraDatasourceNameSuffix="" \
                    -Doverride.config.path=../../integration-tests-imply/docker/environment-configs/test-groups/prepopulated-data \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "imply query integration tests with ${jvmRuntimeOpt}"
            } finally {
                dockerCleanup()
            }
        }
    }
}

def implyKeycloakSecurityIntegrationTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    getInstanceId()
    script {
        retry(3) {
            try {
                sh script: """
                    \${MVN} -pl integration-tests process-resources && ${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
                    -Dgroups=keycloak-security \
                    -Dit.indexer=middleManager \
                    -Doverride.config.path=../../integration-tests-imply/docker/environment-configs/test-groups/keycloak-security \
                    ${jvmRuntimeOpt} \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "imply keycloak security integration tests with ${jvmRuntimeOpt}"
            } finally {
                dockerCleanup()
            }
        }
    }
}

def implyVirtualSegmentsIntegrationTests(jvmRuntimeOpt) {
    mavenInstall()
    prepareIfconfig()
    getInstanceId()
    script {
        retry(2) {
            try {
                sh script: """
                    \${MVN} -pl integration-tests process-resources && ${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
                    -Dgroups=virtual-segments \
                    -Dit.indexer=middleManager \
                    -Doverride.config.path=../../integration-tests-imply/docker/environment-configs/test-groups/virtual-segments \
                    ${jvmRuntimeOpt} \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "imply cold tier integration tests with ${jvmRuntimeOpt}"
            } finally {
                dockerCleanup()
            }
        }
    }
}

def implyAsyncDownloadIntegrationTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        retry(2) {
            try {
                sh script: """
                    \${MVN} -pl integration-tests process-resources && ${MVN} verify -Pintegration-tests-imply -pl integration-tests-imply \
                    -Dgroups=async-download \
                    -Dit.indexer=middleManager \
                    -Doverride.config.path=../../integration-tests-imply/docker/environment-configs/test-groups/async-download \
                    ${jvmRuntimeOpt} \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "imply async query result download integration tests with ${jvmRuntimeOpt}"
            } finally {
                dockerCleanup()
            }
        }
    }
}

def implyS3Tests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'aws', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
            def cloudpath = UUID.randomUUID().toString()
            writeFile   file: "jenkins/s3-config",
                        text: "druid_storage_type=s3\ndruid_storage_bucket=druid-qa\ndruid_storage_baseKey=${cloudpath}\ndruid_s3_accessKey=${AWS_ACCESS_KEY_ID}\ndruid_s3_secretKey=${AWS_SECRET_ACCESS_KEY}\nAWS_REGION=us-east-1\ndruid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"imply-sql-async\",\"druid-s3-extensions\"]\ndruid_query_async_storage_type=s3\ndruid_query_async_storage_s3_bucket=druid-qa\ndruid_query_async_storage_s3_prefix=${cloudpath}/async-test/\ndruid_query_async_storage_s3_tempDir=/shared/async-tmp-results\ndruid_metadata_storage_type=mysql\ndruid_metadata_storage_connector_connectURI=jdbc:mysql://druid-metadata-storage/druid\ndruid_metadata_storage_connector_user=druid\ndruid_metadata_storage_connector_password=diurd"
            retry(2) {
                try {
                    sh script: """
                        \${MVN} -pl integration-tests process-resources && ${MVN} verify -P integration-tests-imply -pl integration-tests-imply \
                        -Dgroups=imply-s3 \
                        -Doverride.config.path=\${WORKSPACE}/jenkins/s3-config \
                        ${jvmRuntimeOpt} \
                        -Ddruid.test.config.cloudBucket=druid-qa \
                        -Ddruid.test.config.cloudPath=${cloudpath}/ \
                        -ff \${MAVEN_SKIP} -Djacoco.skip=true
                    """,
                    label: "imply-s3 with ${jvmRuntimeOpt}"
                }
                finally {
                    dockerCleanup()
                }
            }
        }
    }
}

def s3DeepStorageTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'aws', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
            def cloudpath = UUID.randomUUID().toString()
            writeFile   file: "jenkins/s3-config",
                        text: "druid_storage_type=s3\n" +
                        "druid_storage_bucket=druid-qa\n" +
                        "druid_storage_baseKey=${cloudpath}\n" +
                        "druid_s3_accessKey=${AWS_ACCESS_KEY_ID}\n" +
                        "druid_s3_secretKey=${AWS_SECRET_ACCESS_KEY}\n" +
                        "AWS_REGION=us-east-1\n" +
                        "druid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"druid-hdfs-storage\",\"druid-s3-extensions\",\"druid-parquet-extensions\",\"druid-avro-extensions\",\"druid-protobuf-extensions\",\"druid-orc-extensions\",\"druid-kafka-indexing-service\"]"
            retry(2) {
                try {
                    sh script: """
                        aws s3 sync \
                        ./integration-tests/src/test/resources/data/batch_index/json/ s3://druid-qa/${cloudpath} \
                        --exclude "*" --include "wikipedia_index_data*.json"
                    """,
                    label: "uploading data to bucket"
                    sh script: """
                        \${MVN} verify -P integration-tests -pl integration-tests \
                        -Dgroups=s3-deep-storage \
                        -Doverride.config.path=\${WORKSPACE}/jenkins/s3-config \
                        ${jvmRuntimeOpt} \
                        -Ddruid.test.config.cloudBucket=druid-qa \
                        -Ddruid.test.config.cloudPath=${cloudpath}/ \
                        -Ddocker.build.hadoop=true \
                        -Dstart.hadoop.docker=true \
                        -ff \${MAVEN_SKIP} -Djacoco.skip=true
                    """,
                    label: "s3-deep-storage with ${jvmRuntimeOpt}"
                }
                finally {
                    dockerCleanup()
                    sh script: "aws s3 rm s3://druid-qa/${cloudpath} --recursive"
                }
            }
        }
    }
}

def kinesisDeepStorageTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        withCredentials([[$class: 'AmazonWebServicesCredentialsBinding', accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId:  'aws', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY']]) {
            lock('awsResource') {
                writeFile   file: "jenkins/kinesis-config",
                            text: "druid_kinesis_accessKey=${AWS_ACCESS_KEY_ID}\ndruid_kinesis_secretKey=${AWS_SECRET_ACCESS_KEY}\nAWS_REGION=us-east-1\ndruid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"druid-kinesis-indexing-service\"]"
                retry(2) {
                    try {
                        sh script: """
                            \${MVN} verify -P integration-tests -pl integration-tests \
                            -Dgroups=kinesis-index \
                            -Doverride.config.path=\${WORKSPACE}/jenkins/kinesis-config \
                            ${jvmRuntimeOpt} \
                            -Ddruid.test.config.streamEndpoint=kinesis.us-east-1.amazonaws.com \
                            -Ddocker.build.hadoop=true \
                            -Dstart.hadoop.docker=true \
                            -ff \${MAVEN_SKIP} -Djacoco.skip=true
                        """,
                        label: "kinesis-deep-storage with ${jvmRuntimeOpt}"
                    } finally {
                        dockerCleanup()
                    }
                }
            }
        }
    }
}

def azureDeepStorageTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        withCredentials([usernamePassword(credentialsId: 'azure_credentials', usernameVariable: 'AZURE_ACCOUNT', passwordVariable: 'AZURE_KEY')]) {
            def containerName = UUID.randomUUID().toString()
            writeFile   file: "jenkins/azure-config",
                        text: "druid_storage_type=azure\n" +
                        "druid_azure_account=${AZURE_ACCOUNT}\n" +
                        "druid_azure_key=${AZURE_KEY}\n" +
                        "druid_azure_container=${containerName}\n" +
                        "druid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"druid-hdfs-storage\",\"druid-azure-extensions\",\"druid-parquet-extensions\",\"druid-avro-extensions\",\"druid-protobuf-extensions\",\"druid-orc-extensions\",\"druid-kafka-indexing-service\"]"
            retry(2) {
                try {
                    sh script: """
                        az storage container create -n ${containerName} \
                        --public-access blob \
                        --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY}
                    """,
                    label: "creating storage container"
                    sh script: """
                        az storage blob upload-batch \
                        --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY} \
                        -d ${containerName} \
                        --source ./integration-tests/src/test/resources/data/batch_index/json/ --pattern "wikipedia_index_data*.json"
                    """,
                    label: "uploading data to storage container"
                    sh script: """
                        \${MVN} verify -P integration-tests -pl integration-tests \
                        -Dgroups=azure-deep-storage \
                        -Doverride.config.path=\${WORKSPACE}/jenkins/azure-config \
                        ${jvmRuntimeOpt} \
                        -Ddruid.test.config.cloudBucket=${containerName} \
                        -Ddruid.test.config.cloudPath= \
                        -Ddocker.build.hadoop=true \
                        -Dstart.hadoop.docker=true \
                        -ff \${MAVEN_SKIP} -Djacoco.skip=true
                    """,
                    label: "azure-deep-storage with ${jvmRuntimeOpt}"
                }
                finally {
                    dockerCleanup()
                    sh script: """
                        az storage container delete -n ${containerName}\
                        --account-name \${AZURE_ACCOUNT} --account-key \${AZURE_KEY}
                    """
                }
            }
        }
    }
}

def gcsDeepStorageTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        withCredentials([file(credentialsId: 'gcs-bucket-qa', variable: 'GC_KEY')]) {
            def cloudpath = "gcs-test-${UUID.randomUUID().toString()}/"
            def bucket = "imply-qa-testing"
            writeFile   file: "jenkins/gcs-config",
                        text: "druid_storage_type=google\ndruid_google_bucket=${bucket}\ndruid_google_prefix=${cloudpath}\ndruid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"druid-hdfs-storage\",\"druid-google-extensions\",\"druid-parquet-extensions\",\"druid-avro-extensions\",\"druid-protobuf-extensions\",\"druid-orc-extensions\",\"druid-kafka-indexing-service\"]\nGOOGLE_APPLICATION_CREDENTIALS=/shared/docker/credentials/creds.json"
            retry(3) {
                try {
                    sh script: """
                        mkdir -p jenkins/gcs
                        cp -f \${GC_KEY} jenkins/gcs/creds.json
                        chmod 764 jenkins/gcs/creds.json
                        gsutil \
                            -o Credentials:gs_service_key_file=\${WORKSPACE}/jenkins/gcs/creds.json \
                            cp \${WORKSPACE}/integration-tests/src/test/resources/data/batch_index/json/wikipedia_index_data*.json \
                            gs://${bucket}/${cloudpath}
                    """,
                    label: "copying gcs creds"
                    sh script: """
                        \${MVN} verify -P integration-tests -pl integration-tests \
                        -Doverride.config.path=\${WORKSPACE}/jenkins/gcs-config \
                        -Dresource.file.dir.path=\${WORKSPACE}/jenkins/gcs \
                        -Dgroups=gcs-deep-storage \
                        ${jvmRuntimeOpt} \
                        -Ddruid.test.config.cloudBucket=${bucket} \
                        -Ddruid.test.config.cloudPath=${cloudpath} \
                        -Ddocker.build.hadoop=true \
                        -Dstart.hadoop.docker=true \
                        -ff \${MAVEN_SKIP} -Djacoco.skip=true
                    """,
                    label: "gcs-deep-storage with ${jvmRuntimeOpt}"
                } finally {
                    dockerCleanup()
                    sh script: "gsutil -o Credentials:gs_service_key_file=\${WORKSPACE}/jenkins/gcs/creds.json rm -r gs://${bucket}/${cloudpath}"
                }
            }
        }
    }
}

def hdfsDeepStorageTests(jvmRuntimeOpt) {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    writeFile   file:   "jenkins/hdfs-config",
                text:   "druid_storage_type=hdfs\n" +
                        "druid_storage_storageDirectory=/druid/segments\n" +
                        "druid_extensions_loadList=[\"mysql-metadata-storage\",\"druid-basic-security\",\"simple-client-sslcontext\",\"druid-testing-tools\",\"druid-lookups-cached-global\",\"druid-histogram\",\"druid-datasketches\",\"druid-hdfs-storage\",\"druid-parquet-extensions\",\"druid-avro-extensions\",\"druid-protobuf-extensions\",\"druid-orc-extensions\",\"druid-kafka-indexing-service\"]\n" +
                        "druid_indexer_logs_type=hdfs\n" +
                        "druid_indexer_logs_directory=/druid/indexing-logs"
    script {
        retry(2) {
            try {
                sh script: """
                    \${MVN} verify -P integration-tests -pl integration-tests \
                    -Doverride.config.path=\${WORKSPACE}/jenkins/hdfs-config \
                    -Dgroups=hdfs-deep-storage \
                    -Ddocker.build.hadoop=true \
                    -Dstart.hadoop.docker=true \
                    ${jvmRuntimeOpt} \
                    -Ddruid.test.config.extraDatasourceNameSuffix="" \
                    -Dit.test=ITHdfsToHdfsParallelIndexTest \
                    -ff \${MAVEN_SKIP} -Djacoco.skip=true
                """,
                label: "hdfs-deep-storage with ${jvmRuntimeOpt}"
            } finally {
                dockerCleanup()
            }
        }
    }
}

def coreIntegrationTests() {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        retry(3) {
            try {
                sh script: '''
                    ${MVN} verify -pl integration-tests -P integration-tests \
                    ${TESTNG_GROUPS} ${JVM_RUNTIME} -Dit.indexer=${USE_INDEXER} \
                    ${MAVEN_SKIP}
                ''',
                label: 'core integration tests'
            } finally {
                dockerCleanup()
            }
        }
    }
}

def coreIntegrationTestsWithPrepopulatedData() {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        retry(3) {
            try {
                sh script: '''
                    ${MVN} verify -pl integration-tests -P integration-tests \
                    ${TESTNG_GROUPS} ${JVM_RUNTIME} -Dit.indexer=${USE_INDEXER} \
                    -Doverride.config.path=../../integration-tests/docker/environment-configs/test-groups/prepopulated-data \
                    ${MAVEN_SKIP}
                ''',
                label: 'core integration tests'
            } finally {
                dockerCleanup()
            }
        }
    }
}

def coreIntegrationTestsWithShuffleDeepStorage() {
    mavenCacheRestore()
    prepareIfconfig()
    getInstanceId()
    script {
        retry(3) {
            try {
                sh script: '''
                    ${MVN} verify -pl integration-tests -P integration-tests \
                    ${TESTNG_GROUPS} ${JVM_RUNTIME} -Dit.indexer=${USE_INDEXER} \
                    -Doverride.config.path=../../integration-tests/docker/environment-configs/test-groups/shuffle-deep-store \
                    ${MAVEN_SKIP}
                ''',
                label: 'core integration tests'
            } finally {
                dockerCleanup()
            }
        }
    }
}

def runUnitTest() {
    mavenCacheRestore()
    getInstanceId()
    script {
        try {
            sh script: '''#!/bin/bash
                unset _JAVA_OPTIONS
                # Set MAVEN_OPTS for Surefire launcher. Skip remoteresources to avoid intermittent connection timeouts when
                # resolving the SIGAR dependency.
                export BUILD_STATUS=0
                MAVEN_OPTS='-Xmx1100m' ${MVN} test -pl ${MAVEN_PROJECTS:-processing} \
                    ${MAVEN_SKIP} -Dremoteresources.skip=true -Ddruid.generic.useDefaultValueForNull=${DRUID_USE_DEFAULT_VALUE_FOR_NULL:-true} \
                    || export BUILD_STATUS=1

                ${MVN} -pl ${MAVEN_PROJECTS} jacoco:report
                exit $BUILD_STATUS
            ''',
            label: "processing module test"
        } finally {
           dockerCleanup()
        }
    }
}

@NonCPS
def cancelPreviousBuilds() {
    println("Checking to see if any previous builds need to be aborted.")
    def jobName = env.JOB_NAME
    def buildNumber = env.BUILD_NUMBER.toInteger()
    /* Get job name */
    def currentJob = Jenkins.instance.getItemByFullName(jobName)
    /* Iterating over the builds for specific job */
    for (def build : currentJob.builds) {
        def exec = build.getExecutor()
        /* If there is a build that is currently running and it's not current build */
        if (build.isBuilding() && build.number.toInteger() != buildNumber && exec != null) {
            println("Initiating interrupt of previous build #${build.number}")
            /* Then stop it */
            exec.interrupt(
                    Result.ABORTED,
                    new CauseOfInterruption.UserInterruption("Aborted by #${currentBuild.number}")
                )
            println("Aborted previously running build #${build.number}")
        }
    }
}

pipeline {
    options {
        timeout(time: 4, unit: 'HOURS')
        buildDiscarder(logRotator(artifactDaysToKeepStr: '15', artifactNumToKeepStr: '10', daysToKeepStr: '30', numToKeepStr: '20'))
    }

    parameters {
        booleanParam(name: 'SKIP_ALL_JENKINS_TESTS', defaultValue: false, description: 'Skip all jenkins tests defined in the jenkinsfile')
        booleanParam(name: 'SKIP_JENKINS_INTEGRATION_TESTS', defaultValue: false, description: 'Skip integration tests defined in the jenkinsfile')
        booleanParam(name: 'SKIP_JENKINS_UNIT_TESTS_AND_CHECKS', defaultValue: false, description: 'Skip unit test and checks defined in the jenkinsfile')
        booleanParam(name: 'PUBLISH_ON_ANY_BRANCH', defaultValue: false, description: 'Build and publish to artifactory regardless branch name')
    }

    agent none

    environment {
        MVN = "mvn -B"
        MAVEN_SKIP = "-Pskip-static-checks -Ddruid.console.skip=true -Dmaven.javadoc.skip=true"
        MAVEN_SKIP_TESTS = "-Pskip-tests"
        DOCKER_IP = "127.0.0.1"
        APACHE_ARCHIVE_MIRROR_HOST = "https://repo.cnc.imply.io/artifactory/archive-apache-org-remote"
        COMPOSE_HTTP_TIMEOUT = "600"

        ACTIVE_DEV_BRANCH_REGEX = '^monthly$'
        RELEASE_BRANCH_PREFIX_REGEX = '^release\\/'
        RELEASE_BRANCH_SUFFIX_REGEX = '[^/]+$'

        IMAGE_JDK8 = "docker/druid-ci-jdk8:1620827975"
        IMAGE_JDK11 = "docker/druid-ci-jdk11:1620827975"
        IMAGE_JDK15 = "docker/druid-ci-jdk15:1625878101"
        DOCKER_TAG = "${BUILD_TAG}".replaceAll(~/[^A-Za-z0-9_-]/,'_')
        RT_REGISTRY_URL = "https://repo.cnc.imply.io/artifactory"
        RT_REGISTRY_CREDS = "repo.qa.imply.io"
        RT_SERVER_ID = "repo-qa-imply-io"
        RT_DOCKER_REPOSITORY_PREFIX = "repo.cnc.imply.io"
        DOCKER_REGISTRY_MIRROR = "https://registry-mirror.cnc.imply.io:443"
        DOCKER_AGENT_LABEL = "ubuntu-sysbox-1621268334"
        EXPECTED_SHA1_RUN_DRUID = "d47a0a0a3886fc3fa26f98c02f35a3ffba848437"
        EXPECTED_SHA1_VERIFY_JAVA = "3e181b1d0c49d8d6a3db55da8c5337a548e0b37b"
        EXPECTED_SHA1_RUN_ZK = "ee4c4f10bb721a69738d49c4835d81456992e295"

    }

    stages {
        stage('Cancel old builds, check labels') {
            steps {
                script {
                    cancelPreviousBuilds()
                    if (env.CHANGE_ID && pullRequest.labels.contains("Don't Build")) {
                        currentBuild.result = 'ABORTED'
                        error("Tagged Don't Build")
                    }
                }
            }
        }

        stage('Maven install') {
            when {
                expression { !params.SKIP_ALL_JENKINS_TESTS }
                beforeAgent true
            }
            environment {
                DOCKER_AGENT_ARGS = "-u root:root --runtime=sysbox-runc -e RUN_DOCKER=1"
            }
            parallel {

                stage('(openjdk11) maven install') {
                    agent {docker {image "${IMAGE_JDK11}"; args "${DOCKER_AGENT_ARGS}"; label "${DOCKER_AGENT_LABEL}"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"}}
                    environment { CURRENT_DOCKER_IMAGE="${IMAGE_JDK11}" }
                    steps {
                        mavenInstall()
                        mavenDockerBackup()

                    }
                    post {
                        cleanup {
                            resetWs()
                            dockerCleanup()
                        }
                    }
                }
            }
        }

        stage('Tests - phase 1') {
            when {
                expression { !params.SKIP_JENKINS_UNIT_TESTS_AND_CHECKS && !params.SKIP_ALL_JENKINS_TESTS }
                beforeAgent true
            }
            environment {
                DOCKER_AGENT_ARGS = "-u root:root"
            }
            parallel {

                stage('(openjdk8) processing module test') {
                    agent {docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" }}
                    environment {
                        MAVEN_PROJECTS="processing"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="true"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk8) processing module test (SQL Compatibility)') {
                    agent {docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" }}
                    environment {
                        MAVEN_PROJECTS="processing"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="false"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk8) indexing modules test') {
                    agent {docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" }}
                    environment {
                        MAVEN_PROJECTS="indexing-hadoop,indexing-service,extensions-core/kafka-indexing-service,extensions-core/kinesis-indexing-service"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="true"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk8) indexing modules test (SQL Compatibility)') {
                    agent {docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" }}
                    environment {
                        MAVEN_PROJECTS="indexing-hadoop,indexing-service,extensions-core/kafka-indexing-service,extensions-core/kinesis-indexing-service"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="false"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk8) server module test') {
                    agent {docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" }}
                    environment {
                        MAVEN_PROJECTS="server"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="true"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk8) server module test (SQL Compatibility)') {
                    agent {docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" }}
                    environment {
                        MAVEN_PROJECTS="server"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="false"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk8) other modules test') {
                    agent {docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" }}
                    environment {
                        MAVEN_PROJECTS="!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests,!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="true"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk8) other modules test (SQL Compatibility)') {
                    agent {docker { image "${IMAGE_JDK8}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" }}
                    environment {
                        MAVEN_PROJECTS="!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests,!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL="false"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) processing module test') {
                    agent { docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    environment {
                        MAVEN_PROJECTS = "processing"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL = "true"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) processing module test (SQL Compatibility)') {
                    agent { docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    environment {
                        MAVEN_PROJECTS = "processing"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL = "false"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) indexing modules test') {
                    agent { docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    environment {
                        MAVEN_PROJECTS = "indexing-hadoop,indexing-service,extensions-core/kafka-indexing-service,extensions-core/kinesis-indexing-service"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL = "true"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) indexing modules test (SQL Compatibility)') {
                    agent { docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    environment {
                        MAVEN_PROJECTS = "indexing-hadoop,indexing-service,extensions-core/kafka-indexing-service,extensions-core/kinesis-indexing-service"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL = "false"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) server module test') {
                    agent { docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    environment {
                        MAVEN_PROJECTS = "server"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL = "true"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) server module test (SQL Compatibility)') {
                    agent { docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    environment {
                        MAVEN_PROJECTS = "server"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL = "false"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) other modules test') {
                    agent { docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    environment {
                        MAVEN_PROJECTS = "!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests,!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL = "true"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }

                stage('(openjdk11) other modules test (SQL Compatibility)') {
                    agent { docker { image "${IMAGE_JDK11}-${DOCKER_TAG}"; args "-u jenkins --runtime=sysbox-runc -e RUN_DOCKER=1"; registryUrl "${RT_REGISTRY_URL}"; registryCredentialsId "${RT_REGISTRY_CREDS}"; label "${DOCKER_AGENT_LABEL}" } }
                    environment {
                        MAVEN_PROJECTS = "!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests,!processing,!indexing-hadoop,!indexing-service,!extensions-core/kafka-indexing-service,!extensions-core/kinesis-indexing-service,!server,!web-console,!integration-tests"
                        DRUID_USE_DEFAULT_VALUE_FOR_NULL = "false"
                    }
                    steps { runUnitTest() }
                    post { always { buildArtifacts(env.STAGE_NAME) }; cleanup { resetWs() } }
                }
            }
        }



        stage("Build and publish") {
            when {
                anyOf {
                    expression { (env.CHANGE_BRANCH ?: env.BRANCH_NAME) ==~ env.ACTIVE_DEV_BRANCH_REGEX }
                    expression { (env.CHANGE_BRANCH ?: env.BRANCH_NAME) ==~ (env.RELEASE_BRANCH_PREFIX_REGEX + env.RELEASE_BRANCH_SUFFIX_REGEX) }
                    expression { params.PUBLISH_ON_ANY_BRANCH }
                }
                beforeAgent true
            }

            matrix {
                axes {
                    axis {
                        name 'BUILD_PROFILE'
                        values 'dist', 'imply-saas', 'imply-experimental'
                    }
                }
                stages {
                    stage('profile build and upload') {
                        environment {
                            GIT_ASKPASS = "/tmp/askpass.sh"
                            GITHUB_NETRC = "/tmp/github-netrc"
                            STAGING_BASE_DIR= "/tmp/druid-build/stage"
                            TMP_DIR = "/tmp/druid-build/tmp"
                            BUILD_DIR = "/tmp/druid-build"
                            ARTIFACT_NAME = sh(script:"""#!/bin/bash
                                if [ "${BUILD_PROFILE}" == "imply-saas" ]; then
                                    echo "druid-saas"
                                elif [ "${BUILD_PROFILE}" == "imply-experimental" ]; then
                                    echo "druid-experimental"
                                else
                                    echo "druid"
                                fi
                            """, returnStdout: true).trim()
                            ARTIFACTORY_BUILD_NAME = sh(script:"""#!/bin/bash
                                if [ "${BUILD_PROFILE}" == "imply-saas" ]; then
                                    echo "druid.saas"
                                elif [ "${BUILD_PROFILE}" == "imply-experimental" ]; then
                                    echo "druid.experimental"
                                else
                                    echo "druid"
                                fi
                            """, returnStdout: true).trim()
                        }
                        agent {
                            docker {
                                image 'docker/buildabear:20200923'
                                args '-u root:root'
                                label "${DOCKER_AGENT_LABEL}"
                                registryUrl "${RT_REGISTRY_URL}"
                                registryCredentialsId "${RT_REGISTRY_CREDS}"
                            }
                        }
                        stages {
                            stage('prepare to build') {
                                steps {
                                    script {
                                        def gitCredsId = scm.getUserRemoteConfigs()[0].getCredentialsId()
                                        withCredentials([usernamePassword(credentialsId: gitCredsId, usernameVariable: 'GIT_CREDS_USR', passwordVariable: 'GIT_CREDS_PSW')]) {
                                            sh script:'''#!/bin/bash -eux
                                                echo -e '#!/bin/bash\nPATH=/opt/maven/apache-maven-3.5.4/bin:$PATH exec mvn -B $@' > /usr/bin/mvn
                                                chmod u+x /usr/bin/mvn

                                                cat <<EOF > "${GIT_ASKPASS}"
                                                #!/bin/sh
                                                case "\\$1" in
                                                    Username*) echo "${GIT_CREDS_USR}" ;;
                                                    Password*) echo "${GIT_CREDS_PSW}" ;;
                                                esac
                                                EOF

                                                cat <<EOF > "${GITHUB_NETRC}"
                                                machine api.github.com
                                                login ${GIT_CREDS_USR}
                                                password ${GIT_CREDS_PSW}
                                                EOF

                                                chmod u+x "${GIT_ASKPASS}"
                                                chmod 400 "${GITHUB_NETRC}"
                                            '''.replaceAll(/\n\s+/, "\n"),
                                            label: 'prepare for build'
                                        }
                                    }
                                }
                            }
                            stage('build') {
                                steps {
                                    setupMavenSettings()
                                    sh script:'''#!/bin/bash -eux
                                        COMMIT_SHA=$(git rev-parse HEAD)
                                        if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                            UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                        else
                                            COMMIT_SHA=$(git rev-parse HEAD^)
                                            if [[ $(git branch origin/${BRANCH_NAME} -r --contains $COMMIT_SHA) ]]; then
                                                UPSTREAM_COMMIT_SHA=$COMMIT_SHA
                                            else
                                                echo "head branch commit SHA is not found"
                                                exit 1
                                            fi
                                        fi

                                        # Matching the version from pom.xml
                                        DERBYTOOLS_VERSION="10.14.2.0"

                                        # Imply includes additional hadoop libraries (e.g., hadoop-aws)
                                        HADOOP_VERSION="2.8.5"

                                        MARIADB_CONNECTOR_VERSION="2.7.3"

                                        cd $WORKSPACE

                                        # remove snapshot suffix from pom files
                                        INIT_DRUID_VERSION=$(mvn org.apache.maven.plugins:maven-help-plugin:3.2.0:evaluate -Dexpression=project.version -q -DforceStdout)
                                        DRUID_VERSION=${INIT_DRUID_VERSION%%-SNAPSHOT}
                                        mvn versions:set -DnewVersion=$DRUID_VERSION  -DgenerateBackupPoms=false

                                        current_java_version=$(java -version 2>&1 >/dev/null | grep 'version' | awk '{print $3}')
                                        echo "Compiling Druid with current Java version set. Java version=$current_java_version"

                                        mvn clean install \
                                            -Dhadoop.compile.version="$HADOOP_VERSION" \
                                            -Danimal.sniffer.skip=true \
                                            -Dcheckstyle.skip=true \
                                            -Denforcer.skip=true \
                                            -Dforbiddenapis.skip=true \
                                            -Djacoco.skip=true \
                                            -Dmaven.javadoc.skip=true \
                                            -Dpmd.skip=true \
                                            -Dspotbugs.skip=true \
                                            -DskipTests \
                                            -T1C \
                                            -Dtar \
                                            -P${BUILD_PROFILE}

                                        echo "Built Druid, version: $DRUID_VERSION"

                                        # Stage Druid
                                        export STAGING_DIR=${STAGING_BASE_DIR}/${BUILD_PROFILE}
                                        mkdir -p "$STAGING_DIR/dist"
                                        mkdir -p "$TMP_DIR"
                                        tar -C "$TMP_DIR" -xzf "$WORKSPACE"/distribution/target/apache-druid-*-bin.tar.gz
                                        mv "$TMP_DIR"/apache-druid-* "$STAGING_DIR/dist/druid"

                                        # Fetch the MariaDB Client from mariadb.com
                                        MARIADB_CONNECTOR_LOCATION="$STAGING_DIR/dist/druid/extensions/mysql-metadata-storage/mariadb-java-client-${MARIADB_CONNECTOR_VERSION}.jar"
                                        #mkdir -p "$STAGING_DIR/dist/druid/extensions/mysql-metadata-storage"
                                        curl -o "$MARIADB_CONNECTOR_LOCATION" --retry 10 "https://downloads.mariadb.com/Connectors/java/connector-java-${MARIADB_CONNECTOR_VERSION}/mariadb-java-client-${MARIADB_CONNECTOR_VERSION}.jar"

                                        if [ "$(sha1sum "$MARIADB_CONNECTOR_LOCATION" | awk '{print $1}')" != "4a2edc05bd882ad19371d2615c2635dccf8d74f0" ]
                                        then
                                          echo "$MARIADB_CONNECTOR_LOCATION: checksum mismatch" >&2
                                          exit 1
                                        fi

                                        # Add hadoop-aws
                                        (cd "$STAGING_DIR"/dist/druid && java -classpath "lib/*" org.apache.druid.cli.Main tools pull-deps -h "org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}")

                                        # Add derbytools to lib
                                        DERBYTOOLS_LOCATION="$STAGING_DIR/dist/druid/lib/derbytools-$DERBYTOOLS_VERSION.jar"
                                        curl -Lo "$DERBYTOOLS_LOCATION" "https://search.maven.org/remotecontent?filepath=org/apache/derby/derbytools/$DERBYTOOLS_VERSION/derbytools-$DERBYTOOLS_VERSION.jar"

                                        if [ "$(sha1sum "$DERBYTOOLS_LOCATION" | awk '{print $1}')" != "338d5a54b4089c80414fe0ecb3899d521da69b26" ]
                                        then
                                          echo "$DERBYTOOLS_LOCATION: checksum mismatch" >&2
                                          exit 1
                                        fi

                                        # Remove unsupported open-source extensions
                                        for extension in druid-pac4j druid-ranger-security materialized-view-maintenance materialized-view-selection; do
                                            rm -rf "${STAGING_DIR}/dist/druid/extensions/${extension}"
                                        done

                                        # Put druid on a diet
                                        perl - <<'EOT'
                                        use strict;
                                        use File::Basename;

                                        my $dir = "$ENV{STAGING_DIR}/dist/druid";
                                        chdir $dir or die "chdir $dir: $!";

                                        my %jars;
                                        for my $jar (qx!find ./lib -name '*.jar'!, qx!find ./extensions -name '*.jar'!, qx!find ./hadoop-dependencies -name '*.jar'!) {
                                          chomp $jar;
                                          my $jarname = basename($jar);
                                          if (exists $jars{$jarname}) {
                                            my $depth = $jar =~ tr !/!/! - 1;
                                            my $dots = "";
                                            for my $x (1..$depth) {
                                              $dots .= "../";
                                            }
                                            system("ln", "-sf", "${dots}$jars{$jarname}", $jar);
                                          } else {
                                            $jars{$jarname} = $jar;
                                          }
                                        }
                                        EOT

                                        function verify_checksum() {
                                            local file="$1"
                                            local checksum="$2"
                                            if [ "$(sha1sum "$file" | awk '{print $1}')" != "$checksum" ]; then
                                                1>&2 cat << EOF
                                                    $file has changed in the druid repo! Look at the changes and incorporate them in the
                                                    script if needed. Then update the expected sha1 in this script. Also update the imply
                                                    startup scripts located at https://github.com/implydata/imply-release/blob/master/imply-resources/bin/$file
                                                EOF
                                                exit 1
                                            fi
                                        }

                                        verify_checksum "$STAGING_DIR/dist/druid/bin/run-druid" "$EXPECTED_SHA1_RUN_DRUID"
                                        verify_checksum "$STAGING_DIR/dist/druid/bin/verify-java" "$EXPECTED_SHA1_VERIFY_JAVA"
                                        verify_checksum "$STAGING_DIR/dist/druid/bin/run-zk" "$EXPECTED_SHA1_RUN_ZK"

                                        tar -C $STAGING_DIR/dist -czf $WORKSPACE/${ARTIFACT_NAME}-${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}.tar.gz druid

                                        echo "${DRUID_VERSION}-${UPSTREAM_COMMIT_SHA:0:8}" > $WORKSPACE/build.version

                                    '''.replaceAll(/\n\s+/, "\n"),
                                    label: 'build druid and verify'
                                } // end of steps
                            } // end of stage('build')

                            stage('upload') {
                                steps {
                                    script {
                                        def release_version = "None"
                                        def headBranch = env.CHANGE_BRANCH ?: env.BRANCH_NAME
                                        if(headBranch ==~ (env.RELEASE_BRANCH_PREFIX_REGEX + env.RELEASE_BRANCH_SUFFIX_REGEX)) {
                                            release_version = (headBranch =~ env.RELEASE_BRANCH_SUFFIX_REGEX).getAt(0)
                                        } else if(headBranch ==~ env.ACTIVE_DEV_BRANCH_REGEX) {
                                            release_version = sh(
                                                script: '''#!/bin/bash -e
                                                    curl -s --netrc-file ${GITHUB_NETRC} \
                                                    -H "Accept: application/vnd.github.v3.raw" \
                                                    https://api.github.com/repos/implydata/imply-release/contents/monthly-release.version |  \
                                                    tee /dev/stderr
                                                ''',
                                                returnStdout: true
                                            ).trim()
                                        }
                                        def artifact_build_version = sh(
                                            script: 'cat build.version',
                                            returnStdout: true
                                        ).trim()
                                        def repo_url = sh(
                                            script: 'git config --get remote.origin.url',
                                            returnStdout: true
                                        ).trim()
                                        rtUpload (
                                            serverId: "${RT_SERVER_ID}",
                                            spec: """{
                                                "files": [
                                                    {
                                                      "pattern": "${ARTIFACT_NAME}-${artifact_build_version}.tar.gz",
                                                      "target": "tgz-local/${ARTIFACT_NAME}/",
                                                      "props": "release.version=${release_version};BVT=Pass;build.url=${BUILD_URL};vcs.url=${repo_url};vcs.branch=${headBranch};build.version=${artifact_build_version}"
                                                    }
                                                ]
                                            }""",
                                            buildName: "${ARTIFACTORY_BUILD_NAME}",
                                            buildNumber: "${BUILD_NUMBER}"
                                        )
                                    }
                                }
                            } // end of stage('upload')
                        } // end of stages
                        post {
                            cleanup {
                                resetWs()
                            }
                        }
                    } // end of stage('profile build and upload')
                } // end of stages
            } // end of matrix
        } // end of stage("Build and publish")
    } // end of stages
    post {
        failure {
            // notify about failed build
            script {
                if(isEligibleToNotify()) {
                    library 'imply-shared-library'
                    failedDruidBVTNotification(currentBuild)
                }
            }
        }
        success {
            // notify about fixed build
            script {
                if(isEligibleToNotify()) {
                    library 'imply-shared-library'
                    fixedDruidBVTNotification(currentBuild)
                }
            }
        }
    }
}

